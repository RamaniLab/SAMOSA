{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility Notebook for 'Massively multiplex single-molecule oligonucleosome footprinting' (Abdulhay, McNally et al)\n",
    "\n",
    "(Notebook Authors: CP McNally & V Ramani; updated 12/07/20)\n",
    "\n",
    "Included here are scripts & code-blocks necessary for regenerating the figures shown in Abdulhay, McNally et al (https://doi.org/10.7554/eLife.59404). Raw and processed data are available on GEO (Accession GSE162410), processed data and some summaries can be found on Zenodo (https://doi.org/10.5281/zenodo.3834705). The code is a mix of Python and R, and is marked accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 1: *In vitro* nucleosome array analyses \n",
    "### Supplementary Figures 2, 3\n",
    "\n",
    "The first cell contains python code that generates a CSV containing information various measurements of the mean IPD for each molecule. The second cell contains R code that uses this CSV to generate Supplementary Figures 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information from each molecule in Sequel 1/2 chromatin and controls, for paper Sup Fig 2A,B\n",
    "# 4/15/2020\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pbcore.io as pb\n",
    "from pbcore.sequence import reverseComplement\n",
    "from Bio import Seq, SeqIO\n",
    "from tqdm import tqdm\n",
    "import edlib\n",
    "import warnings\n",
    "\n",
    "def extractIPDarray(cbamFile, bamFile):\n",
    "    bam = pb.IndexedBamReader(bamFile)\n",
    "    cbam = pb.IndexedBamReader(cbamFile)\n",
    "    \n",
    "    #Find bases that are A or T\n",
    "    refisa = np.array([b == 'A' for b in refseq])\n",
    "    refisc = np.array([b == 'C' for b in refseq])\n",
    "    refisg = np.array([b == 'G' for b in refseq])\n",
    "    refist = np.array([b == 'T' for b in refseq])\n",
    "    \n",
    "    #find all ZMW with ccs that pass filter\n",
    "    validzmw = []\n",
    "    for cc in cbam:\n",
    "        validzmw.append(cc.HoleNumber)\n",
    "\n",
    "    dicdat = {'ZMW':[], 'nSubreads':[], 'nForwardSubreads':[], 'nReverseSubreads':[], 'relCCSLength':[],\n",
    "              'ccsEditDistance':[], 'meanIPD':[], 'meanForwardIPD':[], 'meanReverseIPD':[], \n",
    "              'meanIPDA':[], 'meanIPDC':[], 'meanIPDG':[], 'meanIPDT':[],\n",
    "              'normMeanIPD':[], 'normMeanForwardIPD':[], 'normMeanReverseIPD':[], \n",
    "              'normMeanIPDA':[], 'normMeanIPDC':[], 'normMeanIPDG':[], 'normMeanIPDT':[],\n",
    "              'widmeanIPD':[], 'widmeanForwardIPD':[], 'widmeanReverseIPD':[], \n",
    "              'widmeanIPDA':[], 'widmeanIPDC':[], 'widmeanIPDG':[], 'widmeanIPDT':[],\n",
    "              'widnormMeanIPD':[], 'widnormMeanForwardIPD':[], 'widnormMeanReverseIPD':[], \n",
    "              'widnormMeanIPDA':[], 'widnormMeanIPDC':[], 'widnormMeanIPDG':[], 'widnormMeanIPDT':[],\n",
    "              'outmeanIPD':[], 'outmeanForwardIPD':[], 'outmeanReverseIPD':[], \n",
    "              'outmeanIPDA':[], 'outmeanIPDC':[], 'outmeanIPDG':[], 'outmeanIPDT':[],\n",
    "              'outnormMeanIPD':[], 'outnormMeanForwardIPD':[], 'outnormMeanReverseIPD':[], \n",
    "              'outnormMeanIPDA':[], 'outnormMeanIPDC':[], 'outnormMeanIPDG':[], 'outnormMeanIPDT':[]}\n",
    "        \n",
    "    for zind, zmw in enumerate(tqdm(validzmw, position=0, desc='Getting IPDs')):\n",
    "        cc = cbam.readsByHoleNumber(zmw)\n",
    "        if not len(cc) == 1:\n",
    "            print('cc len: %d' % (len(cc)))\n",
    "        cc = cc[0]\n",
    "        forwardCCS = cc.read(aligned=False)\n",
    "        reverseCCS = reverseComplement(forwardCCS)\n",
    "        faln = edlib.align(forwardCCS, refseq, mode='NW', task='path')\n",
    "        raln = edlib.align(reverseCCS, refseq, mode='NW', task='path')\n",
    "        \n",
    "        subrs = bam.readsByHoleNumber(zmw)\n",
    "        if len(subrs) == 0:\n",
    "            continue\n",
    "        allipds = np.empty((len(subrs), len(refseq)))\n",
    "        allipds.fill(np.nan)\n",
    "        subrOrient = np.empty(len(subrs))\n",
    "\n",
    "        for index, sr in enumerate(subrs):\n",
    "            subrOrient[index] = sr.isForwardStrand\n",
    "            cigarv = sr.unrolledCigar(orientation=\"genomic\")\n",
    "            #using subread base calls that are matches or mismatches\n",
    "            #may want to consider ignoring mismatches in the future, especially for non-synethetic source DNA\n",
    "            #insertions = 1, gaps = 2, leaving both out of downstream analysis\n",
    "            usebases =  cigarv == 7 #np.logical_or(cigarv == 7, cigarv == 8)\n",
    "            ipds = sr.baseFeature('Ipd',aligned=True, orientation=\"genomic\")\n",
    "            pws = sr.baseFeature('PulseWidth',aligned=True, orientation=\"genomic\")\n",
    "            refpos = sr.referencePositions(aligned=True, orientation=\"genomic\")\n",
    "            #use the reference positions to index the per base IPD values into their position in the reference sequence\n",
    "            allipds[index, refpos[usebases]] = ipds[usebases]\n",
    "        normallipds = allipds / np.mean(np.percentile(allipds[~np.isnan(allipds)],usepercentiles))\n",
    "        \n",
    "        # I expect to see RuntimeWarnings in this block\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            formean = np.nanmean(allipds[subrOrient == True,], axis=0)\n",
    "            revmean = np.nanmean(allipds[subrOrient == False,], axis=0)\n",
    "            normformean = np.nanmean(normallipds[subrOrient == True,], axis=0)\n",
    "            normrevmean = np.nanmean(normallipds[subrOrient == False,], axis=0)\n",
    "        \n",
    "        \n",
    "        dicdat['ZMW'].append(zmw)\n",
    "        dicdat['nSubreads'].append(len(subrs))\n",
    "        dicdat['nForwardSubreads'].append(np.sum(subrOrient == True))\n",
    "        dicdat['nReverseSubreads'].append(np.sum(subrOrient == False))\n",
    "        dicdat['relCCSLength'].append(cc.readLength - len(refseq))\n",
    "        dicdat['ccsEditDistance'].append(min(faln['editDistance'], raln['editDistance']))\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            dicdat['meanIPD'].append(np.nanmean(np.concatenate([formean, revmean])))\n",
    "            dicdat['meanForwardIPD'].append(np.nanmean(formean))\n",
    "            dicdat['meanReverseIPD'].append(np.nanmean(revmean))\n",
    "            dicdat['meanIPDA'].append(np.nanmean(np.concatenate([formean[refisa], revmean[refist]])))\n",
    "            dicdat['meanIPDC'].append(np.nanmean(np.concatenate([formean[refisc], revmean[refisg]])))\n",
    "            dicdat['meanIPDG'].append(np.nanmean(np.concatenate([formean[refisg], revmean[refisc]])))\n",
    "            dicdat['meanIPDT'].append(np.nanmean(np.concatenate([formean[refist], revmean[refisa]])))\n",
    "            dicdat['normMeanIPD'].append(np.nanmean(np.concatenate([normformean, normrevmean])))\n",
    "            dicdat['normMeanForwardIPD'].append(np.nanmean(normformean))\n",
    "            dicdat['normMeanReverseIPD'].append(np.nanmean(normrevmean))\n",
    "            dicdat['normMeanIPDA'].append(np.nanmean(np.concatenate([normformean[refisa], normrevmean[refist]])))\n",
    "            dicdat['normMeanIPDC'].append(np.nanmean(np.concatenate([normformean[refisc], normrevmean[refisg]])))\n",
    "            dicdat['normMeanIPDG'].append(np.nanmean(np.concatenate([normformean[refisg], normrevmean[refisc]])))\n",
    "            dicdat['normMeanIPDT'].append(np.nanmean(np.concatenate([normformean[refist], normrevmean[refisa]])))\n",
    "            # now within widom\n",
    "            widfmean = formean[chrUseb == 1]\n",
    "            widrmean = revmean[chrUseb == 1]\n",
    "            normwidfmean = normformean[chrUseb == 1]\n",
    "            normwidrmean = normrevmean[chrUseb == 1]\n",
    "            widisa = refisa[chrUseb == 1]\n",
    "            widisc = refisc[chrUseb == 1]\n",
    "            widisg = refisg[chrUseb == 1]\n",
    "            widist = refist[chrUseb == 1]\n",
    "            dicdat['widmeanIPD'].append(np.nanmean(np.concatenate([widfmean, widrmean])))\n",
    "            dicdat['widmeanForwardIPD'].append(np.nanmean(widfmean))\n",
    "            dicdat['widmeanReverseIPD'].append(np.nanmean(widrmean))\n",
    "            dicdat['widmeanIPDA'].append(np.nanmean(np.concatenate([widfmean[widisa], widrmean[widist]])))\n",
    "            dicdat['widmeanIPDC'].append(np.nanmean(np.concatenate([widfmean[widisc], widrmean[widisg]])))\n",
    "            dicdat['widmeanIPDG'].append(np.nanmean(np.concatenate([widfmean[widisg], widrmean[widisc]])))\n",
    "            dicdat['widmeanIPDT'].append(np.nanmean(np.concatenate([widfmean[widist], widrmean[widisa]])))\n",
    "            dicdat['widnormMeanIPD'].append(np.nanmean(np.concatenate([normwidfmean, normwidrmean])))\n",
    "            dicdat['widnormMeanForwardIPD'].append(np.nanmean(normwidfmean))\n",
    "            dicdat['widnormMeanReverseIPD'].append(np.nanmean(normwidrmean))\n",
    "            dicdat['widnormMeanIPDA'].append(np.nanmean(np.concatenate([normwidfmean[widisa], normwidrmean[widist]])))\n",
    "            dicdat['widnormMeanIPDC'].append(np.nanmean(np.concatenate([normwidfmean[widisc], normwidrmean[widisg]])))\n",
    "            dicdat['widnormMeanIPDG'].append(np.nanmean(np.concatenate([normwidfmean[widisg], normwidrmean[widisc]])))\n",
    "            dicdat['widnormMeanIPDT'].append(np.nanmean(np.concatenate([normwidfmean[widist], normwidrmean[widisa]])))\n",
    "            # now outside widom\n",
    "            outfmean = formean[chrUseb == 2]\n",
    "            outrmean = revmean[chrUseb == 2]\n",
    "            normoutfmean = normformean[chrUseb == 2]\n",
    "            normoutrmean = normrevmean[chrUseb == 2]\n",
    "            outisa = refisa[chrUseb == 2]\n",
    "            outisc = refisc[chrUseb == 2]\n",
    "            outisg = refisg[chrUseb == 2]\n",
    "            outist = refist[chrUseb == 2]\n",
    "            dicdat['outmeanIPD'].append(np.nanmean(np.concatenate([outfmean, outrmean])))\n",
    "            dicdat['outmeanForwardIPD'].append(np.nanmean(outfmean))\n",
    "            dicdat['outmeanReverseIPD'].append(np.nanmean(outrmean))\n",
    "            dicdat['outmeanIPDA'].append(np.nanmean(np.concatenate([outfmean[outisa], outrmean[outist]])))\n",
    "            dicdat['outmeanIPDC'].append(np.nanmean(np.concatenate([outfmean[outisc], outrmean[outisg]])))\n",
    "            dicdat['outmeanIPDG'].append(np.nanmean(np.concatenate([outfmean[outisg], outrmean[outisc]])))\n",
    "            dicdat['outmeanIPDT'].append(np.nanmean(np.concatenate([outfmean[outist], outrmean[outisa]])))\n",
    "            dicdat['outnormMeanIPD'].append(np.nanmean(np.concatenate([normoutfmean, normoutrmean])))\n",
    "            dicdat['outnormMeanForwardIPD'].append(np.nanmean(normoutfmean))\n",
    "            dicdat['outnormMeanReverseIPD'].append(np.nanmean(normoutrmean))\n",
    "            dicdat['outnormMeanIPDA'].append(np.nanmean(np.concatenate([normoutfmean[outisa], normoutrmean[outist]])))\n",
    "            dicdat['outnormMeanIPDC'].append(np.nanmean(np.concatenate([normoutfmean[outisc], normoutrmean[outisg]])))\n",
    "            dicdat['outnormMeanIPDG'].append(np.nanmean(np.concatenate([normoutfmean[outisg], normoutrmean[outisc]])))\n",
    "            dicdat['outnormMeanIPDT'].append(np.nanmean(np.concatenate([normoutfmean[outist], normoutrmean[outisa]])))\n",
    "              \n",
    "    sampdf = pd.DataFrame(dicdat)\n",
    "    return(sampdf)\n",
    "\n",
    "os.chdir('~/pbanalysis')\n",
    "sampleRef = pd.read_csv('/avicenna/vramani/analyses/pacbio/pbrun345_SampleReference.csv')\n",
    "usepercentiles = range(10,41)\n",
    "for ir, record in enumerate(SeqIO.parse(sampleRef.reference[0], 'fasta')):\n",
    "        if ir > 0:\n",
    "            raise InputError('Reference fasta has multiple entries')\n",
    "        refseq = record.seq\n",
    "\n",
    "# bases containing repeats of the widom 601 sequence are labeled with 1\n",
    "chrUseb = np.empty(len(refseq))\n",
    "chrUseb[0:1735] = 2\n",
    "chrUseb[23:170] = 1\n",
    "chrUseb[216-363] = 1\n",
    "chrUseb[409:556] = 1\n",
    "chrUseb[602:749] = 1\n",
    "chrUseb[795:942] = 1\n",
    "chrUseb[983:1130] = 1\n",
    "chrUseb[1180:1326] = 1\n",
    "chrUseb[1372:1519] = 1\n",
    "chrUseb[1565:1712] = 1\n",
    "\n",
    "\n",
    "\n",
    "comparesamps = [0,1,2,3, 29, 30]\n",
    "unifiedSampleName = {0:'Chromatin', 1:'Naked Methylated', 2:'Naked Negative',\n",
    "                     3:'Chromatin', 30:'Naked Methylated', 29:'Naked Negative'}\n",
    "sampDFs = []\n",
    "for samp in comparesamps:\n",
    "    sdf = extractIPDarray(sampleRef.ccsFile[samp], sampleRef.alignedSubreadsFile[samp])\n",
    "    sdf['sample'] = unifiedSampleName[samp]\n",
    "    sdf['cell'] = sampleRef.sampleName[samp]\n",
    "    if samp <= 2:\n",
    "        sdf['machine'] = 1\n",
    "    else:\n",
    "        sdf['machine'] = 2\n",
    "    sampDFs.append(sdf)\n",
    "\n",
    "totdf = pd.concat(sampDFs)\n",
    "#totdf.to_pickle('processed/forFigures/meanIPDinfoChrControls.pickle', protocol=-1)\n",
    "totdf.to_csv('processed/forFigures/meanIPDinfoChrControls.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(tidyr)\n",
    "setwd('~/pbanalysis')\n",
    "ipddf = read.csv(\"processed/forFigures/meanIPDinfoChrControls.csv\")\n",
    "ipddf$sample = factor(ipddf$sample, levels=c('Naked Negative', 'Naked Methylated', 'Chromatin')) #changing order\n",
    "\n",
    "\n",
    "filtercriteria = abs(ipddf$relCCSLength) <= 2 & ipddf$nForwardSubreads >= 3 & ipddf$nReverseSubreads >= 3\n",
    "\n",
    "sprintf('Before filtering, %d Sequel molecules, %d Sequel II molecules, %d total', sum(ipddf$machine == 1),\n",
    "        sum(ipddf$machine == 2), dim(ipddf)[1])\n",
    "sprintf('After filtering, %d Sequel molecules, %d Sequel II molecules, %d total', sum(filtercriteria & ipddf$machine == 1),\n",
    "        sum(filtercriteria & ipddf$machine == 2), sum(filtercriteria))\n",
    "\n",
    "# Supplementary Figure 2\n",
    "ipddfsub = ipddf[filtercriteria, c(\"meanIPD\", \"normMeanIPD\", \"machine\", \"sample\")]\n",
    "# rescale mean normalized IPD to put it back on the same scale as the mean raw IPD\n",
    "ipddfsub$normMeanIPD = ipddfsub$normMeanIPD * (mean(ipddfsub$meanIPD) / mean(ipddfsub$normMeanIPD))\n",
    "normnot_long = pivot_longer(ipddfsub, c(\"meanIPD\", \"normMeanIPD\"), names_to=\"normed\", values_to=\"meanIPD\")\n",
    "\n",
    "facetlab_names = c('1'='Sequel', '2'='Sequel II', 'meanIPD'='Raw IPD', 'normMeanIPD'='Quantile Normalized')\n",
    "ggplot(data = normnot_long, aes(meanIPD, colour=sample, fill=sample)) +\n",
    "  geom_density(alpha=0.4) +\n",
    "  facet_grid(cols=vars(machine), rows=vars(normed), labeller=as_labeller(facetlab_names), scales='free_y') +\n",
    "  scale_fill_brewer(palette='Dark2', name='Sample', labels=c('Naked DNA', 'Naked DNA + EcoGII', 'Chromatin + EcoGII')) +\n",
    "  scale_colour_brewer(palette='Dark2', name='Sample', labels=c('Naked DNA', 'Naked DNA + EcoGII', 'Chromatin + EcoGII')) +\n",
    "  scale_x_continuous(limits=c(0,150)) +\n",
    "  labs(x='Mean IPD', y='Density') +\n",
    "  theme_bw()\n",
    "ggsave(\"Figures/nearFinal/moleculeMeanIPDdist.png\", width=8, height=6, dpi=300)\n",
    "ggsave(\"Figures/nearFinal/moleculeMeanIPDdist.pdf\", width=8, height=6)\n",
    "\n",
    "\n",
    "# Supplementary Figure 3\n",
    "ipddfsub = ipddf[filtercriteria, c(\"widnormMeanIPDA\",\"widnormMeanIPDC\",\"widnormMeanIPDG\",\"widnormMeanIPDT\",\n",
    "                                               \"outnormMeanIPDA\",\"outnormMeanIPDC\",\"outnormMeanIPDG\",\"outnormMeanIPDT\",\n",
    "                                               \"machine\", \"sample\")]\n",
    "ipddflong = pivot_longer(ipddfsub, c(\"widnormMeanIPDA\",\"widnormMeanIPDC\",\"widnormMeanIPDG\",\"widnormMeanIPDT\",\n",
    "                                     \"outnormMeanIPDA\",\"outnormMeanIPDC\",\"outnormMeanIPDG\",\"outnormMeanIPDT\"),\n",
    "                         names_to=c(\"widom\",\"base\"), names_pattern=\"([a-z]{3})normMeanIPD([A-Z])\", values_to=\"meanIPD\")\n",
    "ipddflong$base = factor(ipddflong$base, levels=c('T','G','C','A'))\n",
    "facetlab_names = c('out'='Linker', 'wid'='Widom 601 Sequence', 'Naked Negative'='Naked DNA',\n",
    "                   'Naked Methylated'='Naked DNA + EcoGII', 'Chromatin'='Chromatin + EcoGII')\n",
    "ggplot(data = ipddflong[ipddflong$machine == 1,], aes(x=base, y=meanIPD)) +\n",
    "  geom_violin(fill='rosybrown') +\n",
    "  facet_grid(cols=vars(sample), rows=vars(widom), labeller=as_labeller(facetlab_names)) +\n",
    "  scale_x_discrete(labels=c('A','C','G','T')) +\n",
    "  labs(x=\"Template Base\", y='Mean IPD', title='Sequel') +\n",
    "  theme_bw()\n",
    "ggsave(\"Figures/nearFinal/moleculeMeanIPDbyBase_Sequel.png\", width=10, height=6, dpi=300)\n",
    "ggsave(\"Figures/nearFinal/moleculeMeanIPDbyBase_Sequel.pdf\", width=10, height=6)\n",
    "\n",
    "ggplot(data = ipddflong[ipddflong$machine == 2,], aes(x=base, y=meanIPD)) +\n",
    "  geom_violin(fill='rosybrown') +\n",
    "  facet_grid(cols=vars(sample), rows=vars(widom), labeller=as_labeller(facetlab_names)) +\n",
    "  scale_x_discrete(labels=c('A','C','G','T')) +\n",
    "  labs(x=\"Template Base\", y='Mean IPD', title='Sequel II') +\n",
    "  theme_bw()\n",
    "ggsave(\"Figures/nearFinal/moleculeMeanIPDbyBase_SequelII.png\", width=10, height=6, dpi=300)\n",
    "ggsave(\"Figures/nearFinal/moleculeMeanIPDbyBase_SequelII.pdf\", width=10, height=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1D, Supplementary Figure 5\n",
    "\n",
    "The below code produces heatmaps of the methylation posterior probability across individual molecules. It takes as input the numpy files produced as outputs from extractIPD.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(tidyr)\n",
    "library(RcppCNPy)\n",
    "library(scales)\n",
    "library(zoo)\n",
    "\n",
    "nanmean <- function(x) {\n",
    "  nonnan = x[is.finite(x)]\n",
    "  if (length(nonnan) < 1) {\n",
    "    return(NA)\n",
    "  } else {\n",
    "    return( mean(nonnan))\n",
    "  }\n",
    "}\n",
    "\n",
    "plotNmolecules = 500\n",
    "smoothbases = 5\n",
    "\n",
    "basep = '~/pbanalysis/processed/binarized'\n",
    "figurefolder = '~/pbanalysis/Figures/nearFinal/'\n",
    "samples = c('pbrun4_gold_nuc47_chromatin', 'pbrun5_cell2_47bp_DNA_minusM_rep1', 'pbrun5_cell2_47bp_DNA_plusM_rep1')\n",
    "\n",
    "for (samp in samples) {\n",
    "  bipds = npyLoad(file.path(basep, paste(samp, '_bingmm.npy', sep='')))\n",
    "  \n",
    "  rolltipd = bipds[1:plotNmolecules,]\n",
    "  for (i in 1:500) {\n",
    "    rolltipd[i,] = rollapply(bipds[i,], smoothbases, nanmean, align='left', partial=TRUE)\n",
    "  }\n",
    "  \n",
    "  rzdf = data.frame(rolltipd[1:plotNmolecules,])\n",
    "  rzdf$molecule = 1:plotNmolecules\n",
    "  rzdflong = pivot_longer(rzdf, X1:X2268, names_prefix='X', names_to='base', values_to='ipd')\n",
    "  rzdflong$base = as.integer(rzdflong$base)\n",
    "  ggplot(rzdflong[is.finite(rzdflong$ipd) & rzdflong$molecule <= plotNmolecules,], aes(x=base, y=molecule, fill=ipd)) +\n",
    "    geom_raster() +\n",
    "    scale_fill_distiller(palette='BuPu', direction=1, limits=c(0,1), oob=squish, name='Posterior probability methylated\\n(5 bp smoothed)') +\n",
    "    scale_x_continuous(expand=c(0,0)) +\n",
    "    scale_y_continuous(expand=c(0,0)) +\n",
    "    labs(x=\"Position along molecule\", y='Molecules') +\n",
    "    theme_bw()\n",
    "  ggsave(file.path(figurefolder, paste('heatmapRollingpp5c_', samp, '.png',sep='')), width=6, heigh=5, dpi=300)\n",
    "  ggsave(file.path(figurefolder, paste('heatmapRollingpp5c_', samp, '.pdf',sep='')), width=6, heigh=5)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1B, 1C\n",
    "\n",
    "The below code uses files output by callNucPeaks.py and calculates deviations of these nucleosome calls from the expected center of the widom 601 sequence, as well as distances between adjacent nucleosome calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(feather)\n",
    "library(ggplot2)\n",
    "\n",
    "setwd('~/pbanalysis')\n",
    "bpc = read_feather('processed/peaks/binarypeaks/pbrun4_gold_nuc47_chromatin_peaks.feather')\n",
    "\n",
    "widomcenters = c(23, 216, 409, 602, 795, 983, 1179, 1372, 1565) + 73\n",
    "distToWidC = vector(length=dim(bpc)[1])\n",
    "for (i in 1:dim(bpc)[1]) {\n",
    "  dtow = bpc$pos[i] - widomcenters\n",
    "  distToWidC[i] = dtow[which.min(abs(dtow))]\n",
    "}\n",
    "bpc$distToW = distToWidC\n",
    "\n",
    "\n",
    "hist(bpc$distToW[bpc$distToW <= 100])\n",
    "ggplot(bpc[bpc$distToW <= 100,], aes(distToW)) +\n",
    "  geom_density(fill='black') +\n",
    "  labs(x='Distance to expected dyad', y='Density') +\n",
    "  theme_bw()\n",
    "ggsave('Figures/nearFinal/dToExpDyad.png', width=4, height=3,units='in')\n",
    "ggsave('Figures/nearFinal/dToExpDyad.pdf', width=4, height=3,units='in')\n",
    "sprintf('Mean distance to expected dyad: %f', mean(bpc$distToW[bpc$distToW <= 100]))\n",
    "sprintf('Standard deviation of predicted dyad position: %f', sd(bpc$distToW[bpc$distToW <= 100]))\n",
    "sprintf('Mean absolute distance to expected dyad: %f', mean(abs(bpc$distToW[bpc$distToW <= 100])))\n",
    "\n",
    "bpc$dToNext = NA\n",
    "for (i in 1:(dim(bpc)[1]-1)) {\n",
    "  if (bpc$zmw[i+1] == bpc$zmw[i]) {\n",
    "    bpc$dToNext[i] = bpc$pos[i+1] - bpc$pos[i]\n",
    "  }\n",
    "}\n",
    "abpc = bpc[bpc$pos <= (widomcenters[8] + (widomcenters[9] - widomcenters[8]) / 2),]\n",
    "abpc = abpc[is.finite(abpc$dToNext),]\n",
    "\n",
    "zmw = unique(abpc$zmw)\n",
    "molmeandtn = vector(length=length(zmw))\n",
    "for (i in 1:length(zmw)) {\n",
    "  molmeandtn[i] = mean(abpc$dToNext[abpc$zmw == zmw[i]])\n",
    "}\n",
    "\n",
    "molmeand = data.frame(molmeandtn)\n",
    "molmeand$type = 'molecule'\n",
    "molmeand$nrl = molmeand$molmeandtn\n",
    "subd = abpc$dToNext\n",
    "subd = data.frame(subd)\n",
    "subd$nrl = subd$subd\n",
    "subd$type = 'individual'\n",
    "combd = rbind(subset(subd, select=c(nrl,type)), subset(molmeand, select=c(nrl,type)))\n",
    "type_names = c('individual'='Single gap', 'molecule'='Molecule mean')\n",
    "ggplot(combd, aes(nrl, fill=type, color=type)) +\n",
    "  geom_vline(xintercept=193) +\n",
    "  geom_density(alpha=0.5) +\n",
    "  scale_x_continuous(limits=c(147,275)) +\n",
    "  scale_color_brewer(palette = \"Set1\", name='',labels = c('Single distances', 'Molecule means')) +\n",
    "  scale_fill_brewer(palette = \"Set1\", name='',labels = c('Single distances', 'Molecule means')) +\n",
    "  labs(y='Density', x='Distance between nucleosome calls') +\n",
    "  theme_bw()\n",
    "ggsave('Figures/nearFinal/dBetweenNucCalls.png', width=5, height=3,units='in')\n",
    "ggsave('Figures/nearFinal/dBetweenNucCalls.pdf', width=5, height=3,units='in')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplementary Figure 5\n",
    "\n",
    "This code plots the distribution of nucleosome calls across the molecule as well as the average posterior probabilities of methylation.\n",
    "\n",
    "Input: Binarized base calls output by extractIPD.py, and predicted nucleosome centers from callNucPeaks.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make comparison of remodeled arrays - Now with Binarized data\n",
    "# Colin McNally 2020/03/02\n",
    "\n",
    "library(RcppCNPy)\n",
    "library(ggplot2)\n",
    "library(RColorBrewer)\n",
    "library(feather)\n",
    "\n",
    "setwd('~/pbanalysis')\n",
    "\n",
    "usesamples = list(c('pbrun4_gold', 'nuc47_chromatin'))\n",
    "use_names = c('nuc47_chromatin' = 'Chromatin')\n",
    "\n",
    "add_widomRect <- function(xstart){\n",
    "    annotate(\"rect\", ymin = -Inf, ymax = Inf, xmin=xstart, xmax=xstart+146, alpha = 0.7, fill=bcols[3] )\n",
    "}\n",
    "\n",
    "thesesamples = usesamples\n",
    "thesenames = use_names\n",
    "\n",
    "dfipd = data.frame()\n",
    "for (samp in thesesamples) {\n",
    "    allTipds = npyLoad(file.path('processed','binarized', paste(samp[1], '_', samp[2], '_bingmm.npy', sep='')))\n",
    "    ipdmeans = colMeans(allTipds, na.rm=T)\n",
    "    ipdsds = apply(allTipds, 2, sd, na.rm=T)\n",
    "    xind = which(is.finite(ipdmeans))\n",
    "    df = data.frame(\"pos\" = xind, \"means\" = ipdmeans[xind], \"sds\" = ipdsds[xind])\n",
    "    df$sample = samp[2]\n",
    "    dfipd = rbind(dfipd, df)\n",
    "}\n",
    "\n",
    "dfpeaks = data.frame()\n",
    "for (samp in thesesamples) {\n",
    "    samppeaks = read_feather(file.path('processed','peaks','binarypeaks', paste(samp[1], '_', samp[2], '_peaks.feather', sep='')))\n",
    "    samppeaks$sample = samp[2]\n",
    "    dfpeaks = rbind(dfpeaks, samppeaks)\n",
    "}\n",
    "\n",
    "widomstart = c(23, 216, 409, 602, 795, 983, 1179, 1372, 1565)\n",
    "bcols = brewer.pal(7, 'PuOr')\n",
    "\n",
    "ggplot(data = dfipd, aes(x=pos)) + \n",
    "    scale_y_continuous(\"Called dyad density\", sec.axis = sec_axis(~ ( . - .15) / .65, name = \"Mean methylation call\")) +\n",
    "    add_widomRect(widomstart) +\n",
    "    geom_histogram(data=dfpeaks, aes(y=..count../max(dfpeaks$zmw)), binwidth=5, fill=bcols[1]) +\n",
    "    geom_point(y=.15 + dfipd$means * .65, size=1, shape=19, color=bcols[7], alpha=1, stroke=0) +\n",
    "    xlab('Position along molecule') + \n",
    "    theme_bw()\n",
    "\n",
    "fname = 'BinarizedPeakCallWholeMolecule'\n",
    "ggsave(paste(\"Figures/nearFinal/\", fname,'.pdf', sep=''), width=9.5,height=3,units=\"in\")\n",
    "ggsave(paste(\"Figures/nearFinal/\", fname,'.png', sep=''), width=9.5,height=3,dpi=300,units=\"in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplementary Figure 4\n",
    "\n",
    "This figure shows the bias in methylation predictions at different sequence contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib as mpl\n",
    "from Bio import Seq, SeqIO, SeqRecord\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "os.chdir('/avicenna/cmcnally/pbanalysis')\n",
    "\n",
    "sampleRef = pd.read_csv('/avicenna/vramani/analyses/pacbio/pbrun3-8c2_SampleReference.csv')\n",
    "for record in SeqIO.parse(sampleRef['reference'][0], 'fasta'):\n",
    "    refseq = record.seq\n",
    "    \n",
    "# set Arial as the default sans-serif font\n",
    "mpl.rcParams['font.sans-serif'] = \"Arial\"\n",
    "# Then, \"ALWAYS use sans-serif fonts\"\n",
    "mpl.rcParams['font.family'] = \"sans-serif\"\n",
    "\n",
    "usesamples = [29, 30]\n",
    "tipds = {}\n",
    "bipds = {}\n",
    "lipds = {}\n",
    "for isamp, samp in enumerate(usesamples):\n",
    "    tipds[samp] = np.load(os.path.join('processed','onlyT',\n",
    "                                 sampleRef['cell'][samp] + '_' + sampleRef['sampleName'][samp] + '_onlyT.npy'))\n",
    "    bipds[samp] = np.load(os.path.join('processed','binarized',\n",
    "                             sampleRef['cell'][samp] + '_' + sampleRef['sampleName'][samp] + '_bingmm.npy'))\n",
    "    lipds[samp] = np.log10(tipds[samp])\n",
    "bisa = np.nonzero(np.sum(~np.isnan(tipds[30]), 0) > 0)[0]\n",
    "\n",
    "# final version using -2:+7\n",
    "\n",
    "kmerlen = 10\n",
    "basecomplement = {'A':'T', 'C':'G', 'G':'C', 'T':'A'}\n",
    "basesame = {'A':'A', 'C':'C', 'G':'G', 'T':'T'}\n",
    "seqMatrix = np.full((len(bisa), kmerlen), '-', dtype=np.dtype('U1'))\n",
    "\n",
    "for ib, base in enumerate(bisa):\n",
    "    for offset in np.arange(-2,8):\n",
    "        if refseq[base] == 'A':\n",
    "            pos = base + offset\n",
    "            usedic = basesame\n",
    "        if refseq[base] == 'T':\n",
    "            pos = base - offset\n",
    "            usedic = basecomplement\n",
    "        if pos >= 0 and pos < len(refseq):\n",
    "            seqMatrix[ib, offset + 2] = usedic[refseq[pos][0]]\n",
    "\n",
    "posGrouped = []\n",
    "for ib, base in enumerate(bisa):\n",
    "    if np.sum(seqMatrix[ib,:] == '-') == 0:\n",
    "        if len(posGrouped) == 0:\n",
    "            posGrouped.append([ib])\n",
    "        else:\n",
    "            foundExistingGroup = False\n",
    "            for grp in range(len(posGrouped)):\n",
    "                if np.sum(seqMatrix[ib,:] == seqMatrix[posGrouped[grp][0],:]) == seqMatrix.shape[1]:\n",
    "                    posGrouped[grp].append(ib)\n",
    "                    foundExistingGroup = True\n",
    "                    break\n",
    "            if not foundExistingGroup:\n",
    "                posGrouped.append([ib])\n",
    "                \n",
    "vseqdict = {'A':3, 'C':2, 'G':1, 'T':0}\n",
    "vseqMat = np.full((len(posGrouped), kmerlen), '0', np.int8)\n",
    "for grp in range(len(posGrouped)):\n",
    "    for x in range(seqMatrix.shape[1]):\n",
    "        vseqMat[grp,x] = vseqdict[seqMatrix[posGrouped[grp][0],x]]\n",
    "\n",
    "cmaplist = [(251/255, 65/255, 53/255, 1),\n",
    "            (18/255, 18/255, 18/255, 1),\n",
    "            (65/255, 105/255, 225/255, 1),\n",
    "            (50/255, 205/255, 51/255, 1)]\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "    'Custom cmap', cmaplist, 4)\n",
    "\n",
    "lbins = np.linspace(0, 2.2, 101)\n",
    "bhist = {s:np.full((len(posGrouped), 100), 0.0) for s in usesamples}\n",
    "for samp in usesamples:\n",
    "    for ig, group in enumerate(posGrouped):\n",
    "        h, _ = np.histogram(np.concatenate([lipds[samp][:,bisa[ib]] for ib in group]), lbins, density=True)\n",
    "        bhist[samp][ig,:] = h\n",
    "\n",
    "combhist = np.full((len(posGrouped), 100), 0.0)\n",
    "for ig, group in enumerate(posGrouped):\n",
    "        h, _ = np.histogram(np.concatenate([lipds[samp][:,bisa[ib]] for ib in group for samp in [29,30]]), lbins, density=True)\n",
    "        combhist[ig,:] = h\n",
    "\n",
    "fracmeth = np.full((len(posGrouped)), np.nan)\n",
    "for ig, group in enumerate(posGrouped):\n",
    "    thisbasebin = np.concatenate([bipds[samp][:,bisa[ib]] for ib in group for samp in [29,30]])\n",
    "    fracmeth[ig] = np.sum(thisbasebin > 0.5) / np.sum(~np.isnan(thisbasebin))\n",
    "sortby = np.argsort(fracmeth)\n",
    "\n",
    "fig, ax = plt.subplots(1,4, figsize=(9,8), gridspec_kw={'width_ratios':[1,1,1.25,1.2]})\n",
    "ax[0].imshow(bhist[29][sortby,:], aspect='auto', extent=[0, 2, len(posGrouped), 0], vmin=0, vmax=4)\n",
    "\n",
    "im1 = ax[1].imshow(combhist[sortby,:], aspect='auto', extent=[0, 2, len(posGrouped), 0], vmin=0, vmax=4)\n",
    "#cb1 = fig.colorbar(im1, ax=ax[1], aspect=10, label='Density')\n",
    "im2 = ax[2].imshow(bhist[30][sortby,:], aspect='auto', extent=[0, 2, len(posGrouped), 0], vmin=0, vmax=4)\n",
    "cb2 = fig.colorbar(im2, ax=ax[2], aspect=10, label='Density')\n",
    "cb2.set_ticks([0,1,2,3,4])\n",
    "\n",
    "im3 = ax[3].imshow(vseqMat[sortby,:], aspect='auto', cmap=cmap, interpolation='nearest', vmin=-0.5, vmax=3.5)\n",
    "cb = fig.colorbar(im3, ax=ax[3], aspect=10)\n",
    "cb.set_ticks([0,1,2,3])\n",
    "cb.set_ticklabels(['T','G','C','A'])\n",
    "ax[0].set_title('Negative Control')\n",
    "ax[1].set_title('Combined')\n",
    "ax[2].set_title('Positive Control')\n",
    "ax[3].set_title('Sequence Context')\n",
    "ax[0].set_ylabel('%d-mers' % (kmerlen))\n",
    "for i in range(3):\n",
    "    ax[i].set_xticks([0,1,2])\n",
    "    ax[i].set_xticklabels([1, 10, 100])\n",
    "    ax[i].set_xlabel('Normalized IPD')\n",
    "ax[3].set_xticks([0,2,5,9])\n",
    "ax[3].set_xticklabels([-2, 0,3,7])\n",
    "#plt.subplots_adjust(wspace=0.3)\n",
    "plt.tight_layout(0.4)\n",
    "\n",
    "fig.savefig('Figures/Revision/widom_%dmerDistributions_byCombBinF.png' % (kmerlen), dpi=300)\n",
    "fig.savefig('Figures/Revision/widom_%dmerDistributions_byCombBinF.pdf' % (kmerlen))\n",
    "\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures 2 - 5: *In vivo* data analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few hard-coded parameters here that we mention in the Methods section but would also like to call out here.\n",
    "\n",
    "\n",
    "1. We smooth the posterior probabilities a few different ways during the paper to account for regions with low local A/T content and generally denoise the single-molecule signal. For in vitro analyses, we smooth the calculated posterior probabilities using a 5 bp rolling mean. For all in vivo analyses in the paper that involve calculation of single-molecule autocorrelograms, averaging over multiple templates, and visualizing individual molecules, we smooth posteriors with a 33 bp rolling mean. \n",
    "\n",
    "\n",
    "2. For all autocorrelation calculations we ignore regions where compared lengths would be unequal; this has the effect of rendering the returned autocorrelogram exactly 0.5 * the input length.\n",
    "\n",
    "## These are helpful functions + modules that are used in the below cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from pylab import *\n",
    "from scipy import signal\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "def ccf(x, y):\n",
    "    result = np.correlate(y - np.mean(y), x - np.mean(x), 'same') / (np.std(y) * np.std(x) * len(y))\n",
    "    length = (len(result) - 1) // 2\n",
    "    return result[length:]\n",
    "\n",
    "def xcor(a,b, maxlen = 1000, lengths=False):\n",
    "    foo = []\n",
    "    if isinstance(lengths, np.ndarray):\n",
    "        for i in range(len(a)):\n",
    "            res = ccf(a[i][:lengths[i]],b[i][:lengths[i]]) #take the nonNan portion of the read\n",
    "            if len(res) < maxlen:\n",
    "                filler = np.empty(maxlen - len(res))\n",
    "                filler[:] = np.nan\n",
    "                res = np.append(res, filler)\n",
    "                foo.append(res)\n",
    "            else:\n",
    "                res = ccf(a[i],b[i])\n",
    "                foo.append(res)                \n",
    "    else:\n",
    "        for i in range(len(a)):\n",
    "            res = ccf(a[i],b[i])\n",
    "            foo.append(res)\n",
    "    a = None\n",
    "    b = None\n",
    "    return foo\n",
    "\n",
    "def eat_pickle(pick, zmwinfo, clean_cutoff=1.02):\n",
    "    '''Given a cutoff to exclude unmethylated templates, convert pickle-format ZMW info into a zmw_dict\n",
    "    and return valid ZMWs + the whole dict.'''\n",
    "    valid_zmws = {}\n",
    "    with open(pick, 'rb') as fout:\n",
    "        tipds = pickle.load(fout, encoding=\"latin1\")\n",
    "    with open(zmwinfo, 'rb') as fout:\n",
    "        zmws = pickle.load(fout, encoding=\"latin1\")\n",
    "    subset = zmws[zmws['basemeanT'] / zmws['basemeanA'] >= clean_cutoff]\n",
    "    for i in subset['zmw']:\n",
    "        valid_zmws[i] = True\n",
    "    return (tipds, valid_zmws)\n",
    "\n",
    "def return_ipd_ratios(zmwinfo):\n",
    "    with open(zmwinfo, 'rb') as fout:\n",
    "        zmws = pickle.load(fout, encoding='latin1')\n",
    "    ratios = zmws['basemeanT'] / zmws['basemeanA']\n",
    "    return ratios\n",
    "\n",
    "def return_probs(zmwinfo):\n",
    "    prob_tot = []\n",
    "    with open(zmwinfo, 'rb') as fout:\n",
    "        tipds = pickle.load(fout, encoding=\"latin1\")\n",
    "    for hole in tipds:\n",
    "        tot = len(tipds[hole][~np.isnan(tipds[hole])])\n",
    "        frac_meth = len(tipds[hole][(tipds[hole] >= 0.25) & (~np.isnan(tipds[hole]))]) / tot\n",
    "        prob_tot.append(frac_meth)\n",
    "    return prob_tot\n",
    "\n",
    "def eat_pickle_binary(pick, samp_label):\n",
    "    with open(pick, 'rb') as fout:\n",
    "        tipds = pickle.load(fout, encoding=\"latin1\")    \n",
    "    return (tipds)\n",
    "\n",
    "def eat_pickle_b2m(pick, samp_label,length, min_len = 500, samp= 0):\n",
    "    with open(pick, 'rb') as fout:\n",
    "        tipds = pickle.load(fout, encoding=\"latin1\")    \n",
    "    nmol = 0\n",
    "    lengths = []\n",
    "    reads_f = []\n",
    "    labels = []\n",
    "    holes = []\n",
    "#     reads_r = []\n",
    "    mean_probs = []\n",
    "    index = 0\n",
    "    for hole in tipds:\n",
    "        read = tipds[int(hole)]\n",
    "#         read_r = read[::-1]\n",
    "        if len(read) < min_len: continue\n",
    "        labels.append(samp_label)\n",
    "        holes.append(\"%s_%s\" % (samp_label, hole))\n",
    "        if len(tipds[int(hole)]) >= length:\n",
    "            reads_f.append(read[:length])\n",
    "#             reads_r.append(read_r[:length])\n",
    "            lengths.append(len(tipds[int(hole)]))\n",
    "#            holes.append(rep_label)\n",
    "        elif len(tipds[int(hole)]) < length:\n",
    "            filler = np.empty(length - len(tipds[int(hole)]))\n",
    "            filler[:] = np.nan\n",
    "            lengths.append(len(read))\n",
    "            read = np.append(read, filler)\n",
    "            reads_f.append(read)\n",
    "        nmol += 1\n",
    "        index += 1\n",
    "        if samp != 0:\n",
    "            if nmol == samp: break\n",
    "    new_mat = np.vstack(reads_f)\n",
    "#     new_mat_r = np.vstack(reads_r)\n",
    "    return (new_mat, lengths, labels, holes)\n",
    "    \n",
    "def distill_tipds_bed(tipds, valid_zmws, sitelist, length, label, subsample=0,min_len=500):\n",
    "    '''Used to extract relevant mapped ZMWs given a list of ZMW ids. Also excludes bad ZMWs given the valid\n",
    "    ZMW list. This is for fragment ends (doesn't search for a flat feaure of interest).'''\n",
    "    sites = open(sitelist,'r')\n",
    "    hole_nos = {}\n",
    "    invalid = 0\n",
    "    tot = 0\n",
    "    for i in sites:\n",
    "        split = i.split()\n",
    "        tot += 1\n",
    "        if int(split[0]) not in valid_zmws: \n",
    "            invalid +=1\n",
    "            continue\n",
    "        hole_nos[split[0]] = [split[1], int(split[3]) - int(split[2]), split[4]]\n",
    "    sites.close()\n",
    "    new_mat = np.zeros(length)\n",
    "    lengths = []\n",
    "    labs = []\n",
    "    reads_f = []\n",
    "    reads_r = []\n",
    "    n_mol = 0\n",
    "    holes = []\n",
    "    for hole in hole_nos:\n",
    "        if subsample != 0:\n",
    "            if n_mol >= subsample: break\n",
    "        read = tipds[int(hole)]\n",
    "        read_r = read[::-1]\n",
    "        dist = hole_nos[hole]\n",
    "        if len(tipds[int(hole)]) < min_len: continue\n",
    "        if len(tipds[int(hole)]) >= length:\n",
    "            reads_f.append(read[:length])\n",
    "            reads_r.append(read_r[:length])\n",
    "            lengths.append(len(tipds[int(hole)]))\n",
    "            labs.append(dist[-1])\n",
    "#            holes.append(rep_label)\n",
    "            n_mol +=1\n",
    "        elif len(tipds[int(hole)]) < length:\n",
    "            filler = np.empty(length - len(tipds[int(hole)]))\n",
    "            filler[:] = np.nan\n",
    "            lengths.append(len(read))\n",
    "            read = np.append(read, filler)\n",
    "            read_r = np.append(read_r, filler)\n",
    "            reads_f.append(read)\n",
    "            reads_r.append(read_r)\n",
    "            labs.append(dist[-1])\n",
    "#            holes.append(rep_label)\n",
    "            n_mol +=1\n",
    "    new_mat = np.vstack(reads_f)\n",
    "    new_mat_r = np.vstack(reads_r)\n",
    "    dinuc_mat = np.vstack(dinucs)\n",
    "    return (new_mat, new_mat_r, lengths, labs,holes,invalid,tot)\n",
    "\n",
    "def distill_tipds_flat_centered(tipds, sitelist, length, label,subsample=0):\n",
    "    sites = open(sitelist,'r')\n",
    "    hole_nos = {}\n",
    "    for i in sites:\n",
    "        split = i.split()\n",
    "        #check to make sure the feature is in the middle of the read \n",
    "        #and enough on both sites\n",
    "        if int(split[2]) <= int(split[4]) <= int(split[3]):\n",
    "            index1 = int(split[4]) - int(split[2])\n",
    "            index2 = int(split[3]) - int(split[4])\n",
    "            strand = split[-1]\n",
    "            r_strand = split[-2]\n",
    "            if index1 > (length / 2) and index2 > (length / 2):\n",
    "                hole_nos[split[0]] = (index1, strand, r_strand)\n",
    "    sites.close()\n",
    "    lengths = []\n",
    "    labs = []\n",
    "    reads = []\n",
    "    mno = 0\n",
    "    for hole in hole_nos:\n",
    "        read = tipds[int(hole)]\n",
    "        index,strand,r_strand = hole_nos[hole]\n",
    "        if r_strand == '-':\n",
    "            read = read[::-1]\n",
    "        if strand == \"+\":\n",
    "            extract = tipds[int(hole)][int(index - (length / 2)): int(index + (length / 2))]\n",
    "            if len(extract) != length: continue\n",
    "            reads.append(extract)\n",
    "            labs.append(label)\n",
    "            lengths.append(len(tipds[int(hole)]))\n",
    "        else:\n",
    "            extract = tipds[int(hole)][::-1][int(index - (length / 2)):int(index + (length / 2))]\n",
    "            if len(extract) != length: continue\n",
    "            reads.append(extract)\n",
    "            labs.append(label)\n",
    "            lengths.append(len(tipds[int(hole)]))\n",
    "        if subsample != 0:\n",
    "            mno += 1\n",
    "            if mno == subsample: break\n",
    "    new_mat = np.vstack(reads)\n",
    "    return (new_mat, np.array(lengths), np.array(labs))\n",
    "\n",
    "def distill_tipds_tad(tipds, valid_zmws, sitelist, dinuclist, length, label, rep_label, subsample=0, min_len=500):\n",
    "    '''Used to extract relevant mapped ZMWs given a list of ZMW ids. Also excludes bad ZMWs given the valid\n",
    "    ZMW list. This is for fragment ends (doesn't search for a flat feaure of interest). We also need to validate \n",
    "    that findings on the 3 prime end of fragments.'''\n",
    "    sites = open(sitelist,'r')\n",
    "    dinucs = open(dinuclist).readlines()\n",
    "    hole_nos = {}\n",
    "    dinuc_content = {}\n",
    "    idx=0\n",
    "    for i in sites:\n",
    "        split = i.split()\n",
    "        nucs = np.array(dinucs[idx].split()[7:],dtype=int)\n",
    "        idx+=1\n",
    "        if int(split[0]) not in valid_zmws: continue\n",
    "        dinuc_content[split[0]] = nucs\n",
    "        hole_nos[split[0]] = [split[1], int(split[3]) - int(split[2]), split[4]]\n",
    "    sites.close()\n",
    "    new_mat = np.zeros(length)\n",
    "    lengths = []\n",
    "    labs = []\n",
    "    reads_f = []\n",
    "    reads_r = []\n",
    "    n_mol = 0\n",
    "    holes = []\n",
    "    dinucs = []\n",
    "    for hole in hole_nos:\n",
    "        if subsample != 0:\n",
    "            if n_mol >= subsample: break\n",
    "        read = tipds[int(hole)]\n",
    "        read_r = read[::-1]\n",
    "        dist = hole_nos[hole]\n",
    "        dinuc_vec = dinuc_content[hole]\n",
    "        if len(tipds[int(hole)]) < min_len: continue\n",
    "        if len(tipds[int(hole)]) >= length:\n",
    "            reads_f.append(read[:length])\n",
    "            reads_r.append(read_r[:length])\n",
    "            lengths.append(len(tipds[int(hole)]))\n",
    "            labs.append(dist[-1])\n",
    "            holes.append(valid_zmws[int(hole)])\n",
    "            dinucs.append(dinuc_vec)\n",
    "            n_mol +=1\n",
    "        elif len(tipds[int(hole)]) < length:\n",
    "            filler = np.empty(length - len(tipds[int(hole)]))\n",
    "            filler[:] = np.nan\n",
    "            lengths.append(len(read))\n",
    "            read = np.append(read, filler)\n",
    "            read_r = np.append(read_r, filler)\n",
    "            reads_f.append(read)\n",
    "            reads_r.append(read_r)\n",
    "            labs.append(dist[-1])\n",
    "            holes.append(valid_zmws[int(hole)])\n",
    "            dinucs.append(dinuc_vec)\n",
    "            n_mol +=1\n",
    "    new_mat = np.vstack(reads_f)\n",
    "    new_mat_r = np.vstack(reads_r)\n",
    "    dinuc_mat = np.vstack(dinucs)\n",
    "    return (new_mat, new_mat_r, lengths, labs,holes,dinucs)\n",
    "\n",
    "def process_xcors(new_mat, max_len=1000, lengths=False):\n",
    "    process = np.nan_to_num(pd.DataFrame(new_mat).rolling(33,axis=1, center=True, min_periods=1).mean())\n",
    "    if isinstance(lengths, np.ndarray):\n",
    "        auto_cor = xcor(process, process, maxlen=max_len, lengths=lengths)\n",
    "    else:\n",
    "        auto_cor = xcor(process, process, maxlen=max_len)\n",
    "    process = None\n",
    "    return auto_cor\n",
    "\n",
    "def cluster_mats(matrix, res=0.8,neighbors=15):\n",
    "    mat = scanpy.AnnData(X=np.nan_to_num(matrix))\n",
    "    scanpy.tl.pca(mat)\n",
    "    scanpy.pp.neighbors(mat,metric='correlation',n_neighbors=neighbors)\n",
    "    scanpy.tl.leiden(mat,resolution=res)\n",
    "    return (np.array(mat.obs['leiden']))\n",
    "\n",
    "def plotter(new_mat, auto_cor, clusters,smooth):\n",
    "    for i in np.unique(clusters):\n",
    "        print(i)\n",
    "        mat_new = auto_cor[clusters == i]\n",
    "        print(len(mat_new))\n",
    "        mat_new2 = new_mat[clusters == i]\n",
    "        mat_new2 = pd.DataFrame(mat_new2).rolling(smooth, axis=1, center=True, min_periods=1).mean()\n",
    "        plt.figure(figsize=(30,10))\n",
    "        subplot(121)\n",
    "        plt.plot(range(len(mat_new[0])), np.nanmean(mat_new, axis=0))\n",
    "        xlabel('Offset (bp)')\n",
    "        ylabel('Pearson\\'s r')\n",
    "        subplot(122)\n",
    "        plt.plot(range(len(mat_new2[0])), np.nanmean(mat_new2, axis=0))\n",
    "        xlabel('Distance to 5\\'-MNase cut')\n",
    "        ylabel('Average smoothed IPD (frames)')\n",
    "        plt.show()\n",
    "\n",
    "def binarize_sig(mat,cutoff=0.5):\n",
    "    test = np.copy(mat)\n",
    "    test[test < cutoff] = 0\n",
    "    test[test >= cutoff] = 1\n",
    "    return test\n",
    "\n",
    "def vec2ggplot(vecs, fho, label):\n",
    "    for i in range(len(vecs)):\n",
    "        print(\"%s\\t%s\\t%s\" % (i, vecs[i], label), file=fho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load up files\n",
    "\n",
    "This cell loads up the *in vivo* data from the pickle format generated by extractIPD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 1000\n",
    "# #BINARIZED\n",
    "k562_rep1 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/binarized/pbrun4_purple_k562_postlyso_chromatin_bingmm'\n",
    "k562_rep2 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/binarized/pbrun4_purple_k562_postlyso_chromatin_rep2_bingmm'\n",
    "k562_rep3 = '/avicenna/vramani/analyses/pacbio/pbrun6/processed/binarized/pbrun6_k562_postlyso_chromatin_bingmm'\n",
    "k562_rep4 = '/avicenna/vramani/analyses/pacbio/pbrun6/processed/binarized/pbrun6_k562_postlyso_chromatin_rep2_bingmm'\n",
    "\n",
    "k562_rep1_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/onlyT/pbrun4_purple_k562_postlyso_chromatin_onlyT'\n",
    "k562_rep2_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/onlyT/pbrun4_purple_k562_postlyso_chromatin_rep2_onlyT'\n",
    "k562_rep3_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun6/processed/onlyT/pbrun6_k562_postlyso_chromatin_onlyT'\n",
    "k562_rep4_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun6/processed/onlyT/pbrun6_k562_postlyso_chromatin_rep2_onlyT'\n",
    "\n",
    "k562_neg_rep1 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/binarized/pbrun4_purple_k562_postlyso_neg_bingmm'\n",
    "k562_neg_rep2 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/binarized/pbrun4_purple_k562_postlyso_neg_rep2_bingmm'\n",
    "k562_pos_rep1 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/binarized/pbrun4_purple_k562_postlyso_pos_bingmm'\n",
    "k562_pos_rep2 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/binarized/pbrun4_purple_k562_postlyso_pos_rep2_bingmm'\n",
    "\n",
    "k562_neg_rep1_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/onlyT/pbrun4_purple_k562_postlyso_neg_onlyT'\n",
    "k562_neg_rep2_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/onlyT/pbrun4_purple_k562_postlyso_neg_rep2_onlyT'\n",
    "k562_pos_rep1_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/onlyT/pbrun4_purple_k562_postlyso_pos_onlyT'\n",
    "k562_pos_rep2_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/onlyT/pbrun4_purple_k562_postlyso_pos_rep2_onlyT'\n",
    "\n",
    "\n",
    "# neg_rep1 = eat_pickle_binary(\"%s.pickle\" % k562_neg_rep1,  'K562_neg_Rep1')\n",
    "# neg_rep2 = eat_pickle_binary(\"%s.pickle\" % k562_neg_rep2,  'K562_neg_Rep2')\n",
    "# pos_rep1 = eat_pickle_binary(\"%s.pickle\" % k562_pos_rep1,  'K562_pos_Rep1')\n",
    "# pos_rep2 = eat_pickle_binary(\"%s.pickle\" % k562_pos_rep2,  'K562_pos_Rep2')\n",
    "# neg_rep1 = eat_pickle_binary(\"%s.pickle\" % k562_neg_rep1,  'K562_neg_Rep1')\n",
    "# neg_rep2 = eat_pickle_binary(\"%s.pickle\" % k562_neg_rep2,  'K562_neg_Rep2')\n",
    "# pos_rep1 = eat_pickle_binary(\"%s.pickle\" % k562_pos_rep1,  'K562_pos_Rep1')\n",
    "# pos_rep2 = eat_pickle_binary(\"%s.pickle\" % k562_pos_rep2,  'K562_pos_Rep2')\n",
    "\n",
    "\n",
    "#NONBINARIZED\n",
    "# k562_rep1 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/onlyT/pbrun4_purple_k562_postlyso_chromatin_onlyT'\n",
    "# k562_rep2 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/onlyT/pbrun4_purple_k562_postlyso_chromatin_rep2_onlyT'\n",
    "# k562_rep3 = '/avicenna/vramani/analyses/pacbio/pbrun6/processed/onlyT/pbrun6_k562_postlyso_chromatin_onlyT'\n",
    "# k562_rep4 = '/avicenna/vramani/analyses/pacbio/pbrun6/processed/onlyT/pbrun6_k562_postlyso_chromatin_rep2_onlyT'\n",
    "\n",
    "# k562_neg_rep1 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/onlyT/pbrun4_purple_k562_postlyso_neg_onlyT'\n",
    "# k562_neg_rep2 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/onlyT/pbrun4_purple_k562_postlyso_neg_rep2_onlyT'\n",
    "# # k562_neg_rep3 = '/avicenna/vramani/analyses/pacbio/pbrun6/processed/onlyT/pbrun6_k562_postlyso_neg_onlyT'\n",
    "# # k562_neg_rep4 = '/avicenna/vramani/analyses/pacbio/pbrun6/processed/onlyT/pbrun6_k562_postlyso_neg_rep2_onlyT'\n",
    "\n",
    "# rep1, rep1_valid = eat_pickle(\"%s.pickle\" % k562_rep1, \"%s_zmwinfo.pickle\" % k562_rep1)\n",
    "# rep2, rep2_valid = eat_pickle(\"%s.pickle\" % k562_rep2, \"%s_zmwinfo.pickle\" % k562_rep2)\n",
    "# rep3, rep3_valid = eat_pickle(\"%s.pickle\" % k562_rep3, \"%s_zmwinfo.pickle\" % k562_rep3)\n",
    "# rep4, rep4_valid = eat_pickle(\"%s.pickle\" % k562_rep4, \"%s_zmwinfo.pickle\" % k562_rep4)\n",
    "# neg_rep1, rep3_valid = eat_pickle(\"%s.pickle\" % k562_neg_rep1, \"%s_zmwinfo.pickle\" % k562_rep3)\n",
    "# neg_rep2, rep4_valid = eat_pickle(\"%s.pickle\" % k562_neg_rep2, \"%s_zmwinfo.pickle\" % k562_rep4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all files (direct to array)\n",
    "A version of the eat_pickle function that loads the data directly to an array instead of a dictionary. We include here two length cutoffs -- we ignore all molecules < 500 nt in length, and also only inspect the first 1000 bp of molecules from the 5' end of the unaligned CCS read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 1000\n",
    "rep1, l1, lb1, h1 = eat_pickle_b2m(\"%s.pickle\" % k562_rep1, 'K562_Rep1', length)\n",
    "rep1, l2, lb2, h2 = eat_pickle_b2m(\"%s.pickle\" % k562_rep2, 'K562_Rep2', length)\n",
    "rep1, l3, lb3, h3 = eat_pickle_b2m(\"%s.pickle\" % k562_rep3, 'K562_Rep3', length)\n",
    "rep1, l4, lb4, h4 = eat_pickle_b2m(\"%s.pickle\" % k562_rep4, 'K562_Rep4', length)\n",
    "#mat_total=np.vstack((rep1,rep2,rep3,rep4))\n",
    "#lengths_total = np.array(np.concatenate((l1,l2,l3,l4), axis=None))\n",
    "#lb_total = np.array(np.concatenate((lb1,lb2,lb3,lb4), axis=None))\n",
    "h_total = np.array(np.concatenate((h1,h2,h3,h4), axis=None))\n",
    "rep1, rep2, rep3, rep4 = [None, None, None, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell generates a hash table that connects ZMW hole numbers across multiple runs with their respective indices in the total array (useful for the Figure 5 subsetting analyses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zmw2index = {}\n",
    "idx = 0\n",
    "for i in range(len(h_total)):\n",
    "    zmw2index[h_total[i]] = idx\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the array to get around some of these memory leak issues\n",
    "In case memory is limiting, can save the length filtered IPD arrays to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('length_filtered_molecules', mat_total)\n",
    "np.save('length_filtered_lengths',lengths_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Analysis: Deciding on a cutoff to filter out unmethylated molecules\n",
    "We need to figure out a reasonable mean probability cutoff for each PacBio sequencing run to\n",
    "filter out molecules that don't seem to harbor much methylation with respect to negative controls. Let's visualize the distributions for each replicate and draw cutoffs from that. It's difficult to see an obvious cutoff in the data (despite it being clear that chromatin & controls are separated), so let's not draw any cutoffs at the moment, especially because we are interested in heterochromatin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "length = 1000\n",
    "\n",
    "# mp1 = return_probs(\"%s.pickle\" % k562_rep1)\n",
    "# mp2 = return_probs(\"%s.pickle\" % k562_rep2)\n",
    "# mp3 = return_probs(\"%s.pickle\" % k562_rep3)\n",
    "# mp4 = return_probs(\"%s.pickle\" % k562_rep4)\n",
    "# nmp1 = return_probs(\"%s.pickle\" % k562_neg_rep1)\n",
    "# nmp2 = return_probs(\"%s.pickle\" % k562_neg_rep2)\n",
    "# pmp1 = return_probs(\"%s.pickle\" % k562_pos_rep1)\n",
    "# pmp2 = return_probs(\"%s.pickle\" % k562_pos_rep2)\n",
    "\n",
    "# mat_rep1, mat_rep1_r, lengths1, mp1 = distill_tipds_tot(rep1, rep1_valid, length, min_len = 0, take_all = True)\n",
    "# mat_rep2, mat_rep2_r, lengths2, mp2 = distill_tipds_tot(rep2, rep1_valid, length, min_len = 0, take_all = True)\n",
    "# mat_rep3, mat_rep3_r, lengths3, mp3 = distill_tipds_tot(rep3, rep1_valid, length, min_len = 0, take_all = True)\n",
    "# mat_rep4, mat_rep4_r, lengths4, mp4 = distill_tipds_tot(rep4, rep1_valid, length, min_len = 0, take_all = True)\n",
    "# mat_neg_rep1, mat_neg_rep1_r, neg_lengths1, nmp1 = distill_tipds_tot(neg_rep1, rep1_valid, length, min_len = 0, take_all = True) \n",
    "# mat_neg_rep2, mat_neg_rep2_r, neg_lengths2, nmp2 = distill_tipds_tot(neg_rep2, rep1_valid, length, min_len = 0, take_all = True)\n",
    "# mat_pos_rep1, mat_pos_rep1_r, pos_lengths1, pmp1 = distill_tipds_tot(neg_rep1, rep1_valid, length, min_len = 0, take_all = True) \n",
    "# mat_pos_rep2, mat_pos_rep2_r, pos_lengths2, pmp2 = distill_tipds_tot(neg_rep2, rep1_valid, length, min_len = 0, take_all = True)\n",
    "\n",
    "print(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\" % (np.median(mp1),np.median(mp2),np.median(mp3),np.median(mp4),np.median(nmp1),np.median(nmp2),np.median(pmp1),np.median(pmp2)))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.distplot(mp1)\n",
    "sns.distplot(nmp1)\n",
    "sns.distplot(pmp1)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.distplot(mp2)\n",
    "sns.distplot(nmp2)\n",
    "sns.distplot(pmp2)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.distplot(mp3)\n",
    "sns.distplot(nmp1)\n",
    "sns.distplot(pmp1)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.distplot(mp4)\n",
    "sns.distplot(nmp2)\n",
    "sns.distplot(pmp2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2: Averages of the modification signal across the first 1 kb of K562 oligonucleosomes.\n",
    "Here we're sampling molecules to relate the average methylation signature on 1kb of template DNA. We load up the raw data (with length filtration), concatenate all of the resulting matrices from each of the four separate samples / runs, and then plot the NaN-sensitive mean over the matrix as a function of distance along the molecule. This should have the effect of revealing average nucleosome positions, as the termini of each sequenced molecule results from an MNase cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 1000\n",
    "mat_rep1, lengths1 = distill_tipds_tot(rep1, length, min_len = 500)\n",
    "mat_rep2, lengths2 = distill_tipds_tot(rep2, length, min_len = 500)\n",
    "mat_rep3, lengths3 = distill_tipds_tot(rep3, length, min_len = 500)\n",
    "mat_rep4, lengths4 = distill_tipds_tot(rep4, length, min_len = 500)\n",
    "mat_total = np.vstack((mat_rep1, mat_rep2, mat_rep3, mat_rep4))\n",
    "mat_rep1,mat_rep2,mat_rep3,mat_rep4 = [None, None, None, None]\n",
    "lengths_total = np.concatenate((lengths1,lengths2, lengths3, lengths4), axis=None)\n",
    "lengths1,lengths2,lengths3,lengths4 = [None,None,None,None]\n",
    "rep1, rep2, rep3, rep4 = [None,None,None,None]\n",
    "#Controls for plotting the below cell\n",
    "# mat_neg_rep1, neg_lengths1 = distill_tipds_tot(neg_rep1, length, min_len = 500)\n",
    "# mat_neg_rep2, neg_lengths2 = distill_tipds_tot(neg_rep2, length, min_len = 500)\n",
    "# mat_neg_rep1 = np.vstack((mat_neg_rep1,mat_neg_rep2))\n",
    "# neg_lengths1 = np.append(neg_lengths1,neg_lengths2)\n",
    "# mat_neg_rep2 = 0\n",
    "# neg_lengths2 = 0\n",
    "# mat_pos_rep1, pos_lengths1 = distill_tipds_tot(pos_rep1, length, min_len = 500) \n",
    "# mat_pos_rep2, pos_lengths2 = distill_tipds_tot(pos_rep2, length, min_len = 500)\n",
    "# mat_pos_rep1 = np.vstack((mat_pos_rep1,mat_pos_rep2))\n",
    "# pos_lengths1 = np.append(pos_lengths1,pos_lengths2)\n",
    "# mat_pos_rep2 = 0\n",
    "# pos_lengths2 = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're just creating a text file to easily plot the results in ggplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,20))\n",
    "# plt.plot(range(len(mat_rep1[0])), np.nanmean(mat_rep1,axis=0))\n",
    "# plt.plot(range(len(mat_rep1[0])), np.nanmean(mat_rep2,axis=0))\n",
    "# plt.plot(range(len(mat_rep1[0])), np.nanmean(mat_neg_rep1,axis=0))\n",
    "# plt.plot(range(len(mat_rep1[0])), np.nanmean(mat_pos_rep1,axis=0))\n",
    "# plt.show()\n",
    "\n",
    "fho = open('Final_ggplot_Fig2C.txt','w')\n",
    "vec2ggplot(np.nanmean(mat_rep1,axis=0), fho, 'Replicate 1')\n",
    "vec2ggplot(np.nanmean(mat_rep2,axis=0), fho, 'Replicate 2')\n",
    "vec2ggplot(np.nanmean(mat_neg_rep1,axis=0), fho, '(-) Control')\n",
    "vec2ggplot(np.nanmean(mat_pos_rep1,axis=0), fho, '(+) Control')\n",
    "fho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary Figure 6: ChromHMM Coverage Enrichment Analysis\n",
    "We downloaded K562 ChromHMM labels from the UCSC Genome Browser, lifted over coordinates to hg38 using the  ```liftOver``` tool, and then used ```bedtools multicov``` to compute the read coverage for each ChromHMM BED entry. We then read this file in as a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_file = pd.read_table('cov_check.bed',header=None)\n",
    "hc_file.columns = ['chrid','start','end','chromhmm_label','c2','c3','c4','c5','c6','r1_rep1','r1_rep2','r2_rep1','r2_rep2','mnase','dnase','shotgun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next use this dataframe to estimate the relative enrichment / depletion of each label in each dataset. The datasets we use in addition to our in-house SAMOSA data, we use aligned ENCODE DNaseI-seq data (accession: ENCFF156LGK), in-house aligned K562 MNase-seq data (PMID: 27151365), and publicly released whole-genome shotgun sequencing CCS data from PacBio (https://downloads.pacbcloud.com/public/dataset/HG002_SV_and_SNV_CCS/consensusreads/). We estimated enrichment by evaluating normalizing the number of reads mapping to each ChromHMM labeled genomic bin, dividing this value by the total number of bins with that ChromHMM label, and taking the natural log. We then plotted this data in heatmap form using ggplot2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_cov_expt = np.sum(hc_file['r1_rep1']) +\\\n",
    "    np.sum(hc_file['r1_rep2']) + np.sum(hc_file['r2_rep1']) + np.sum(hc_file['r2_rep2'])\n",
    "print(tot_cov_expt)\n",
    "tot_cov_mnase = np.sum(hc_file['mnase'])\n",
    "print(tot_cov_mnase)\n",
    "tot_cov_dnase = np.sum(hc_file['dnase'])\n",
    "print(tot_cov_dnase)\n",
    "tot_cov_shotgun = np.sum(hc_file['shotgun'])\n",
    "print(tot_cov_shotgun)\n",
    "fho = open('supp_figure_enrichments.txt','w')\n",
    "for i in np.unique(hc_file['chromhmm_label']):\n",
    "    if i[:3] == 'Gen':continue\n",
    "    print(i)\n",
    "    cluster_sub = hc_file[hc_file['chromhmm_label'] == i]\n",
    "    cluster_frac = len(cluster_sub) / len(hc_file)\n",
    "    cluster_cov_expt = np.sum(cluster_sub['r1_rep1']) +\\\n",
    "        np.sum(cluster_sub['r1_rep2']) + np.sum(cluster_sub['r2_rep1']) + np.sum(cluster_sub['r2_rep2'])\n",
    "    cluster_cov_mnase = np.sum(cluster_sub['mnase'])\n",
    "    cluster_cov_dnase = np.sum(cluster_sub['dnase'])\n",
    "    cluster_cov_shotgun = np.sum(cluster_sub['shotgun'])\n",
    "    cluster_log_odds = np.log(cluster_cov_expt / tot_cov_expt / cluster_frac)\n",
    "    mnase_log_odds = np.log(cluster_cov_mnase / tot_cov_mnase / cluster_frac)\n",
    "    dnase_log_odds = np.log(cluster_cov_dnase / tot_cov_dnase / cluster_frac)\n",
    "    shotgun_log_odds = np.log(cluster_cov_shotgun / tot_cov_shotgun / cluster_frac)\n",
    "    odds = [cluster_log_odds, mnase_log_odds, dnase_log_odds, shotgun_log_odds]\n",
    "    labels = ['SAMOSA','MNase-seq','DNase-seq','Shotgun CCS']\n",
    "    for j in range(len(odds)):\n",
    "        print(\"%s\\t%s\\t%s\" % (i,odds[j],labels[j]), file = fho)\n",
    "fho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3: Clustering analysis of all chromatin molecules >=500 bp in length\n",
    "We used Leiden clustering (PMID: 30914743) to take all molecules in our dataset passing our lower length cutoff and subjected them to unbiased clustering. Resolution and n_neighbors were manually adjusted to avoid generating large numbers of very small clusters (i.e. < 100 molecules). All parameters used for plotting figures in the paper are recapitulated in the Jupyter notebook. Our clustering strategy was as follows: first, we smoothed raw signal matrices with a 33 bp NaN-sensitive running mean. We next computed the autocorrelation function for each molecule in the matrix, using the full length of the molecule up to 1000 bp. We then used Scanpy (PMID: 29409532) to perform Leiden clustering on the resulting matrix. We visualized the resulting cluster averages with respect to the average autocorrelation function, and with respect to averaged modification probabilities for each cluster. For a subset of clusters we also randomly sampled n molecules to directly visualize in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auto_cor = np.load('length_filtered_auto_cor.npy')\n",
    "np.nan_to_num(auto_cor, copy=False)\n",
    "mat = scanpy.AnnData(X=auto_cor[:,:500])\n",
    "scanpy.tl.pca(mat)\n",
    "scanpy.pp.neighbors(mat,metric='correlation',n_neighbors=10)\n",
    "scanpy.tl.leiden(mat,resolution=0.4)\n",
    "clusters = np.array(mat.obs['leiden'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cluster labels to text file\n",
    "We saved the cluster labels to a text file for later access, as clustering took > 12 hours on our server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fho = open('final_cluster_labels_total.txt','w')\n",
    "for i in range(len(clusters)):\n",
    "    print(\"%s\\t%s\" % (i, clusters[i]), file = fho)\n",
    "\n",
    "fho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3: Inspect clusters, compute single-molecule autocorrelograms.\n",
    "We computed single-molecule autocorrelograms and discovered peaks on these autocorrelograms as follows: for each molecule, we used the scipy ```find_peaks``` function to in the computed autocorrelogram and annotated the location of that peak. We also kept track of the molecules where ```find_peaks``` could not detect a peak using the given parameters (optimized manually by modifying peak height / width to detect peaks on the averaged autocorrelograms; detection of a peaks between 180-190 bp, the expected NRL in human cells suggests that these parameters are sound). For each collection of single-molecule autocorrelogram peaks we computed the median, the median absolute deviation, and visualized the distribution of peak locations as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def peak_cruncher(ac):\n",
    "    peaks1 = []\n",
    "    for i in range(len(ac)):\n",
    "        mol1 = ac[i]\n",
    "        peak1, data = signal.find_peaks(mol1, height = 0.10, width = 25)\n",
    "        if len(peak1) == 0:\n",
    "            ac1p = np.nan\n",
    "        else:\n",
    "            ac1p = int(peak1[0])\n",
    "        peaks1.append(ac1p)\n",
    "    peaks1 = np.array(peaks1)\n",
    "    med1 = np.nanmedian(peaks1)\n",
    "    mad1 = scipy.stats.median_absolute_deviation(peaks1,nan_policy='omit')\n",
    "    mis1 = np.count_nonzero(np.isnan(peaks1))\n",
    "    return (peaks1, med1, mad1,mis1)\n",
    "\n",
    "cluster_file = open('final_cluster_labels_total.txt')\n",
    "clusters = []\n",
    "for line in cluster_file:\n",
    "    split = line.split()\n",
    "    clusters.append(split[1])\n",
    "clusters = np.array(clusters)\n",
    "print(clusters)\n",
    "smooth = 33\n",
    "print(\"this happens\")\n",
    "fho1 = open('Fig3_hmaps.txt','w')\n",
    "fho2 = open('Fig3_autocors.txt','w')\n",
    "fho3 = open('Fig3_distros.txt','w')\n",
    "fho4 = open('Fig3_sample_hmaps.txt', 'w')\n",
    "for i in np.unique(clusters):\n",
    "    if i == '7': continue\n",
    "    mat_total = np.load('length_filtered_molecules.npy')[clusters == i]\n",
    "    mat_total = np.nanmean(pd.DataFrame(mat_total).rolling(smooth, axis=1, center=True, min_periods=1).mean(),axis=0)\n",
    "    auto_cor = np.load('length_filtered_auto_cor.npy')[clusters == i]\n",
    "    peaks1, med1, mad1, mis1 = peak_cruncher(auto_cor)\n",
    "    overall_peak, data = signal.find_peaks(np.nanmean(auto_cor,axis=0), height = 0.10, width = 25) \n",
    "    auto_cor = np.nanmean(auto_cor,axis=0)\n",
    "    for j in range(len(mat_total)):\n",
    "        print(\"%s\\t%s\\t%s\" % (i, j, mat_total[j]), file=fho1)\n",
    "    for j in range(len(auto_cor)):\n",
    "        print(\"%s\\t%s\\t%s\" % (i, j, auto_cor[j]), file=fho2)\n",
    "    print(len(mat_total))\n",
    "    print(i)\n",
    "    for j in range(len(peaks1)):\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\" % (i, peaks1[j], overall_peak, mis1, len(mat_total)), file=fho3)\n",
    "    if len(overall_peak) == 0:\n",
    "        overall_peak = 0\n",
    "    else:\n",
    "        overall_peak = overall_peak[0]\n",
    "    print(\"%s\\t%s\\t%s\\t%s\" % (med1,mad1,mis1,overall_peak))\n",
    "    plt.figure(figsize=(30,10))\n",
    "    subplot(131)\n",
    "    plt.plot(range(1000), auto_cor)\n",
    "    xlabel('Offset (bp)')\n",
    "    ylabel('Pearson\\'s r')\n",
    "    subplot(132)\n",
    "    plt.plot(range(1000), mat_total)\n",
    "    xlabel('Distance to 5\\'-MNase cut')\n",
    "    ylabel('Average smoothed IPD (frames)')\n",
    "    subplot(133)\n",
    "    sns.distplot(peaks1[~np.isnan(peaks1)])\n",
    "    axvline(overall_peak)\n",
    "    plt.show()\n",
    "#     if \n",
    "#     for j in range(len(mat_total)):\n",
    "#         for k in range(len(mat_total[j])):\n",
    "#             print(\"%s\\t%s\\t%s\" % (i, j, mat_total[j][k]), file=fho1)        \n",
    "            \n",
    "fho1.close()\n",
    "fho2.close()\n",
    "fho3.close()\n",
    "fho4.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write heatmap files to ggplottable txt file for easy data frame manipulation.\n",
    "We wrote the resulting signal averages to a text file for easy import to R or Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_cruncher(ac):\n",
    "    peaks1 = []\n",
    "    for i in range(len(ac)):\n",
    "        mol1 = ac[i]\n",
    "        peak1, data = signal.find_peaks(mol1, height = 0.10, width = 25)\n",
    "        if len(peak1) == 0:\n",
    "            ac1p = np.nan\n",
    "        else:\n",
    "            ac1p = int(peak1[0])\n",
    "        peaks1.append(ac1p)\n",
    "    peaks1 = np.array(peaks1)\n",
    "    med1 = np.nanmedian(peaks1)\n",
    "    mad1 = scipy.stats.median_absolute_deviation(peaks1,nan_policy='omit')\n",
    "    mis1 = np.count_nonzero(np.isnan(peaks1))\n",
    "    return (peaks1, med1, mad1,mis1)\n",
    "\n",
    "cluster_file = open('final_cluster_labels_total.txt')\n",
    "lengths_total = np.load('length_filtered_lengths.npy')\n",
    "clusters = []\n",
    "for line in cluster_file:\n",
    "    split = line.split()\n",
    "    clusters.append(split[1])\n",
    "clusters = np.array(clusters)\n",
    "print(clusters)\n",
    "smooth = 33\n",
    "print(\"this happens\")\n",
    "\n",
    "fho4 = open('Fig3_sample_hmaps.txt', 'w')\n",
    "for i in np.unique(clusters):\n",
    "    if i == '0' or i == '2' or i == '6':\n",
    "        mat_total = np.load('length_filtered_molecules.npy')[(clusters == i) & (lengths_total >= 1000)]\n",
    "        mat_total = pd.DataFrame(mat_total).rolling(smooth, axis=1, center=True, min_periods=1).mean().values\n",
    "        print(len(mat_total))\n",
    "        idx = np.random.choice(mat_total.shape[0], 5000, replace=False)\n",
    "        mat_total = mat_total[idx,:]\n",
    "        for j in range(len(mat_total)):\n",
    "            for k in range(len(mat_total[j])):\n",
    "                print(\"%s\\t%s\\t%s\\t%s\" % (i, j, k, mat_total[j][k]), file=fho4)\n",
    "fho4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mat_total.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3: Cluster Statistics\n",
    "We'd like to define the 1.) average modification probability, 2.) cluster percentage, and 3.) cluster NRL distributions which we visualize as a violin plot, stacked bar plot, and violin plot, respectively. To aid in visualizing all of these easily, we write all of these values to a single txt file that can be loaded into R or Pandas.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lb_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_total = np.load('length_filtered_molecules.npy')\n",
    "cluster_file = open('final_cluster_labels_total.txt')\n",
    "lengths_total = np.load('length_filtered_lengths.npy')\n",
    "clusters = []\n",
    "for line in cluster_file:\n",
    "    split = line.split()\n",
    "    clusters.append(split[1])\n",
    "clusters = np.array(clusters)\n",
    "mat_total = np.nanmean(mat_total, axis=1)\n",
    "fho = open('Fig3BCD.txt','w')\n",
    "for i in range(len(clusters)):\n",
    "    if clusters[i] == '7': continue\n",
    "    print(\"%s\\t%s\\t%s\\t%s\" % (clusters[i], lengths_total[i], lb_total[i], mat_total[i]), file=fho)\n",
    "fho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Analysis: Fragment length fold enrichment. \n",
    "This analysis did not make it into the paper, but we also computed the relative enrichment / depletion of different fragment lengths as in Risca *et al* (PMID: 28024297) and Ramani *et al* (PMID: 30811994) to determine what fragment length classes were favored across each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_table('Fig3BCD.txt')\n",
    "frame.columns = ['clust_label','length','replicate','methyl_mean']\n",
    "bg_fld = frame[frame['length'] <= 2000]['length'].value_counts() / len(frame[frame['length'] <= 2000]['length'])\n",
    "bg_fld = dict(zip(bg_fld.index.values, bg_fld.values))\n",
    "tot_mat = []\n",
    "fho = open('FLFE_Fig4.txt','w')\n",
    "for i in np.unique(frame['clust_label']):\n",
    "    clust_vals = []\n",
    "    sub_clust = frame[(frame['length'] <= 2000) & (frame['clust_label'] == i)]['length']\n",
    "    clust_fld = sub_clust.value_counts() / len(sub_clust)\n",
    "    clust_fld = dict(zip(clust_fld.index.values, clust_fld.values))\n",
    "    for j in range(500,2000):\n",
    "        if j in clust_fld:\n",
    "            val = np.log2(clust_fld[j] / bg_fld[j])\n",
    "            clust_vals.append(np.log2(clust_fld[j] / bg_fld[j]))\n",
    "        else:\n",
    "            val = np.nan\n",
    "        print(\"%s\\t%s\\t%s\" % (i, j, val), file=fho)\n",
    "    tot_mat.append(clust_vals)\n",
    "fho.close()\n",
    "# tot_mat = np.vstack(tot_mat)\n",
    "# plt.figure(figsize=(20,20))\n",
    "# imshow(tot_mat, cmap=cm.RdBu_r,aspect=100)\n",
    "# colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster fractions\n",
    "for i in np.unique(frame['clust_label']):\n",
    "    print(i)\n",
    "    print(len(frame[(frame['clust_label'] == i)]['clust_label']) / len(frame['clust_label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 4: Transcription factor binding motif analyses.\n",
    "Transcription factor binding sites were obtained as in Ramani et al. Briefly, we downloaded IDR-filtered ENCODE ChIP-seq peaks for CTCF, NRF1, REST, c-MYC, PU.1, and GATA1, and then used FIMO to predict TF binding sites within these peaks using CISTROME PWM definitions for each transcription factor. For MNase-cleavage analyses, we plotted the abundance of MNase cuts (2 per molecule) with respect to TF binding sites and plotted these as number of cleavages per molecules sequenced.\n",
    "\n",
    "\n",
    "To examine modification probabilities around TF binding sites, we wrote a custom script (```zmw_selector.py```) to find the ZMWs that overlap with features of interest (e.g. transcription factor binding sites). We extracted all ZMWs where a portion of the read alignment falls within 1 kb of a given feature, and annotated the position of the alignment starts, ends, and strand with respect to the feature. We then used these coordinates and strand information to extract all modification signal falling within a 500 bp window centered at each TF binding site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data as dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k562_rep1 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/binarized/pbrun4_purple_k562_postlyso_chromatin_bingmm'\n",
    "k562_rep2 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/binarized/pbrun4_purple_k562_postlyso_chromatin_rep2_bingmm'\n",
    "k562_rep3 = '/avicenna/vramani/analyses/pacbio/pbrun6/processed/binarized/pbrun6_k562_postlyso_chromatin_bingmm'\n",
    "k562_rep4 = '/avicenna/vramani/analyses/pacbio/pbrun6/processed/binarized/pbrun6_k562_postlyso_chromatin_rep2_bingmm'\n",
    "\n",
    "rep1 = eat_pickle_binary(\"%s.pickle\" % k562_rep1,  'K562_Rep1')\n",
    "rep2 = eat_pickle_binary(\"%s.pickle\" % k562_rep2,  'K562_neg_Rep2')\n",
    "rep3 = eat_pickle_binary(\"%s.pickle\" % k562_rep3,  'K562_pos_Rep1')\n",
    "rep4 = eat_pickle_binary(\"%s.pickle\" % k562_rep4,  'K562_pos_Rep2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNase-cleavage analysis around transcription factor binding sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs = open('./pbrun4_purple/biology_analyses/tfs.txt')\n",
    "#tfs = ['CTCF\\n']\n",
    "tot_zmws = 247302 + 226280 + 732496 + 747401\n",
    "for line in tfs:\n",
    "    tf = \"%s_flat\" % (line.split()[0])\n",
    "    test_file1 = open('./pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % tf, 'r')\n",
    "    test_file2 = open('./pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % tf, 'r')\n",
    "    test_file3 = open('./pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % tf, 'r')\n",
    "    test_file4 = open('./pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % tf, 'r')\n",
    "    files = [test_file1,test_file2,test_file3,test_file4]\n",
    "    #files = [test_file1,test_file2]\n",
    "    length = 1000\n",
    "    meta = np.zeros(length)\n",
    "    for test_file in files:\n",
    "        for line in test_file:\n",
    "            split = line.split()\n",
    "            start = int(split[2])\n",
    "            end = int(split[3])\n",
    "            frag = int(split[4])\n",
    "            if split[-1] == '+':\n",
    "                dist1 = start - frag + int(length / 2)\n",
    "                dist2 = end - frag + int(length / 2)\n",
    "                if 0 <= dist1 < length:\n",
    "                    meta[dist1] += 1\n",
    "                if 0 <= dist2 < length:\n",
    "                    meta[dist2] += 1\n",
    "            else:\n",
    "                dist1 = frag - start + int(length / 2)\n",
    "                dist2 = frag - end + int(length / 2)\n",
    "                if 0 <= dist1 < length:\n",
    "                    meta[dist1] += 1\n",
    "                if 0 <= dist2 < length:\n",
    "                    meta[dist2] += 1\n",
    "    meta=pd.Series(meta).rolling(5,center=True, min_periods=1).mean()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(range(len(meta)), meta)\n",
    "    plt.show()\n",
    "    for test_file in files:\n",
    "        test_file.close()\n",
    "    fho = open('%s_MNase.txt' % tf,'w')\n",
    "    for i in range(len(meta)):\n",
    "        print(\"%s\\t%s\\t%s\" % (i, meta[i] / tot_zmws * 1000000, tf), file=fho)\n",
    "    fho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Modification probability analysis around six transcription factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 500\n",
    "\n",
    "tfs = open('./pbrun4_purple/biology_analyses/tfs.txt')\n",
    "#tfs = ['GATA1\\n']\n",
    "lengths_total = np.array([0])\n",
    "labels_total = np.array([0])\n",
    "mat_total = []\n",
    "\n",
    "for line in tfs:\n",
    "    tf = line.split()[0]\n",
    "    sample = '%s' % tf\n",
    "    sites_rep1 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_flat_zmws' % sample\n",
    "    sites_rep2 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_flat_zmws' % sample\n",
    "    sites_rep3 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_flat_zmws' % sample\n",
    "    sites_rep4 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_flat_zmws' % sample\n",
    "    cntrl_sites_rep1 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_random_flat_zmws' % sample\n",
    "    cntrl_sites_rep2 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_random_flat_zmws' % sample\n",
    "    cntrl_sites_rep3 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_random_flat_zmws' % sample\n",
    "    cntrl_sites_rep4 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_random_flat_zmws' % sample\n",
    "    mat_rep1, lengths, labels = distill_tipds_flat_centered(rep1, sites_rep1, length, sample)\n",
    "    \n",
    "    cntrl_mat_rep1, cntrl_lengths, cntrl_labels = distill_tipds_flat_centered(rep1, cntrl_sites_rep1, length, \"%s_control\" % sample ,subsample=len(lengths))    \n",
    "    \n",
    "    mat_rep2, lengths2, labels2 = distill_tipds_flat_centered(rep2,  sites_rep2, length, sample)\n",
    "    \n",
    "    cntrl_mat_rep2, cntrl_lengths2, cntrl_labels2 = distill_tipds_flat_centered(rep2,  cntrl_sites_rep2, length, \"%s_control\" % sample,subsample=len(lengths2))    \n",
    "    \n",
    "    mat_rep3, lengths3, labels3 = distill_tipds_flat_centered(rep3,  sites_rep3, length, sample)\n",
    "    \n",
    "    cntrl_mat_rep3, cntrl_lengths3, cntrl_labels3 = distill_tipds_flat_centered(rep3,cntrl_sites_rep3, length, \"%s_control\" % sample,subsample=len(lengths3))    \n",
    "    \n",
    "    mat_rep4, lengths4, labels4 = distill_tipds_flat_centered(rep4, sites_rep4, length, sample)\n",
    "    \n",
    "    cntrl_mat_rep4, cntrl_lengths4, cntrl_labels4 = distill_tipds_flat_centered(rep4, cntrl_sites_rep4, length, \"%s_control\" % sample,subsample=len(lengths4))    \n",
    "    \n",
    "    mat_total.extend((mat_rep1,mat_rep2,mat_rep3,mat_rep4,cntrl_mat_rep1,cntrl_mat_rep2,cntrl_mat_rep3,cntrl_mat_rep4))\n",
    "#    mat_total.extend((mat_rep1,mat_rep2,mat_rep3,mat_rep4))\n",
    "    mat_msix = np.vstack((mat_rep1, mat_rep2, mat_rep3, mat_rep4))\n",
    "    mat_msix_control = np.vstack((cntrl_mat_rep1, cntrl_mat_rep2, cntrl_mat_rep3, cntrl_mat_rep4))\n",
    "    print(\"%s\\t%s\" % (sample, (len(mat_rep1)+len(mat_rep2)+len(mat_rep3)+len(mat_rep4))))\n",
    "    print(\"%s_control\\t%s\" % (sample, (len(cntrl_mat_rep1)+len(cntrl_mat_rep2)+len(cntrl_mat_rep3)+len(cntrl_mat_rep4))))\n",
    "    lengths_total = np.concatenate((lengths_total, lengths, lengths2,lengths3,lengths4, cntrl_lengths, cntrl_lengths2,cntrl_lengths3,cntrl_lengths4))\n",
    "    labels_total = np.concatenate((labels_total, labels, labels2,labels3,labels4,cntrl_labels, cntrl_labels2, cntrl_labels3, cntrl_labels4))\n",
    "#     lengths_total = np.concatenate((lengths_total, lengths, lengths2,lengths3,lengths4))\n",
    "#     labels_total = np.concatenate((labels_total, labels, labels2,labels3,labels4))\n",
    "    process = pd.Series(np.nanmean(mat_msix, axis=0)).rolling(33,center=True,min_periods=1).mean()\n",
    "    process_neg = pd.Series(np.nanmean(mat_msix_control, axis=0)).rolling(33,center=True,min_periods=1).mean()\n",
    "#     process = np.nanmean(binarize_sig(pd.DataFrame(mat_msix).rolling(10, axis=1, center=True, min_periods=1).mean()), axis=0)\n",
    "#     process_neg = np.nanmean(binarize_sig(pd.DataFrame(mat_msix_control).rolling(10, axis=1, center=True, min_periods=1).mean()), axis=0)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    subplot(121)\n",
    "    plt.plot(range(len(process)), process)\n",
    "    subplot(122)\n",
    "    plt.plot(range(len(process_neg)), process_neg)\n",
    "    plt.show()\n",
    "    fho=open('%s_m6A.txt' % tf,'w')\n",
    "    for i in range(len(process)):\n",
    "        print(\"%s\\t%s\\t%s\\tsite\" % (i, process[i], tf),file=fho)\n",
    "        print(\"%s\\t%s\\t%s\\tcontrol\" % (i, process_neg[i], tf),file=fho)\n",
    "    fho.close()\n",
    "\n",
    "mat_total = np.vstack(mat_total)\n",
    "lengths_total = lengths_total[1:]\n",
    "labels_total = labels_total[1:]\n",
    "\n",
    "tfs.close()\n",
    "\n",
    "#np.save('length_filtered_molecules_tfs',mat_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 4: Leiden clustering of molecules with TF binding motifs.\n",
    "Unlike the prior analysis, we do *not* cluster molecules on the basis of the autocorrelation function, and instead use the smoothed modification probabilities instead. We reason that while in the prior case we are explicitly interested in nucleosome regularity, in this case we do not have a strong prior assumption that nucleosomes will be regular or irregular about TF binding sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_cor = np.load('length_filtered_molecules_tfs.npy')\n",
    "auto_cor = pd.DataFrame(auto_cor).rolling(33, axis=1, center=True, min_periods=1).mean()\n",
    "print(len(auto_cor))\n",
    "print(len(auto_cor[0]))\n",
    "mat = scanpy.AnnData(X=np.nan_to_num(auto_cor,copy=False))\n",
    "scanpy.tl.pca(mat)\n",
    "scanpy.pp.neighbors(mat,metric='correlation',n_neighbors=10)\n",
    "scanpy.tl.leiden(mat,resolution=0.7)\n",
    "clusters = np.array(mat.obs['leiden'])\n",
    "\n",
    "fho = open('final_cluster_labels_tfs.txt','w')\n",
    "for i in range(len(clusters)):\n",
    "    print(\"%s\" % clusters[i], file=fho)\n",
    "fho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the resulting clusters and write data to easy-to-plot text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "smooth = 33\n",
    "for i in np.unique(clusters):\n",
    "    mat_total = np.load('length_filtered_molecules_tfs.npy')[clusters == i]\n",
    "    auto_cor = np.load('length_filtered_auto_cor_tfs.npy')[clusters == i]\n",
    "    print(len(mat_total))\n",
    "    print(i)\n",
    "    mat_total = pd.DataFrame(mat_total).rolling(smooth, axis=1, center=True, min_periods=1).mean()\n",
    "    plt.figure(figsize=(30,10))\n",
    "    subplot(121)\n",
    "    plt.plot(range(251), np.nanmean(auto_cor, axis=0))\n",
    "    xlabel('Offset (bp)')\n",
    "    ylabel('Pearson\\'s r')\n",
    "    subplot(122)\n",
    "    plt.plot(range(500), np.nanmean(mat_total, axis=0))\n",
    "    xlabel('Distance to 5\\'-MNase cut')\n",
    "    ylabel('Average smoothed IPD (frames)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_file = open('final_cluster_labels_tfs.txt')\n",
    "clusters = []\n",
    "for line in cluster_file:\n",
    "    split = line.split()\n",
    "    clusters.append(split[0])\n",
    "clusters = np.array(clusters)\n",
    "print(clusters)\n",
    "smooth = 33\n",
    "print(\"this happens\")\n",
    "fho1 = open('Fig4_hmaps.txt','w')\n",
    "#fho2 = open('Fig4_autocors.txt','w')\n",
    "#fho3 = open('Fig3_distros.txt','w')\n",
    "fho4 = open('Fig4_sample_hmaps.txt', 'w')\n",
    "for i in np.unique(clusters):\n",
    "    mat_total = np.load('length_filtered_molecules_tfs.npy')[(clusters == i)]\n",
    "    mat_total = pd.DataFrame(mat_total).rolling(smooth, axis=1, center=True, min_periods=1).mean().values\n",
    "    if i == '2' or i == '7' or i == '11':\n",
    "        print(len(mat_total))\n",
    "        idx = np.random.choice(mat_total.shape[0], 500, replace=False)\n",
    "        mat_total = mat_total[idx,:]\n",
    "        for j in range(len(mat_total)):\n",
    "            for k in range(len(mat_total[j])):\n",
    "                print(\"%s\\t%s\\t%s\\t%s\" % (i, j, k, mat_total[j][k]), file=fho4)\n",
    "#    if i == '7': continue\n",
    "    mat_total=np.nanmean(mat_total,axis=0)\n",
    "    for j in range(len(mat_total)):\n",
    "        print(\"%s\\t%s\\t%s\" % (i, j, mat_total[j]), file=fho1)\n",
    "    print(len(mat_total))\n",
    "    print(i)\n",
    "   \n",
    "fho1.close()\n",
    "fho4.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary Figure 9: Enrichment Tests for TF Binding\n",
    "We constructed a series of enrichment tests (Fisher's Exact) to determine odds ratios / p-values to find specific cluster label--transcription factor pairs that were enriched with respect to the total set of all labeled molecules. Finally, we used the Storey q-value package to correct for the number of Fisher's exact tests performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fho = open('fishers_tests_fig3.txt','w')\n",
    "cluster_df = {'clusters':clusters, 'labels':labels_total}\n",
    "cluster_df = pd.DataFrame(cluster_df)\n",
    "for i in np.unique(clusters):\n",
    "    for j in np.unique(labels_total):\n",
    "        num_clust_lab = len(cluster_df[(cluster_df['clusters'] == i) & (cluster_df['labels'] == j)])\n",
    "        num_clust_notlab = len(cluster_df[(cluster_df['clusters'] == i) & (cluster_df['labels'] != j)])\n",
    "        num_notclust_lab = len(cluster_df[(cluster_df['clusters'] != i) & (cluster_df['labels'] == j)])\n",
    "        num_notclust_notlab = len(cluster_df[(cluster_df['clusters'] != i) & (cluster_df['labels'] != j)])\n",
    "        odds_r, pval = sp.stats.fisher_exact([[num_clust_lab, num_notclust_lab], \\\n",
    "            [num_clust_notlab, num_notclust_notlab]])\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\" % (i, j, num_clust_lab, odds_r, pval), file=fho)\n",
    "fho.close()\n",
    "#labs = np.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df.to_csv('tfs_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5: Enrichment Tests for Chromatin States\n",
    "We used a custom python script (```zmw_selector_bed.py```) or directly scanned for satellite-containing CCS reads (see Methods section) to extract molecules that fall within ENCODE-defined chromatin states / pertain to human major satellite sequences. We then used the above defined hashmap linking ZMW IDs to indices along the total matrix of molecules to link Cluster IDs and chromatin states. Finally, we constructed a series of enrichment tests (Fisher's Exact) to determine odds ratios / p-values to find specific cluster label--chromatin state pairs that were enriched with respect to the total set of all labeled molecules. Finally, we used the Storey q-value package to correct for the number of Fisher's exact tests performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(sites, zmw_dict, samp_label, chrom_label):\n",
    "    lines = open(sites)\n",
    "    indices = []\n",
    "    for entry in lines:\n",
    "        split = entry.split()\n",
    "        hole = \"%s_%s\" % (samp_label, split[0])\n",
    "        if hole in zmw_dict:\n",
    "            indices.append(zmw_dict[hole])\n",
    "    return indices\n",
    "        \n",
    "#print(list(zmw2index.keys())[:100])\n",
    "cluster_file = open('final_cluster_labels_total.txt')\n",
    "lengths_total = np.load('length_filtered_lengths.npy')\n",
    "clusters = []\n",
    "for line in cluster_file:\n",
    "    split = line.split()\n",
    "    clusters.append(split[1])\n",
    "clusters = np.array(clusters)\n",
    "\n",
    "#total_dataframe = pd.DataFrame({'cluster_id': [], 'chromatin_type': [], 'total_index': [], 'frag_length' : []})\n",
    "cluster_ids = []\n",
    "chromatin_types = []\n",
    "total_index = []\n",
    "frag_length = []\n",
    "\n",
    "tfs = open('./pbrun4_purple/biology_analyses/new_beds.txt')\n",
    "#tfs = ['coopfoots\\n']\n",
    "\n",
    "for line in tfs:\n",
    "    tf = line.split()[0]\n",
    "    sample = '%s' % tf\n",
    "    print(sample)\n",
    "    sites_rep1 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "    sites_rep2 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "    sites_rep3 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "    sites_rep4 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "    rep1_indices = find_indices(sites_rep1, zmw2index, 'K562_Rep1', tf)\n",
    "    rep2_indices = find_indices(sites_rep2, zmw2index, 'K562_Rep2', tf)\n",
    "    rep3_indices = find_indices(sites_rep3, zmw2index, 'K562_Rep3', tf)\n",
    "    rep4_indices = find_indices(sites_rep4, zmw2index, 'K562_Rep4', tf)\n",
    "    tot_indices = np.concatenate((rep1_indices,rep2_indices,rep3_indices,rep4_indices))    \n",
    "    cluster_indices = clusters[tot_indices]\n",
    "    lengths_indices = lengths_total[tot_indices]\n",
    "    tot_labels = np.repeat(tf, len(tot_indices))\n",
    "    cluster_ids.append(cluster_indices)\n",
    "    chromatin_types.append(tot_labels)\n",
    "    frag_length.append(lengths_indices)\n",
    "    total_index.append(tot_indices)\n",
    "\n",
    "print(cluster_ids)    \n",
    "\n",
    "#Also include Siva Satellite ZMWs\n",
    "sites_rep1 = './pbrun4_purple/biology_analyses/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.blast.satellite.txt.zmws'\n",
    "sites_rep2 = './pbrun4_purple/biology_analyses/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.blast.satellite.txt.zmws'\n",
    "sites_rep3 = './pbrun4_purple/biology_analyses/pbrun6.split.k562_postlyso_chromatin.ccs.blast.satellite.txt.zmws'\n",
    "sites_rep4 = './pbrun4_purple/biology_analyses/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.blast.satellite.txt.zmws'\n",
    "tf='Satellite'\n",
    "rep1_indices = find_indices(sites_rep1, zmw2index, 'K562_Rep1', tf)\n",
    "rep2_indices = find_indices(sites_rep2, zmw2index, 'K562_Rep2', tf)\n",
    "rep3_indices = find_indices(sites_rep3, zmw2index, 'K562_Rep3', tf)\n",
    "rep4_indices = find_indices(sites_rep4, zmw2index, 'K562_Rep4', tf)\n",
    "tot_indices = np.concatenate((rep1_indices,rep2_indices,rep3_indices,rep4_indices))    \n",
    "cluster_indices = clusters[tot_indices]\n",
    "lengths_indices = lengths_total[tot_indices]\n",
    "tot_labels = np.repeat(tf, len(tot_indices))\n",
    "cluster_ids.append(cluster_indices)\n",
    "chromatin_types.append(tot_labels)\n",
    "frag_length.append(lengths_indices)\n",
    "total_index.append(tot_indices)\n",
    "\n",
    "total_dataframe = pd.DataFrame({'cluster_id': np.concatenate(cluster_ids), 'chromatin_type': np.concatenate(chromatin_types) \\\n",
    "    , 'total_index': np.concatenate(total_index), 'frag_length' : np.concatenate(frag_length)})\n",
    "total_dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe.to_csv('fig5_wholedata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5: Distributions of NRLs\n",
    "We used the method described above to estimate NRLs on single-molecules and then visualized these distributions, medians and median absolute deviations in Figure 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_cruncher(ac):\n",
    "    peaks1 = []\n",
    "    for i in range(len(ac)):\n",
    "        mol1 = ac[i]\n",
    "        peak1, data = signal.find_peaks(mol1, height = 0.10, width = 25)\n",
    "        if len(peak1) == 0:\n",
    "            ac1p = np.nan\n",
    "        else:\n",
    "            ac1p = int(peak1[0])\n",
    "        peaks1.append(ac1p)\n",
    "    peaks1 = np.array(peaks1)\n",
    "    med1 = np.nanmedian(peaks1)\n",
    "    mad1 = scipy.stats.median_absolute_deviation(peaks1,nan_policy='omit')\n",
    "    mis1 = np.count_nonzero(np.isnan(peaks1))\n",
    "    return (peaks1, med1, mad1,mis1)\n",
    "\n",
    "fho1 = open('fig5_domainmean_plots.txt','w')\n",
    "fho2 = open('fig5_distplots.txt','w')\n",
    "smooth = 33\n",
    "for i in np.unique(total_dataframe['chromatin_type']):\n",
    "    test_indices = total_dataframe[total_dataframe['chromatin_type'] == i]['total_index']\n",
    "    mat_total = np.load('length_filtered_molecules.npy')[test_indices]\n",
    "    auto_cor = np.load('length_filtered_auto_cor.npy')[test_indices]\n",
    "    peaks1, med1, mad1, mis1 = peak_cruncher(auto_cor)\n",
    "    overall_peak, data = signal.find_peaks(np.nanmean(auto_cor,axis=0), height = 0.10, width = 25) \n",
    "    if len(overall_peak) == 0:\n",
    "        overall_peak = 0\n",
    "    else:\n",
    "        overall_peak = overall_peak[0]\n",
    "    mat_total= np.nanmean(mat_total,axis=0)\n",
    "    auto_cor = np.nanmean(auto_cor,axis=0)\n",
    "    for j in range(len(mat_total)):\n",
    "        print(\"%s\\t%s\\t%s\" % (i, j, mat_total[j]), file=fho1)\n",
    "#     for j in range(len(auto_cor)):\n",
    "#         print(\"%s\\t%s\\t%s\" % (i, j, auto_cor[j]), file=fho2)\n",
    "    print(len(mat_total))\n",
    "    print(i)\n",
    "    for j in range(len(peaks1)):\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\" % (i, peaks1[j], overall_peak, mis1, len(mat_total)), file=fho2)\n",
    "    print(\"%s\\t%s\\t%s\\t%s\" % (med1,mad1,mis1,overall_peak))\n",
    "    plt.figure(figsize=(30,10))\n",
    "    subplot(131)\n",
    "    plt.plot(range(1000), auto_cor)\n",
    "    xlabel('Offset (bp)')\n",
    "    ylabel('Pearson\\'s r')\n",
    "    subplot(132)\n",
    "    plt.plot(range(1000), mat_total)\n",
    "    xlabel('Distance to 5\\'-MNase cut')\n",
    "    ylabel('Average smoothed IPD (frames)')\n",
    "    subplot(133)\n",
    "    sns.distplot(peaks1[~np.isnan(peaks1)])\n",
    "    axvline(overall_peak)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Analyses: FLFE analyses for different chromatin states.\n",
    "We did not use these analyses in the paper but we have included these here for those interested in different fragment length enrichments for each chromatin state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_fld = total_dataframe[total_dataframe['frag_length'] <= 2000]['frag_length'].value_counts() / len(total_dataframe[total_dataframe['frag_length'] <= 2000]['length'])\n",
    "bg_fld = dict(zip(bg_fld.index.values, bg_fld.values))\n",
    "tot_mat = []\n",
    "fho = open('FLFE_Fig4.txt','w')\n",
    "for i in np.unique(frame['clust_label']):\n",
    "    clust_vals = []\n",
    "    sub_clust = frame[(frame['length'] <= 2000) & (frame['clust_label'] == i)]['length']\n",
    "    clust_fld = sub_clust.value_counts() / len(sub_clust)\n",
    "    clust_fld = dict(zip(clust_fld.index.values, clust_fld.values))\n",
    "    for j in range(500,2000):\n",
    "        if j in clust_fld:\n",
    "            val = np.log2(clust_fld[j] / bg_fld[j])\n",
    "            clust_vals.append(np.log2(clust_fld[j] / bg_fld[j]))\n",
    "        else:\n",
    "            val = np.nan\n",
    "        print(\"%s\\t%s\\t%s\" % (i, j, val), file=fho)\n",
    "    tot_mat.append(clust_vals)\n",
    "fho.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5: Fisher's Exact Tests\n",
    "Code to perform the Fisher's exact tests described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "test = total_dataframe[(total_dataframe['chromatin_type'] == 'H3K9me3') | (total_dataframe['chromatin_type'] == 'H3K9me1')]['total_index']\n",
    "print(len(test))\n",
    "print(len(np.unique(test)))\n",
    "\n",
    "fho = open('fishers_tests_fig5.txt','w')\n",
    "for i in np.unique(total_dataframe['cluster_id']):\n",
    "    if i == '7': continue\n",
    "    for j in np.unique(total_dataframe['chromatin_type']):\n",
    "        num_clust_lab = len(total_dataframe[(total_dataframe['cluster_id'] == i) & (total_dataframe['chromatin_type'] == j)])\n",
    "        num_clust_notlab = len(total_dataframe[(total_dataframe['cluster_id'] == i) & (total_dataframe['chromatin_type'] != j)])\n",
    "        num_notclust_lab = len(total_dataframe[(total_dataframe['cluster_id'] != i) & (total_dataframe['chromatin_type'] == j)])\n",
    "        num_notclust_notlab = len(total_dataframe[(total_dataframe['cluster_id'] != i) & (total_dataframe['chromatin_type'] != j)])\n",
    "        odds_r, pval = sp.stats.fisher_exact([[num_clust_lab, num_notclust_lab], \\\n",
    "            [num_clust_notlab, num_notclust_notlab]])\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\" % (i, j, num_clust_lab, odds_r, pval), file=fho)\n",
    "fho.close()\n",
    "                                                                                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Code\n",
    "This is extra code we used when troubleshooting a lot of these analyses. Included here for the sake of posterity but not necessary for any analyses in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering the TFs and Fisher's tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scanpy.AnnData(X=auto_cor[:,:500])\n",
    "scanpy.tl.pca(mat)\n",
    "scanpy.pp.neighbors(mat,metric='correlation',n_neighbors=10)\n",
    "scanpy.tl.leiden(mat,resolution=0.4)\n",
    "clusters = np.array(mat.obs['leiden'])\n",
    "plotter(auto_cor, mat_total, clusters, 33)\n",
    "\n",
    "# #sample labels for plotting\n",
    "# sp_obj.obs['samples'] = labels_total\n",
    "# scanpy.tl.umap(sp_obj)\n",
    "# #mat_ann.obs['colours'] = color_labels\n",
    "# fig, ax = plt.subplots(figsize=(20, 14))\n",
    "# scanpy.pl.umap(sp_obj, color='samples', ax=ax,size=30)\n",
    "# plt.show()\n",
    "\n",
    "# labs = np.array(labels_total)\n",
    "\n",
    "# print(len(labs))\n",
    "# print(len(clusters))\n",
    "\n",
    "# #labs = np.array(labels)\n",
    "# #print(\"Cluster\\t%s\\t%s\\tOR\" % (sample2, sample1))\n",
    "# for i in np.unique(clusters):\n",
    "#     for j in np.unique(labs):\n",
    "#         for k in np.unique(labs):\n",
    "#             sub = labs[clusters == i]\n",
    "#             num_s2 = len(sub[sub == j]) \n",
    "#             num_not_s2 = len(labs[labs == j]) - num_s2\n",
    "#             num_s1 = len(sub[sub == k]) \n",
    "#             num_not_s1 = len(labs[labs == k]) - num_s1\n",
    "#             odds_r, pval = sp.stats.fisher_exact([[num_s2, num_s1], [num_not_s2, num_not_s1]])\n",
    "#             print(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\" % (i, num_s2, num_s1,odds_r,pval, j , k))\n",
    "    #num_neg2 = len(sub[sub == '%s_neg' % sample2]) / len(labs[labs=='%s_neg' % sample2])\n",
    "    #print(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\" % (i, num_control, num_ctcf, num_neg, num_neg2, lo))\n",
    "    #print(\"%s\\t%s\\t%s\\t%s\" % (i, num_control, num_ctcf,lo)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot cluster accessiblity as a heatmap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fho1 = open('hmap_figure.txt','w')\n",
    "fho2 = open('single_mol_heatmaps.txt', 'w')\n",
    "for i in np.unique(clusters):\n",
    "    mat_new = auto_cor[clusters == i]\n",
    "    mat_new2 = mat_total[clusters == i]\n",
    "    hmap = pd.Series(np.nanmean(binarize_sig(mat_new2),axis=0)).rolling(20,center=True, min_periods=1).mean()\n",
    "    for j in range(len(hmap)):\n",
    "        print(\"%s\\t%s\\t%s\" % (i, j, hmap[j]), file=fho1)        \n",
    "    process = pd.DataFrame(binarize_sig(mat_new2)).rolling(5, axis=1, center=True, min_periods=1).mean().values\n",
    "    for j in range(len(process)):\n",
    "        for k in range(len(process[j])):\n",
    "            print(\"%s\\t%s\\t%s\\t%s\\t%s\" % (labels_total[j],j , k, process[j][k], i), file=fho2)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(range(len(mat_new2[0])), np.nanmean(process, axis=0))\n",
    "    xlabel('Distance to 5\\'-MNase cut')\n",
    "    ylabel('Average smoothed IPD (frames)')\n",
    "    subplot(143)\n",
    "    imshow(mat_new, interpolation=None, cmap=cm.BuPu)\n",
    "    xlabel('Offset (bp)')\n",
    "    subplot(144)\n",
    "    imshow(process, interpolation=None, cmap=cm.BuPu)\n",
    "    xlabel('Distance')\n",
    "fho1.close()\n",
    "fho2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## individual cluster heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fho2 = open('single_mol_heatmaps.txt', 'w')\n",
    "process = pd.DataFrame(binarize_sig(mat_total)).rolling(5, axis=1, center=True, min_periods=1).mean().values\n",
    "for i in np.unique(clusters):\n",
    "    labs = labels_total[clusters == i]\n",
    "    sub = process[clusters == i]\n",
    "    for j in range(len(sub)):\n",
    "        for k in range(len(sub[j])):\n",
    "            print(\"%s\\t%s\\t%s\\t%s\\t%s\" % (labs[j],j, k, sub[j][k], i), file=fho2)\n",
    "fho2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(process))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas fishers tests for fig 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just do the damn fishers tests in pandas\n",
    "fho = open('fishers_tests_fig3.txt','w')\n",
    "cluster_df = {'clusters':clusters, 'labels':labels_total}\n",
    "cluster_df = pd.DataFrame(cluster_df)\n",
    "for i in np.unique(clusters):\n",
    "    for j in np.unique(labels_total):\n",
    "        num_clust_lab = len(cluster_df[(cluster_df['clusters'] == i) & (cluster_df['labels'] == j)])\n",
    "        num_clust_notlab = len(cluster_df[(cluster_df['clusters'] == i) & (cluster_df['labels'] != j)])\n",
    "        num_notclust_lab = len(cluster_df[(cluster_df['clusters'] != i) & (cluster_df['labels'] == j)])\n",
    "        num_notclust_notlab = len(cluster_df[(cluster_df['clusters'] != i) & (cluster_df['labels'] != j)])\n",
    "        odds_r, pval = sp.stats.fisher_exact([[num_clust_lab, num_notclust_lab], \\\n",
    "            [num_clust_notlab, num_notclust_notlab]])\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\" % (i, j, num_clust_lab, odds_r, pval), file=fho)\n",
    "fho.close()\n",
    "#labs = np.array(labels)\n",
    "#print(\"Cluster\\t%s\\t%s\\tOR\" % (sample2, sample1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QuickPlots\n",
    "fho = open('ggplot_UMAPs_TFs.txt','w')\n",
    "for i in range(len(labels_total)):\n",
    "    print(\"%s\\t%s\\t%s\\t%s\"%(sp_obj.obsm['X_umap'][:,0][i], sp_obj.obsm['X_umap'][:,1][i], labels_total[i], clusters[i]), file=fho)\n",
    "fho.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine end-positioned regularity across chromatin domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auto_cor, processed = process_xcors(mat_total, length)\n",
    "print(len(auto_cor))\n",
    "motif_sites = processed[:,int(length / 2) - 10:int(length / 2) + 10]\n",
    "clusters = cluster_mats(auto_cor,res=0.3)\n",
    "plotter(mat_total, auto_cor, clusters)\n",
    "\n",
    "labs = np.array(labels_total)\n",
    "\n",
    "print(len(labs))\n",
    "print(len(np.unique(clusters)))\n",
    "\n",
    "fho = open('plotme_fisherstests_500.txt','w')\n",
    "\n",
    "print(\"Cluster\\t%s\\t%s\\tOR\" % (sample2, sample1))\n",
    "for i in np.unique(clusters):\n",
    "    sub = labs[clusters == i]\n",
    "    if len(sub) < 100: continue\n",
    "    for j in np.unique(labs):\n",
    "        for k in np.unique(labs):\n",
    "            num_s2 = len(sub[sub == j]) \n",
    "            num_not_s2 = len(labs[labs == j]) - num_s2\n",
    "            num_s1 = len(sub[sub == k]) \n",
    "            num_not_s1 = len(labs[labs == k]) - num_s1\n",
    "            odds_r, pval = sp.stats.fisher_exact([[num_s2, num_s1], [num_not_s2, num_not_s1]])\n",
    "            print(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\" % (i, num_s2, num_s1,odds_r,pval, j , k), file=fho)\n",
    "fho.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_cor, processed = process_xcors(mat_total, length)\n",
    "print(len(auto_cor))\n",
    "motif_sites = processed[:,int(length / 2) - 10:int(length / 2) + 10]\n",
    "clusters = cluster_mats(processed,res=0.5)\n",
    "plotter(mat_total, auto_cor, clusters)\n",
    "\n",
    "# labs = np.array(labels_total)\n",
    "\n",
    "# print(len(labs))\n",
    "# print(len(clusters))\n",
    "\n",
    "# fho = open('plotme_fisherstests_750.txt','w')\n",
    "# print(\"Cluster\\t%s\\t%s\\tOR\" % (sample2, sample1))\n",
    "# for i in np.unique(clusters):\n",
    "#     sub = labs[clusters == i]\n",
    "#     if len(sub) < 100: continue\n",
    "#     for j in np.unique(labs):\n",
    "#         for k in np.unique(labs):\n",
    "#             num_s2 = len(sub[sub == j]) \n",
    "#             num_not_s2 = len(labs[labs == j]) - num_s2\n",
    "#             num_s1 = len(sub[sub == k]) \n",
    "#             num_not_s1 = len(labs[labs == k]) - num_s1\n",
    "#             odds_r, pval = sp.stats.fisher_exact([[num_s2, num_s1], [num_not_s2, num_not_s1]])\n",
    "#             print(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\" % (i, num_s2, num_s1,odds_r,pval, j , k), file=fho)\n",
    "# fho.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Genome Analyses I: Similarity and difference of nucleosome conformers across + within TADs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "length = 1000\n",
    "\n",
    "tf = 'TAD'\n",
    "sample = '%s' % tf\n",
    "sites_rep1 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "sites_rep2 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "sites_rep3 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "sites_rep4 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "\n",
    "sites_rep1_dinucs = './pbrun4_purple/ccs/for_dinucs/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws.extract.bed.facount' % sample\n",
    "sites_rep2_dinucs = './pbrun4_purple/ccs/for_dinucs/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws.extract.bed.facount' % sample\n",
    "sites_rep3_dinucs = './pbrun6/ccs/for_dinucs/pbrun6.split.k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws.extract.bed.facount'% sample\n",
    "sites_rep4_dinucs = './pbrun6/ccs/for_dinucs/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws.extract.bed.facount' % sample\n",
    "\n",
    "mat_rep1, mat_rep1_r, lengths, labels,holes,dinucs = distill_tipds_tad(rep1, rep1_valid, sites_rep1, sites_rep1_dinucs, length, sample, 'Replicate1', subsample=0)\n",
    "mat_rep2, mat_rep2_r, lengths2, labels2,holes2,dinucs2 = distill_tipds_tad(rep2, rep2_valid, sites_rep2, sites_rep2_dinucs, length, sample, 'Replicate2', subsample=0)\n",
    "mat_rep3, mat_rep3_r, lengths3, labels3,holes3,dinucs3 = distill_tipds_tad(rep3, rep3_valid, sites_rep3, sites_rep3_dinucs, length, sample,'Replicate1', subsample=0)\n",
    "mat_rep4, mat_rep4_r, lengths4, labels4,holes4,dinucs4 = distill_tipds_tad(rep4, rep4_valid, sites_rep4, sites_rep4_dinucs, length, sample,'Replicate2', subsample=0)\n",
    "\n",
    "mat_total = np.vstack((mat_rep1,mat_rep2,mat_rep3,mat_rep4))\n",
    "mat_total_r = np.vstack((mat_rep1_r,mat_rep2_r,mat_rep3_r,mat_rep4_r))\n",
    "\n",
    "dinucs_total = np.vstack((dinucs,dinucs2,dinucs3,dinucs4))\n",
    "lengths_total = np.concatenate((lengths, lengths2,lengths3,lengths4))\n",
    "labels_total = np.concatenate((labels, labels2,labels3,labels4))\n",
    "holes_total = np.concatenate((holes, holes2, holes3, holes4))\n",
    "\n",
    "mat_rep1 = 0\n",
    "mat_rep2 = 0\n",
    "mat_rep3 = 0\n",
    "mat_rep4 = 0\n",
    "mat_rep1_r = 0\n",
    "mat_rep2_r = 0\n",
    "mat_rep3_r = 0\n",
    "mat_rep4_r = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holes_total = np.concatenate((holes, holes2, holes3, holes4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(holes_total))\n",
    "fho = open('forMehran_cluster_zmw_ids','w')\n",
    "for i in range(len(holes_total)):\n",
    "    print(\"%s\\t%s\" % (holes_total[i], clusters[i]),file=fho)\n",
    "fho.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the previously calc'd cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_file = open('./fig4_cluster_labs.txt')\n",
    "clusters = []\n",
    "for line in cluster_file:\n",
    "    split = line.split()\n",
    "    clusters.append(split[0])\n",
    "clusters = np.array(clusters)\n",
    "print(len(clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster TAD-binned ZMWs by auto-correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plotter_v3(new_mat, new_mat2, auto_cor, clusters,smooth):\n",
    "    for i in np.unique(clusters):\n",
    "        print(i)\n",
    "        mat_new = auto_cor[clusters == i]\n",
    "        print(len(mat_new))\n",
    "        mat_new2 = new_mat[clusters == i]\n",
    "        process = pd.DataFrame(mat_new2).rolling(smooth, axis=1, center=True, min_periods=1).mean()\n",
    "        mat_new3 = new_mat2[clusters == i]\n",
    "        process = pd.DataFrame(mat_new2).rolling(smooth, axis=1, center=True, min_periods=1).mean()\n",
    "        process2 = pd.DataFrame(mat_new3).rolling(smooth, axis=1, center=True, min_periods=1).mean()\n",
    "        plt.figure(figsize=(30,10))\n",
    "        subplot(131)\n",
    "        plt.plot(range(len(mat_new[0])), np.nanmean(mat_new, axis=0))\n",
    "        xlabel('Offset (bp)')\n",
    "        ylabel('Pearson\\'s r')\n",
    "        subplot(132)\n",
    "        plt.plot(range(len(mat_new2[0])), np.nanmean(process, axis=0))\n",
    "        plt.plot(range(len(mat_new2[0])), np.nanmean(process2, axis=0))\n",
    "        xlabel('Distance to 5\\'-MNase cut')\n",
    "        ylabel('Average smoothed IPD (frames)')\n",
    "        subplot(133)\n",
    "        imshow(mat_new, interpolation=None, cmap=cm.BuPu)\n",
    "        xlabel('Offset (bp)')\n",
    "        plt.show()\n",
    "\n",
    "nl = 500\n",
    "auto_cor = 0\n",
    "processed = 0\n",
    "auto_cor1, processed = process_xcors(np.nan_to_num(mat_total[:,:nl]), nl)\n",
    "processed = 0\n",
    "auto_cor2, processed = process_xcors(np.nan_to_num(mat_total_r[:,:nl]), nl)\n",
    "processed = 0\n",
    "#clusters, sp_obj = cluster_mats(auto_cor1,res=0.8,neighbors=15)\n",
    "#plotter_v3(binarize_sig(mat_total), binarize_sig(mat_total_r), auto_cor, clusters, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cluster labels for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fho = open('fig4_cluster_labs_v2.txt','w')\n",
    "\n",
    "for i in range(len(clusters)):\n",
    "    print(\"%s\\t%s\\t%s\" % (clusters[i], labels_total[i], lengths_total[i]), file=fho)\n",
    "\n",
    "fho.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot each conformer A/T sig + autocorr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fho = open('fig4_lineplots.txt','w')\n",
    "for i in np.unique(clusters):\n",
    "    cluster_size = len(mat_total[clusters == i])\n",
    "    if cluster_size < 1000:\n",
    "        continue\n",
    "    sub = pd.Series(np.nanmean(binarize_sig(mat_total[(clusters == i)]),axis=0)).rolling(20,center=True, min_periods=1).mean()\n",
    "    label = i\n",
    "    for j in range(len(sub)):\n",
    "        print(\"%s\\t%s\\t%s\\t%s\" % (j, sub[j], label,cluster_size),file=fho)\n",
    "fho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap representations for Fig 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fho1 = open('hmap_figure_fig4.txt','w')\n",
    "for i in np.unique(clusters):\n",
    "    mat_new = auto_cor[clusters == i]\n",
    "    if len(mat_new) < 1000: continue\n",
    "    hmap = np.nanmean(mat_new, axis=0)\n",
    "    for j in range(len(hmap)):\n",
    "        print(\"%s\\t%s\\t%s\" % (i, j, hmap[j]), file=fho1)        \n",
    "fho1.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the clustered autocorrelograms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_cruncher(autocorrelogram):\n",
    "    peaks = []\n",
    "    heights = []\n",
    "    non_peaks = 0\n",
    "    for i in range(len(autocorrelogram)):\n",
    "        mol = autocorrelogram[i]\n",
    "        peak, data = signal.find_peaks(mol, height = 0.10, width = 25)\n",
    "        if len(peak) == 0:\n",
    "            non_peaks+=1\n",
    "            continue\n",
    "        peaks.append(peak[0])\n",
    "        heights.append(data['peak_heights'][0])\n",
    "    #for after I implement filtration via controls\n",
    "#         else:\n",
    "#             peaks.append(0)\n",
    "#             heights.append(0)\n",
    "    med = np.median(peaks)\n",
    "    mad = scipy.stats.median_absolute_deviation(peaks)\n",
    "    return (peaks, heights, non_peaks, med, mad)\n",
    "\n",
    "def plotter_v2(new_mat, auto_cor, clusters, fho):\n",
    "    for i in np.unique(clusters):\n",
    "        mat_new = auto_cor[clusters == i]\n",
    "        peaks, heights, non_peaks, med, mad = peak_cruncher(mat_new)\n",
    "        mat_mean = np.mean(mat_new, axis=0)\n",
    "        overall_peak, data = signal.find_peaks(mat_mean, height = 0.10, width = 25)\n",
    "        if len(overall_peak) == 0:\n",
    "            overall_peak = 0\n",
    "        else:\n",
    "            overall_peak = overall_peak[0]\n",
    "        length_sub = lengths_total[clusters == i]\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\" % (i, overall_peak, non_peaks, len(length_sub) , med, mad))\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\" % (i, overall_peak, non_peaks, len(length_sub), med, mad), file=fho)\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         subplot(121)\n",
    "#         sns.distplot(peaks)\n",
    "#         axvline(overall_peak)\n",
    "#         subplot(122)\n",
    "#         sns.distplot(length_sub,bins=50)\n",
    "#         plt.show()\n",
    "fho = open('figs4_statistics_bothstrands.txt','w')\n",
    "plotter_v2(mat_total, auto_cor, clusters, fho)\n",
    "fho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute single-molecule autocorrelograms and find peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_cruncher(ac1,ac2):\n",
    "    peaks1 = []\n",
    "    peaks2 = []\n",
    "    for i in range(len(ac1)):\n",
    "        mol1 = ac1[i]\n",
    "        mol2 = ac2[i]\n",
    "        peak1, data = signal.find_peaks(mol1, height = 0.10, width = 25)\n",
    "        peak2, data = signal.find_peaks(mol2, height = 0.10, width = 25)\n",
    "        if len(peak1) == 0:\n",
    "            ac1p = np.nan\n",
    "        else:\n",
    "            ac1p = int(peak1[0])\n",
    "        if len(peak2) == 0:\n",
    "            ac2p = np.nan\n",
    "        else:\n",
    "            ac2p = int(peak2[0])\n",
    "        peaks1.append(ac1p)\n",
    "        peaks2.append(ac2p)\n",
    "    peaks1 = np.array(peaks1)\n",
    "    peaks2 = np.array(peaks2)\n",
    "    med1 = np.nanmedian(peaks1)\n",
    "    mad1 = scipy.stats.median_absolute_deviation(peaks1,nan_policy='omit')\n",
    "    mis1 = np.count_nonzero(np.isnan(peaks1))\n",
    "    med2 = np.nanmedian(peaks2)\n",
    "    mad2 = scipy.stats.median_absolute_deviation(peaks2,nan_policy='omit')\n",
    "    mis2 = np.count_nonzero(np.isnan(peaks2))\n",
    "    return (peaks1, peaks2, med1, mad1,mis1, med2, mad2,mis2)\n",
    "\n",
    "def smol_autos(auto_cor1, auto_cor2, clusters, fho):\n",
    "    for i in np.unique(clusters):\n",
    "        mat_new = auto_cor1[clusters == i]\n",
    "        mat_new2 = auto_cor2[clusters == i]\n",
    "        peaks1, peaks2, med1, mad1, mis1, med2, mad2, mis2 = peak_cruncher(mat_new, mat_new2)\n",
    "        mat_mean = np.mean(mat_new, axis=0)\n",
    "        mat_mean2 = np.mean(mat_new2, axis=0)\n",
    "        overall_peak, data = signal.find_peaks(mat_mean, height = 0.10, width = 25)\n",
    "        overall_peak2, data = signal.find_peaks(mat_mean2, height = 0.10, width = 25)\n",
    "        if len(overall_peak2) == 0:\n",
    "            overall_peak2 = 0\n",
    "        else:\n",
    "            overall_peak2 = overall_peak2[0]\n",
    "        if len(overall_peak) == 0:\n",
    "            overall_peak = 0\n",
    "        else:\n",
    "            overall_peak = overall_peak[0]\n",
    "        length_sub = lengths_total[clusters == i]\n",
    "        if len(length_sub) <= 1000: continue\n",
    "        print(len(peaks1[(~np.isnan(peaks1)) & (~np.isnan(peaks2))]))\n",
    "        print(len(peaks2[(~np.isnan(peaks1)) & (~np.isnan(peaks2))]))\n",
    "\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\" % \\\n",
    "              (i, overall_peak, overall_peak2, mis1, mis2, med1, med2, mad1, mad2, len(length_sub)))\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\" % \\\n",
    "              (i, overall_peak, overall_peak2, mis1, mis2, med1, med2, mad1, mad2, len(length_sub)),file=fho\n",
    "              )\n",
    "        plt.figure(figsize=(5,5))\n",
    "        sns.jointplot(peaks1[(~np.isnan(peaks1)) & (~np.isnan(peaks2))], peaks2[(~np.isnan(peaks1)) & (~np.isnan(peaks2))], kind=\"hex\")\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(5,5))\n",
    "        sns.jointplot(peaks1[(~np.isnan(peaks1)) & (~np.isnan(peaks2))], length_sub[(~np.isnan(peaks1)) & (~np.isnan(peaks2))], kind=\"hex\")        \n",
    "        plt.figure(figsize=(5,5))\n",
    "        sns.jointplot(peaks2[(~np.isnan(peaks1)) & (~np.isnan(peaks2))], length_sub[(~np.isnan(peaks1)) & (~np.isnan(peaks2))], kind=\"hex\")\n",
    "#        plt.scatter(peaks1[(~np.isnan(peaks1)) & (~np.isnan(peaks2))], peaks2[(~np.isnan(peaks1)) & (~np.isnan(peaks2))],alpha=0.25)\n",
    "        plt.show()\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         subplot(121)\n",
    "#         sns.distplot(peaks)\n",
    "#         axvline(overall_peak)\n",
    "#         subplot(122)\n",
    "#         sns.distplot(length_sub,bins=50)\n",
    "#         plt.show()\n",
    "fho = open('figs4_statistics_bothstrands.txt','w')\n",
    "smol_autos(auto_cor1, auto_cor2, clusters, fho)\n",
    "fho.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick out three clusters to plot in the main text figure; randomly sample 5000 ZMWs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process=0\n",
    "fho2 = open('single_mol_heatmaps_fig4.txt', 'w')\n",
    "#process = pd.DataFrame(binarize_sig(mat_total)).rolling(5, axis=1, center=True, min_periods=1).mean().values\n",
    "for i in np.unique(clusters):\n",
    "    idx = np.random.choice(mat_total.shape[0], 5000, replace=False)\n",
    "    if i == '13' or i =='4' or i =='6':\n",
    "        sub = pd.DataFrame(binarize_sig(mat_total[(clusters == i) & (lengths_total >= 1000)])).rolling(5, axis=1, center=True, min_periods=1).mean().values\n",
    "        idx = np.random.choice(sub.shape[0], 5000, replace=False)\n",
    "        subsamp = sub[idx,:]\n",
    "        for j in range(len(subsamp)):\n",
    "            for k in range(len(subsamp[j])):\n",
    "                print(\"%s\\t%s\\t%s\\t%s\\t%s\" % (i ,j, k, subsamp[j][k], i), file=fho2)\n",
    "fho2.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genome Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe with TAD_labels, cluster_labels, replicate_labels\n",
    "df = pd.DataFrame({'clusters':clusters,'labels':labels_total, 'reps':holes_total})\n",
    "fho = open('fig4_stacked_bar_plot.txt','w')\n",
    "for i in np.unique(clusters):\n",
    "    cluster_frac_rep1 = len((df[(df['clusters'] == i) & (df['reps'] == 'Replicate1')])) / len(df[(df['reps'] == 'Replicate1')])\n",
    "    cluster_frac_rep2 = len((df[(df['clusters'] == i) & (df['reps'] == 'Replicate2')])) / len(df[(df['reps'] == 'Replicate2')])\n",
    "    print(\"%s\\t%s\\tReplicate 1\" % (i,cluster_frac_rep1), file=fho)\n",
    "    print(\"%s\\t%s\\tReplicate 2\" % (i,cluster_frac_rep2), file=fho)\n",
    "fho.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dinucs_total[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dinucleotide Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dinucs = []\n",
    "\n",
    "for i in np.unique(clusters):\n",
    "    if len(dinucs_total[clusters == i]) < 1000: continue\n",
    "    dinucs_clust = np.sum(dinucs_total[clusters == i],axis=0) \n",
    "    cluster_dinucs.append(dinucs_clust)\n",
    "\n",
    "cluster_dinucs = np.vstack(cluster_dinucs)\n",
    "plt.figure(figsize=(10,10))\n",
    "imshow(np.corrcoef(cluster_dinucs),interpolation=None, cmap=cm.RdBu)\n",
    "colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAD Composition\n",
    "We'd like to test the hypothesis that molecules from the same TADs significantly \n",
    "share (or don't share) molecular states. To test this, we're going to try two different\n",
    "approaches. First, we'll make an information content calculation, asking whether each TAD has \n",
    "more or less information (in bits) compared to a random sampling of molecules. Second, we'll ask\n",
    "whether the variance in computed NRLs for molecules taken from TADs is significantly different from the variance \n",
    "of computed NRLs from random molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "entropy_df = []\n",
    "entropy_cntrl = [] \n",
    "#Entropy calc\n",
    "for label in np.unique(labels_total):\n",
    "    cluster_labs = clusters[labels_total == label]\n",
    "    if len(cluster_labs) <= 50: continue\n",
    "    control_labs = np.random.choice(clusters, size=len(cluster_labs))\n",
    "#    print(len(cluster_labs))\n",
    "    df_control = pd.DataFrame.from_dict(Counter(control_labs), orient='index')\n",
    "    df = pd.DataFrame.from_dict(Counter(cluster_labs), orient='index')\n",
    "#   print(df)\n",
    "    df['norm'] = df[0] / df[0].sum()\n",
    "    df_control['norm'] = df_control[0] / df_control[0].sum()\n",
    "    entropy_df.append(scipy.stats.entropy(df['norm'].values))\n",
    "    entropy_cntrl.append(scipy.stats.entropy(df_control['norm'].values))\n",
    "\n",
    "    #print(entropy_df)\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.distplot(entropy_df)\n",
    "sns.distplot(entropy_cntrl)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fho = open('Entropy_variance_figures.txt','w')\n",
    "for i in range(len(entropy_df)):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp=[]\n",
    "samp_frac = []\n",
    "control=[]\n",
    "control_frac = []\n",
    "samp_lengths = []\n",
    "#print(labels_total)\n",
    "for label in np.unique(labels_total):\n",
    "    samp.append(len(clusters[(labels_total == label) & (clusters != '16') & (clusters != '17')]))\n",
    "    samp_lengths.append(np.average(lengths_total[(labels_total == label) & (clusters != '16') & (clusters != '17')]))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(x=samp)\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "# plt.figure(figsize=(10,10))\n",
    "# sns.distplot(samp)\n",
    "# plt.show()\n",
    "#     num_unique = len(np.unique(cluster_labs))\n",
    "#     cluster_filt = clusters[(clusters != '16') & (clusters != '17')]\n",
    "# #   print(len(cluster_filt))\n",
    "# #   print(len(cluster_labs))\n",
    "#     control_labs = np.random.choice(cluster_filt, size=len(cluster_labs), replace=False)\n",
    "#     num_unique_c = len(np.unique(control_labs))\n",
    "#     samp.append(num_unique)\n",
    "#     control.append(num_unique_c)\n",
    "\n",
    "# print(len(samp))\n",
    "# print(len(control))\n",
    "\n",
    "# plt.figure(figsize=(10,10))\n",
    "# sns.distplot(s)\n",
    "# sns.distplot(control)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(samp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fho=open('variance_plot2.txt', 'w')\n",
    "for i in range(len(samp)):\n",
    "    print(\"%s\\t%s\\tSample\\tCluster Membership\" % (i, samp[i]) , file = fho)\n",
    "    print(\"%s\\t%s\\tControl\\tCluster Membership\" % (i, control[i]) , file = fho)\n",
    "fho.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_variance(autocorrelogram):\n",
    "    peaks = []\n",
    "    non_peaks = 0\n",
    "    for i in range(len(autocorrelogram)):\n",
    "        mol = autocorrelogram[i]\n",
    "        peak, data = signal.find_peaks(mol, height = 0.10, width = 25)\n",
    "        if len(peak) == 0:\n",
    "            non_peaks+=1\n",
    "            continue\n",
    "        peaks.append(peak[0])\n",
    "    var = np.var(peaks)\n",
    "    return (non_peaks, var)\n",
    "\n",
    "samp=[]\n",
    "samp_frac = []\n",
    "control=[]\n",
    "control_frac = []\n",
    "\n",
    "for label in np.unique(labels_total):\n",
    "    cluster_mols = auto_cor[labels_total == label]\n",
    "    if len(cluster_mols) <= 50: continue\n",
    "    control_mols = auto_cor[np.random.choice(auto_cor.shape[0], len(cluster_mols), replace=False),:]\n",
    "    nonpeaks, var = peak_variance(cluster_mols)\n",
    "    nonpeaks_c, var_c = peak_variance(control_mols)\n",
    "    samp.append(var)\n",
    "    control.append(var_c)\n",
    "    samp_frac.append((nonpeaks / len(cluster_mols)))\n",
    "    control_frac.append((nonpeaks_c / len(cluster_mols)))\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(10,10))\n",
    "sns.distplot(samp)\n",
    "sns.distplot(control)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.distplot(samp_frac)\n",
    "sns.distplot(control_frac)\n",
    "plt.show()\n",
    "\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         subplot(121)\n",
    "#         sns.distplot(peaks)\n",
    "#         axvline(overall_peak)\n",
    "#         subplot(122)\n",
    "#         sns.distplot(length_sub,bins=50)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fho = open('Entropy_variance_figures.txt','w')\n",
    "for i in range(len(entropy_df)):\n",
    "    print(\"%s\\t%s\\tSample\\tEntropy\" % (i, entropy_df[i]), file=fho)\n",
    "for i in range(len(entropy_cntrl)):\n",
    "    print(\"%s\\t%s\\tControl\\tEntropy\" % (i, entropy_cntrl[i]), file=fho)\n",
    "for i in range(len(samp)):\n",
    "    print(\"%s\\t%s\\tSample\\tVariance\" % (i, samp[i]), file=fho)\n",
    "for i in range(len(control)):\n",
    "    print(\"%s\\t%s\\tControl\\tVariance\" % (i, control[i]), file=fho)\n",
    "fho.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tad_zmw_coverage = []\n",
    "for label in np.unique(labels_total):\n",
    "    cluster_mols = len(auto_cor[labels_total == label])\n",
    "    tad_zmw_coverage.append(cluster_mols)\n",
    "print(len(tad_zmw_coverage))\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.distplot(tad_zmw_coverage)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = scipy.stats.wilcoxon(entropy_df, entropy_cntrl,alternative='two-sided')\n",
    "print(test1)\n",
    "test2 = scipy.stats.wilcoxon(np.sqrt(samp), np.sqrt(control), alternative='two-sided')\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.distplot(tad_zmw_coverage)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Genome Analyses II: Domain Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_tipds_tad(tipds, valid_zmws, sitelist, length, label, subsample=0):\n",
    "    '''Used to extract relevant mapped ZMWs given a list of ZMW ids. Also excludes bad ZMWs given the valid\n",
    "    ZMW list. This is for fragment ends (doesn't search for a flat feaure of interest).'''\n",
    "    sites = open(sitelist,'r')\n",
    "    hole_nos = {}\n",
    "    for i in sites:\n",
    "        split = i.split()\n",
    "        if int(split[0]) not in valid_zmws: continue\n",
    "        hole_nos[split[0]] = [split[1], int(split[3]) - int(split[2]), split[4]]\n",
    "    sites.close()\n",
    "    new_mat = np.zeros(length)\n",
    "    lengths = []\n",
    "    labs = []\n",
    "    reads = []\n",
    "    n_mol = 0\n",
    "    for hole in hole_nos:\n",
    "        if subsample != 0:\n",
    "            if n_mol > subsample: break\n",
    "        read = tipds[int(hole)]\n",
    "        dist = hole_nos[hole]\n",
    "        if len(read) >= length:\n",
    "            reads.append(read[:length])\n",
    "            lengths.append(len(read))\n",
    "            labs.append(dist[-1])\n",
    "            n_mol +=1\n",
    "    new_mat = np.vstack(reads)\n",
    "    return (new_mat, lengths, labs)\n",
    "\n",
    "length = 1000\n",
    "\n",
    "tfs = open('./pbrun4_purple/biology_analyses/new_beds.txt')\n",
    "#tfs = ['coopfoots\\n']\n",
    "lengths_total = np.array([0])\n",
    "labels_total = np.array([0])\n",
    "holes_total = np.array([0])\n",
    "mat_total = []\n",
    "mat_total_r = []\n",
    "samp = 1000\n",
    "#samp=250\n",
    "for line in tfs:\n",
    "    tf = line.split()[0]\n",
    "    sample = '%s' % tf\n",
    "    print(sample)\n",
    "    sites_rep1 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "    sites_rep2 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "    sites_rep3 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "    sites_rep4 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "    mat_rep1, mat_rep1_r, lengths, labels,holes, frac, tot = distill_tipds_bed(rep1, rep1_valid, sites_rep1, length, sample,subsample=samp)\n",
    "    print(frac/tot)\n",
    "    mat_rep2, mat_rep2_r, lengths2, labels2,holes2, frac, tot = distill_tipds_bed(rep2, rep2_valid, sites_rep2, length, sample,subsample=samp)    \n",
    "    print(frac/tot)    \n",
    "    mat_rep3, mat_rep3_r, lengths3, labels3,holes3, frac, tot = distill_tipds_bed(rep3, rep3_valid, sites_rep3, length, sample,subsample=samp*2)    \n",
    "    print(frac/tot)    \n",
    "    mat_rep4, mat_rep4_r, lengths4, labels4,holes4, frac, tot = distill_tipds_bed(rep4, rep4_valid, sites_rep4, length, sample,subsample=samp*2)    \n",
    "    print(frac/tot)\n",
    "    mat_total.extend((mat_rep1,mat_rep2,mat_rep3,mat_rep4))\n",
    "    mat_total_r.extend((mat_rep1_r,mat_rep2_r,mat_rep3_r,mat_rep4_r))\n",
    "    print(\"%s\\t%s\" % (sample, (len(mat_rep1)+len(mat_rep2)+len(mat_rep3)+len(mat_rep4))))\n",
    "#   print(\"%s_control\\t%s\" % (sample, (len(cntrl_mat_rep1)+len(cntrl_mat_rep2)+len(cntrl_mat_rep3)+len(cntrl_mat_rep4))))\n",
    "    lengths_total = np.concatenate((lengths_total, lengths, lengths2,lengths3,lengths4))\n",
    "    labels_total = np.concatenate((labels_total, labels, labels2,labels3,labels4))\n",
    "    holes_total = np.concatenate((holes_total, holes, holes2, holes3, holes4))\n",
    "#Also include Siva Satellite ZMWs\n",
    "sites_rep1 = './pbrun4_purple/biology_analyses/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.blast.satellite.txt.zmws'\n",
    "sites_rep2 = './pbrun4_purple/biology_analyses/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.blast.satellite.txt.zmws'\n",
    "sites_rep3 = './pbrun4_purple/biology_analyses/pbrun6.split.k562_postlyso_chromatin.ccs.blast.satellite.txt.zmws'\n",
    "sites_rep4 = './pbrun4_purple/biology_analyses/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.blast.satellite.txt.zmws'\n",
    "sample='Satellite'\n",
    "mat_rep1, mat_rep1_r, lengths, labels, holes, frac, tot = distill_tipds_bed(rep1, rep1_valid, sites_rep1, length, sample,subsample=samp)\n",
    "print(frac/tot)\n",
    "mat_rep2, mat_rep2_r, lengths2, labels2, holes2, frac, tot = distill_tipds_bed(rep2, rep2_valid, sites_rep2, length, sample,subsample=samp)    \n",
    "print(frac/tot)\n",
    "mat_rep3, mat_rep3_r, lengths3, labels3, holes3, frac, tot = distill_tipds_bed(rep3, rep3_valid, sites_rep3, length, sample,subsample=samp*2)    \n",
    "print(frac/tot)\n",
    "mat_rep4, mat_rep4_r, lengths4, labels4, holes4, frac, tot = distill_tipds_bed(rep4, rep4_valid, sites_rep4, length, sample,subsample=samp*2)    \n",
    "print(frac/tot)\n",
    "mat_total.extend((mat_rep1,mat_rep2,mat_rep3,mat_rep4))\n",
    "mat_total_r.extend((mat_rep1_r,mat_rep2_r,mat_rep3_r,mat_rep4_r))\n",
    "print(\"%s\\t%s\" % (sample, (len(mat_rep1)+len(mat_rep2)+len(mat_rep3)+len(mat_rep4))))\n",
    "lengths_total = np.concatenate((lengths_total, lengths, lengths2,lengths3,lengths4))\n",
    "labels_total = np.concatenate((labels_total, labels, labels2,labels3,labels4))\n",
    "holes_total = np.concatenate((holes_total, holes, holes2, holes3, holes4))\n",
    "\n",
    "mat_total = np.vstack(mat_total)\n",
    "mat_total_r = np.vstack(mat_total_r)\n",
    "lengths_total = lengths_total[1:]\n",
    "labels_total = labels_total[1:]\n",
    "holes_total = holes_total[1:]\n",
    "tfs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "#plt.plot(range(len(np.nanmean(mat_total, axis=0))), np.nanmean(mat_total[labels_total == 'Satellite'], axis=0))\n",
    "plt.plot(range(len(np.nanmean(mat_total, axis=0))), np.nanmean(mat_total[labels_total == 'H3K9me1'], axis=0))\n",
    "plt.plot(range(len(np.nanmean(mat_total, axis=0))), np.nanmean(mat_total[labels_total == 'H3K27me3_random'], axis=0))\n",
    "# plt.plot(range(len(np.nanmean(mat_total, axis=0))), np.nanmean(mat_total[labels_total == 'H3K36me3'], axis=0))\n",
    "# plt.plot(range(len(np.nanmean(mat_total, axis=0))), np.nanmean(mat_total[labels_total == 'H3K9me3'], axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mat_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also include Siva Satellite ZMWs\n",
    "sites_rep1 = './pbrun4_purple/biology_analyses/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.blast.satellite.txt.zmws'\n",
    "sites_rep2 = './pbrun4_purple/biology_analyses/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.blast.satellite.txt' % sample\n",
    "sites_rep3 = './pbrun4_purple/biology_analyses/pbrun6.split.k562_postlyso_chromatin.ccs.blast.satellite.txt.zmws'\n",
    "sites_rep4 = './pbrun4_purple/biology_analyses/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.blast.satellite.txt.zmws'\n",
    "sample='Satellite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels_total))\n",
    "print(len(lengths_total))\n",
    "print(len(mat_total))\n",
    "print(len(auto_cor))\n",
    "print(len(clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering + Fisher's for NRLs Fig. 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def peak_cruncher2(ac1,ac2):\n",
    "    peaks1 = []\n",
    "    peaks2 = []\n",
    "    for i in range(len(ac1)):\n",
    "        mol1 = ac1[i]\n",
    "        mol2 = ac2[i]\n",
    "        peak1, data = signal.find_peaks(mol1, height = 0.10, width = 25)\n",
    "        peak2, data = signal.find_peaks(mol2, height = 0.10, width = 25)\n",
    "        if len(peak1) == 0:\n",
    "            ac1p = np.nan\n",
    "        else:\n",
    "            ac1p = int(peak1[0])\n",
    "        if len(peak2) == 0:\n",
    "            ac2p = np.nan\n",
    "        else:\n",
    "            ac2p = int(peak2[0])\n",
    "        peaks1.append(ac1p)\n",
    "        peaks2.append(ac2p)\n",
    "    peaks1 = np.array(peaks1)\n",
    "    peaks2 = np.array(peaks2)\n",
    "    med1 = np.nanmedian(peaks1)\n",
    "    mad1 = scipy.stats.median_absolute_deviation(peaks1,nan_policy='omit')\n",
    "    mis1 = np.count_nonzero(np.isnan(peaks1))\n",
    "    med2 = np.nanmedian(peaks2)\n",
    "    mad2 = scipy.stats.median_absolute_deviation(peaks2,nan_policy='omit')\n",
    "    mis2 = np.count_nonzero(np.isnan(peaks2))\n",
    "    return (peaks1, peaks2, med1, mad1,mis1, med2, mad2,mis2)\n",
    "\n",
    "def smol_autos(auto_cor1, auto_cor2, clusters, fho):\n",
    "    for i in np.unique(clusters):\n",
    "        mat_new = auto_cor1[clusters == i]\n",
    "        mat_new2 = auto_cor2[clusters == i]\n",
    "        peaks1, peaks2, med1, mad1, mis1, med2, mad2, mis2 = peak_cruncher2(mat_new, mat_new2)\n",
    "        mat_mean = np.mean(mat_new, axis=0)\n",
    "        mat_mean2 = np.mean(mat_new2, axis=0)\n",
    "        overall_peak, data = signal.find_peaks(mat_mean, height = 0.10, width = 25)\n",
    "        overall_peak2, data = signal.find_peaks(mat_mean2, height = 0.10, width = 25)\n",
    "        if len(overall_peak2) == 0:\n",
    "            overall_peak2 = 0\n",
    "        else:\n",
    "            overall_peak2 = overall_peak2[0]\n",
    "        if len(overall_peak) == 0:\n",
    "            overall_peak = 0\n",
    "        else:\n",
    "            overall_peak = overall_peak[0]\n",
    "        length_sub = lengths_total[clusters == i]\n",
    "        if len(length_sub) <= 1000: continue\n",
    "        print(len(peaks1[(~np.isnan(peaks1)) & (~np.isnan(peaks2))]))\n",
    "        print(len(peaks2[(~np.isnan(peaks1)) & (~np.isnan(peaks2))]))\n",
    "\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\" % \\\n",
    "              (i, overall_peak, overall_peak2, mis1, mis2, med1, med2, mad1, mad2, len(length_sub)))\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\" % \\\n",
    "              (i, overall_peak, overall_peak2, mis1, mis2, med1, med2, mad1, mad2, len(length_sub)),file=fho\n",
    "              )\n",
    "        plt.figure(figsize=(5,5))\n",
    "        sns.jointplot(peaks1[(~np.isnan(peaks1)) & (~np.isnan(peaks2))], peaks2[(~np.isnan(peaks1)) & (~np.isnan(peaks2))], kind=\"hex\")\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(5,5))\n",
    "        sns.jointplot(peaks1[(~np.isnan(peaks1)) & (~np.isnan(peaks2))], length_sub[(~np.isnan(peaks1)) & (~np.isnan(peaks2))], kind=\"hex\")        \n",
    "        plt.figure(figsize=(5,5))\n",
    "        sns.jointplot(peaks2[(~np.isnan(peaks1)) & (~np.isnan(peaks2))], length_sub[(~np.isnan(peaks1)) & (~np.isnan(peaks2))], kind=\"hex\")\n",
    "#        plt.scatter(peaks1[(~np.isnan(peaks1)) & (~np.isnan(peaks2))], peaks2[(~np.isnan(peaks1)) & (~np.isnan(peaks2))],alpha=0.25)\n",
    "        plt.show()\n",
    "\n",
    "fho = open('test_out.txt','w')\n",
    "nl = 1000\n",
    "auto_cor, processed = process_xcors(np.nan_to_num(mat_total), max_len=nl, lengths=lengths_total)\n",
    "auto_cor_r, processed = process_xcors(np.nan_to_num(mat_total_r), max_len=nl, lengths=lengths_total)\n",
    "# # # processed = np.nan_to_num(pd.DataFrame(mat_total).rolling(5,axis=1, center=True, min_periods=1).mean())\n",
    "print(len(auto_cor))\n",
    "clusters, sp_obj = cluster_mats(auto_cor[:,:500],res=0.40,neighbors=15)\n",
    "plotter(binarize_sig(mat_total_r), auto_cor, clusters, 5)\n",
    "plotter(binarize_sig(mat_total), auto_cor, clusters, 5)\n",
    "smol_autos(auto_cor, auto_cor_r, clusters, fho)\n",
    "fho.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(auto_cor[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot specific averages for certain clusters, then plot specific cluster of single molecules for effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg1 = pd.Series(np.nanmean(binarize_sig(mat_total[(labels_total == 'H3K36me3') & (clusters == '2')]),axis=0)).rolling(20,center=True, min_periods=1).mean()[::-1]\n",
    "avg2 = pd.Series(np.nanmean(binarize_sig(mat_total[(labels_total == 'H3K9me3') & (clusters == '2')]),axis=0)).rolling(20,center=True, min_periods=1).mean()[::-1]\n",
    "avg3 =pd.Series(np.nanmean(binarize_sig(mat_total[(labels_total == 'H3K4me3') & (clusters == '5')]),axis=0)).rolling(20,center=True, min_periods=1).mean()[::-1]\n",
    "avg4 =pd.Series(np.nanmean(binarize_sig(mat_total[(labels_total == 'H3K27me3') & (clusters == '7')]),axis=0)).rolling(20,center=True, min_periods=1).mean()[::-1]\n",
    "avg5 =pd.Series(np.nanmean(binarize_sig(mat_total[(labels_total == 'Satellite') & (clusters == '0')]),axis=0)).rolling(20,center=True, min_periods=1).mean()[::-1]\n",
    "\n",
    "fho = open('fig5_lineplots_v2.txt','w')\n",
    "for i in range(len(avg1)):\n",
    "    label = 'H3K36me3'\n",
    "    print(\"%s\\t%s\\t%s\" % (i, avg1[i], label),file=fho)\n",
    "    label = 'H3K9me3'\n",
    "    print(\"%s\\t%s\\t%s\" % (i, avg2[i], label),file=fho)\n",
    "    label = 'H3K4me3'\n",
    "    print(\"%s\\t%s\\t%s\" % (i, avg3[i], label),file=fho)\n",
    "    label = 'H3K27me3'\n",
    "    print(\"%s\\t%s\\t%s\" % (i, avg4[i], label),file=fho)\n",
    "    label = 'Satellite'\n",
    "    print(\"%s\\t%s\\t%s\" % (i, avg5[i], label),file=fho)\n",
    "fho.close()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(len(avg1)),avg1)\n",
    "plt.plot(range(len(avg2)),avg2)\n",
    "plt.plot(range(len(avg3)),avg3)\n",
    "plt.plot(range(len(avg4)),avg4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "#plt.plot(range(len(np.nanmean(mat_total, axis=0))), np.nanmean(mat_total[labels_total == 'Satellite'], axis=0)5\n",
    "plt.plot(range(len(np.nanmean(mat_total, axis=0))), np.nanmean(mat_total[(labels_total == 'H3K36me3') & (clusters == '2')], axis=0))\n",
    "plt.plot(range(len(np.nanmean(mat_total, axis=0))), np.nanmean(mat_total[(labels_total == 'H3K9me3') & (clusters == '2')], axis=0))\n",
    "plt.plot(range(len(np.nanmean(mat_total, axis=0))), np.nanmean(mat_total[(labels_total == 'H3K4me3') & (clusters == '5')], axis=0))\n",
    "plt.plot(range(len(np.nanmean(mat_total, axis=0))), np.nanmean(mat_total[(labels_total == 'H3K27me3') & (clusters == '7')], axis=0))\n",
    "\n",
    "# plt.plot(range(len(np.nanmean(mat_total, axis=0))), np.nanmean(mat_total[labels_total == 'H3K36me3'], axis=0))\n",
    "# plt.plot(range(len(np.nanmean(mat_total, axis=0))), np.nanmean(mat_total[labels_total == 'H3K9me3'], axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap representation of clusters for Fig 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fho1 = open('hmap_figure_fig5.txt','w')\n",
    "fho2 = open('hmap_figure_fig5_smoothed.txt','w')\n",
    "for i in np.unique(clusters):\n",
    "    mat_new = auto_cor[clusters == i]\n",
    "    if len(mat_new) < 100: continue\n",
    "    plotter = np.nanmean(mat_new, axis=0)\n",
    "    mat_new2 = mat_total[clusters == i]\n",
    "    hmap = pd.Series(np.nanmean(binarize_sig(mat_new2),axis=0)).rolling(20,center=True, min_periods=1).mean()\n",
    "    for j in range(len(plotter)):\n",
    "        print(\"%s\\t%s\\t%s\" % (i, j, plotter[j]), file=fho1)\n",
    "    for j in range(len(hmap)):\n",
    "        print(\"%s\\t%s\\t%s\" % (i, j, hmap[j]), file=fho2)        \n",
    "    process = pd.DataFrame(binarize_sig(mat_new2)).rolling(5, axis=1, center=True, min_periods=1).mean().values\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(range(len(mat_new2[0])), np.nanmean(process, axis=0))\n",
    "    xlabel('Distance to 5\\'-MNase cut')\n",
    "    ylabel('Average smoothed IPD (frames)')\n",
    "    subplot(143)\n",
    "    imshow(mat_new, interpolation=None, cmap=cm.BuPu)\n",
    "    xlabel('Offset (bp)')\n",
    "    subplot(144)\n",
    "    imshow(process, interpolation=None, cmap=cm.BuPu)\n",
    "    xlabel('Distance')\n",
    "fho1.close()\n",
    "fho2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NRL Estimates for Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def peak_cruncher(autocorrelogram):\n",
    "    peaks = []\n",
    "    heights = []\n",
    "    for i in range(len(autocorrelogram)):\n",
    "        mol = autocorrelogram[i]\n",
    "        peak, data = signal.find_peaks(mol, height = 0.10, width = 25)\n",
    "        if len(peak) == 0: continue\n",
    "        peaks.append(peak[0])\n",
    "        heights.append(data['peak_heights'][0])\n",
    "    #for after I implement filtration via controls\n",
    "#         else:\n",
    "#             peaks.append(0)\n",
    "#             heights.append(0)\n",
    "    med = np.median(peaks)\n",
    "    mod = scipy.stats.mode(peaks)\n",
    "    return (peaks, heights, med, mod)\n",
    "\n",
    "def plotter_v2(new_mat, auto_cor, clusters):\n",
    "    for i in np.unique(clusters):\n",
    "        print(i)\n",
    "        mat_new = auto_cor[clusters == i]\n",
    "        peaks, heights, med, mod = peak_cruncher(mat_new)\n",
    "        mat_mean = np.mean(mat_new, axis=0)\n",
    "        overall_peak, data = signal.find_peaks(mat_mean, height = 0.08, width = 20)\n",
    "        if len(overall_peak) == 0:\n",
    "            overall_peak = 0\n",
    "        else:\n",
    "            overall_peak = overall_peak\n",
    "        print(overall_peak)\n",
    "        length_sub = lengths_total[clusters == i]\n",
    "        plt.figure(figsize=(10,10))\n",
    "        subplot(121)\n",
    "        sns.distplot(peaks)\n",
    "        axvline(overall_peak)\n",
    "        subplot(122)\n",
    "        sns.distplot(length_sub,bins=50)\n",
    "        plt.show()\n",
    "plotter_v2(mat_total, auto_cor, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find Cluster N molecules and plot the long ones. \n",
    "#create dataframe of clusters, hole_nos, lengths\n",
    "df = pd.DataFrame({'clusters':clusters, 'holes':holes_total, 'lengths':lengths_total,'labels':labels_total})\n",
    "#Cluster 6 holes\n",
    "df_6 = df[(df['clusters'] == '2') & (df['lengths'] >= 1000) & (df['labels'] == 'H3K9me3')]['holes'].values\n",
    "print(df_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_cor, processed = process_xcors(np.nan_to_num(mat_total), length)\n",
    "processed = np.nan_to_num(pd.DataFrame(mat_total).rolling(5,axis=1, center=True, min_periods=1).mean())\n",
    "print(len(auto_cor))\n",
    "motif_sites = processed[:,int(length / 2) - 10:int(length / 2) + 10]\n",
    "clusters, sp_obj = cluster_mats(auto_cor,res=0.5,neighbors=15)\n",
    "plotter(binarize_sig(mat_total), auto_cor, clusters, 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "#just do the damn fishers tests in pandas\n",
    "#fho = open('fishers_tests_fig5.txt','w')\n",
    "fho = open('fishers_tests_figs5_sampled.txt','w')\n",
    "cluster_df = {'clusters':clusters, 'labels':labels_total}\n",
    "cluster_df = pd.DataFrame(cluster_df)\n",
    "for i in np.unique(clusters):\n",
    "    if len(labels_total[clusters == i]) < 100:\n",
    "        plot = 0\n",
    "    else:\n",
    "        plot = 1\n",
    "    for j in np.unique(labels_total):\n",
    "        num_clust_lab = len(cluster_df[(cluster_df['clusters'] == i) & (cluster_df['labels'] == j)])\n",
    "        num_clust_notlab = len(cluster_df[(cluster_df['clusters'] == i) & (cluster_df['labels'] != j)])\n",
    "        num_notclust_lab = len(cluster_df[(cluster_df['clusters'] != i) & (cluster_df['labels'] == j)])\n",
    "        num_notclust_notlab = len(cluster_df[(cluster_df['clusters'] != i) & (cluster_df['labels'] != j)])\n",
    "        odds_r, pval = sp.stats.fisher_exact([[num_clust_lab, num_notclust_lab], \\\n",
    "            [num_clust_notlab, num_notclust_notlab]])\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\" % (i, j, num_clust_lab, odds_r, pval,plot), file=fho)\n",
    "fho.close()\n",
    "#labs = np.array(labels)\n",
    "#print(\"Cluster\\t%s\\t%s\\tOR\" % (sample2, sample1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fho1 = open('hmap_figure_sfig5.txt','w')\n",
    "fho2 = open('hmap_figure_sfig5_smoothed.txt','w')\n",
    "for i in np.unique(clusters):\n",
    "    mat_new = auto_cor[clusters == i]\n",
    "    if len(mat_new) < 100: continue\n",
    "    plotted = np.nanmean(mat_new, axis=0)\n",
    "    mat_new2 = mat_total[clusters == i]\n",
    "    hmap = pd.Series(np.nanmean(binarize_sig(mat_new2),axis=0)).rolling(20,center=True, min_periods=1).mean()\n",
    "    for j in range(len(plotted)):\n",
    "        print(\"%s\\t%s\\t%s\" % (i, j, plotted[j]), file=fho1)\n",
    "    for j in range(len(hmap)):\n",
    "        print(\"%s\\t%s\\t%s\" % (i, j, hmap[j]), file=fho2)        \n",
    "    process = pd.DataFrame(binarize_sig(mat_new2)).rolling(5, axis=1, center=True, min_periods=1).mean().values\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(range(len(mat_new2[0])), np.nanmean(process, axis=0))\n",
    "    xlabel('Distance to 5\\'-MNase cut')\n",
    "    ylabel('Average smoothed IPD (frames)')\n",
    "    subplot(143)\n",
    "    imshow(mat_new, interpolation=None, cmap=cm.BuPu)\n",
    "    xlabel('Offset (bp)')\n",
    "    subplot(144)\n",
    "    imshow(process, interpolation=None, cmap=cm.BuPu)\n",
    "    xlabel('Distance')\n",
    "fho1.close()\n",
    "fho2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_cruncher(autocorrelogram):\n",
    "    peaks = []\n",
    "    heights = []\n",
    "    for i in range(len(autocorrelogram)):\n",
    "        mol = autocorrelogram[i]\n",
    "        peak, data = signal.find_peaks(mol, height = 0.10, width = 25)\n",
    "        if len(peak) == 0: continue\n",
    "        peaks.append(peak[0])\n",
    "        heights.append(data['peak_heights'][0])\n",
    "    #for after I implement filtration via controls\n",
    "#         else:\n",
    "#             peaks.append(0)\n",
    "#             heights.append(0)\n",
    "    med = np.median(peaks)\n",
    "    mod = scipy.stats.mode(peaks)\n",
    "    return (peaks, heights, med, mod)\n",
    "\n",
    "def plotter_v2(new_mat, auto_cor, clusters):\n",
    "    for i in np.unique(clusters):\n",
    "        print(i)\n",
    "        mat_new = auto_cor[clusters == i]\n",
    "        peaks, heights, med, mod = peak_cruncher(mat_new)\n",
    "        mat_mean = np.mean(mat_new, axis=0)\n",
    "        overall_peak, data = signal.find_peaks(mat_mean, height = 0.10, width = 25)\n",
    "        if len(overall_peak) == 0:\n",
    "            overall_peak = 0\n",
    "        else:\n",
    "            overall_peak = overall_peak[0]\n",
    "        print(overall_peak)\n",
    "        length_sub = lengths_total[clusters == i]\n",
    "        plt.figure(figsize=(10,10))\n",
    "        subplot(121)\n",
    "        sns.distplot(peaks)\n",
    "        axvline(overall_peak)\n",
    "        subplot(122)\n",
    "        sns.distplot(length_sub,bins=50)\n",
    "        plt.show()\n",
    "plotter_v2(mat_total, auto_cor, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mat_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "#just do the damn fishers tests in pandas\n",
    "#fho = open('fishers_tests_fig5.txt','w')\n",
    "fho = open('fishers_tests_figs5_sampled.txt','w')\n",
    "cluster_df = {'clusters':clusters, 'labels':labels_total}\n",
    "cluster_df = pd.DataFrame(cluster_df)\n",
    "for i in np.unique(clusters):\n",
    "    if len(labels_total[clusters == i]) < 100:\n",
    "        plot = 0\n",
    "    else:\n",
    "        plot = 1\n",
    "    for j in np.unique(labels_total):\n",
    "        num_clust_lab = len(cluster_df[(cluster_df['clusters'] == i) & (cluster_df['labels'] == j)])\n",
    "        num_clust_notlab = len(cluster_df[(cluster_df['clusters'] == i) & (cluster_df['labels'] != j)])\n",
    "        num_notclust_lab = len(cluster_df[(cluster_df['clusters'] != i) & (cluster_df['labels'] == j)])\n",
    "        num_notclust_notlab = len(cluster_df[(cluster_df['clusters'] != i) & (cluster_df['labels'] != j)])\n",
    "        odds_r, pval = sp.stats.fisher_exact([[num_clust_lab, num_notclust_lab], \\\n",
    "            [num_clust_notlab, num_notclust_notlab]])\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\" % (i, j, num_clust_lab, odds_r, pval,plot), file=fho)\n",
    "fho.close()\n",
    "#labs = np.array(labels)\n",
    "#print(\"Cluster\\t%s\\t%s\\tOR\" % (sample2, sample1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_cruncher(autocorrelogram):\n",
    "    peaks = []\n",
    "    heights = []\n",
    "    for i in range(len(autocorrelogram)):\n",
    "        mol = autocorrelogram[i]\n",
    "        peak, data = signal.find_peaks(mol, height = 0.10, width = 30)\n",
    "        if len(peak) == 0: continue\n",
    "        peaks.append(peak[0])\n",
    "        heights.append(data['peak_heights'][0])\n",
    "    #for after I implement filtration via controls\n",
    "#         else:\n",
    "#             peaks.append(0)\n",
    "#             heights.append(0)\n",
    "    med = np.median(peaks)\n",
    "    mod = scipy.stats.mode(peaks)\n",
    "    return (peaks, heights, med, mod)\n",
    "\n",
    "def plotter_v2(new_mat, auto_cor, clusters):\n",
    "    for i in np.unique(clusters):\n",
    "        print(i)\n",
    "        mat_new = auto_cor[clusters == i]\n",
    "        peaks, heights, med, mod = peak_cruncher(mat_new)\n",
    "        mat_mean = np.mean(mat_new, axis=0)\n",
    "        overall_peak, data = signal.find_peaks(mat_mean, height = 0.15, width = 20)\n",
    "        if len(overall_peak) == 0:\n",
    "            overall_peak = 0\n",
    "        else:\n",
    "            overall_peak = overall_peak\n",
    "        print(overall_peak)\n",
    "        length_sub = lengths_total[clusters == i]\n",
    "        plt.figure(figsize=(10,10))\n",
    "        subplot(121)\n",
    "        sns.distplot(peaks)\n",
    "        axvline(overall_peak)\n",
    "        subplot(122)\n",
    "        sns.distplot(length_sub,bins=50)\n",
    "        plt.show()\n",
    "plotter_v2(mat_total, auto_cor, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GC content and dinucleotide bias for clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuc_comp = './pbrun4_purple/'\n",
    "for line in nuc_comp:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.unique(clusters):\n",
    "    length_sub = lengths_total[clusters == i]\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.distplot(length_sub,bins=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "foo = ['apple','apple','orange'] \n",
    "print(Counter(foo))\n",
    "df = pd.DataFrame.from_dict(Counter(foo), orient='index')\n",
    "print(df)\n",
    "df['norm'] = df[0] / df[0].sum()\n",
    "print(scipy.stats.entropy(df['norm'].values))\n",
    "#for label in np.unique(labels_total):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [0,0,0,0,0,0,0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "entropy_df = []\n",
    "entropy_cntrl = [] \n",
    "#Entropy calc\n",
    "for label in np.unique(labels_total):\n",
    "    cluster_labs = clusters[labels_total == label]\n",
    "    if len(cluster_labs) <= 50: continue\n",
    "    control_labs = np.random.choice(clusters, size=len(cluster_labs))\n",
    "#    print(len(cluster_labs))\n",
    "    df_control = pd.DataFrame.from_dict(Counter(control_labs), orient='index')\n",
    "    df = pd.DataFrame.from_dict(Counter(cluster_labs), orient='index')\n",
    "#   print(df)\n",
    "    df['norm'] = df[0] / df[0].sum()\n",
    "    df_control['norm'] = df_control[0] / df_control[0].sum()\n",
    "    entropy_df.append(scipy.stats.entropy(df['norm'].values))\n",
    "    entropy_cntrl.append(scipy.stats.entropy(df_control['norm'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(entropy_df)\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.distplot(entropy_df)\n",
    "sns.distplot(entropy_cntrl)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load up scanpy objects\n",
    "# labels = np.array(labels_total)\n",
    "# #need to recast NaNs as something, we'll say 0 here, which seems defensible.\n",
    "# mat_ann = scanpy.AnnData(X=auto_cor) \n",
    "# mat_ann.obs.index = labels\n",
    "# #sample labels for plotting\n",
    "# mat_ann.obs['samples'] = labels\n",
    "# scanpy.tl.pca(mat_ann)\n",
    "# scanpy.pp.neighbors(mat_ann)\n",
    "# #we'll use leiden clustering (state-of-the-art community detection)\n",
    "scanpy.tl.leiden(mat_ann)\n",
    "# scanpy.tl.umap(mat_ann)\n",
    "#mat_ann.obs['colours'] = color_labels\n",
    "fig, ax = plt.subplots(figsize=(20, 14))\n",
    "scanpy.pl.umap(mat_ann, color='leiden', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use strand information + fragment end info to make metaplots from MNase data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = 'CAGE_bed_flat'\n",
    "test_file1 = open('./pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % sample1, 'r')\n",
    "test_file2 = open('./pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample1, 'r')\n",
    "test_file3 = open('./pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % sample1, 'r')\n",
    "test_file4 = open('./pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample1, 'r')\n",
    "\n",
    "files = [test_file1,test_file2,test_file3,test_file4]\n",
    "#files = [test_file1,test_file2]\n",
    "length = 2000\n",
    "meta = np.zeros(length)\n",
    "for test_file in files:\n",
    "    for line in test_file:\n",
    "        split = line.split()\n",
    "        start = int(split[2])\n",
    "        end = int(split[3])\n",
    "        frag = int(split[4])\n",
    "        if split[-1] == '+':\n",
    "            dist1 = start - frag + int(length / 2)\n",
    "            dist2 = end - frag + int(length / 2)\n",
    "            if 0 <= dist1 < length:\n",
    "                meta[dist1] += 1\n",
    "            if 0 <= dist2 < length:\n",
    "                meta[dist2] += 1\n",
    "        else:\n",
    "            dist1 = frag - start + int(length / 2)\n",
    "            dist2 = frag - end + int(length / 2)\n",
    "            if 0 <= dist1 < length:\n",
    "                meta[dist1] += 1\n",
    "            if 0 <= dist2 < length:\n",
    "                meta[dist2] += 1\n",
    "\n",
    "meta=pd.Series(meta).rolling(33,center=True, min_periods=1).mean()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(len(meta)), meta)\n",
    "plt.show()\n",
    "\n",
    "for test_file in files:\n",
    "    test_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3 Transcription Factors I: MNase Cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs = open('./pbrun4_purple/biology_analyses/tfs.txt')\n",
    "#tfs = ['CTCF\\n']\n",
    "tot_zmws = 247302 + 226280 + 732496 + 747401\n",
    "for line in tfs:\n",
    "    tf = \"%s_flat\" % (line.split()[0])\n",
    "    test_file1 = open('./pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % tf, 'r')\n",
    "    test_file2 = open('./pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % tf, 'r')\n",
    "    test_file3 = open('./pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % tf, 'r')\n",
    "    test_file4 = open('./pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % tf, 'r')\n",
    "    files = [test_file1,test_file2,test_file3,test_file4]\n",
    "    #files = [test_file1,test_file2]\n",
    "    length = 1000\n",
    "    meta = np.zeros(length)\n",
    "    for test_file in files:\n",
    "        for line in test_file:\n",
    "            split = line.split()\n",
    "            start = int(split[2])\n",
    "            end = int(split[3])\n",
    "            frag = int(split[4])\n",
    "            if split[-1] == '+':\n",
    "                dist1 = start - frag + int(length / 2)\n",
    "                dist2 = end - frag + int(length / 2)\n",
    "                if 0 <= dist1 < length:\n",
    "                    meta[dist1] += 1\n",
    "                if 0 <= dist2 < length:\n",
    "                    meta[dist2] += 1\n",
    "            else:\n",
    "                dist1 = frag - start + int(length / 2)\n",
    "                dist2 = frag - end + int(length / 2)\n",
    "                if 0 <= dist1 < length:\n",
    "                    meta[dist1] += 1\n",
    "                if 0 <= dist2 < length:\n",
    "                    meta[dist2] += 1\n",
    "    meta=pd.Series(meta).rolling(5,center=True, min_periods=1).mean()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(range(len(meta)), meta)\n",
    "    plt.show()\n",
    "    for test_file in files:\n",
    "        test_file.close()\n",
    "    fho = open('%s_MNase.txt' % tf,'w')\n",
    "    for i in range(len(meta)):\n",
    "        print(\"%s\\t%s\\t%s\" % (i, meta[i] / tot_zmws * 1000000, tf), file=fho)\n",
    "    fho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3 Transcription Factors II: m6dA signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 500\n",
    "\n",
    "tfs = open('./pbrun4_purple/biology_analyses/tfs.txt')\n",
    "#tfs = ['CTCF\\n']\n",
    "lengths_total = np.array([0])\n",
    "labels_total = np.array([0])\n",
    "mat_total = []\n",
    "\n",
    "for line in tfs:\n",
    "    tf = line.split()[0]\n",
    "    sample = '%s' % tf\n",
    "    sites_rep1 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_flat_zmws' % sample\n",
    "    sites_rep2 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_flat_zmws' % sample\n",
    "    sites_rep3 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_flat_zmws' % sample\n",
    "    sites_rep4 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_flat_zmws' % sample\n",
    "    cntrl_sites_rep1 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_random_flat_zmws' % sample\n",
    "    cntrl_sites_rep2 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_random_flat_zmws' % sample\n",
    "    cntrl_sites_rep3 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_random_flat_zmws' % sample\n",
    "    cntrl_sites_rep4 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_random_flat_zmws' % sample\n",
    "    mat_rep1, lengths, labels = distill_tipds_flat_centered(rep1, rep1_valid, sites_rep1, length, sample)\n",
    "    mat_rep2, lengths2, labels2 = distill_tipds_flat_centered(rep2, rep2_valid, sites_rep2, length, sample)\n",
    "    mat_rep3, lengths3, labels3 = distill_tipds_flat_centered(rep3, rep3_valid, sites_rep3, length, sample)\n",
    "    mat_rep4, lengths4, labels4 = distill_tipds_flat_centered(rep4, rep4_valid, sites_rep4, length, sample)\n",
    "    cntrl_mat_rep1, cntrl_lengths, cntrl_labels = distill_tipds_flat_centered(rep1, rep1_valid, cntrl_sites_rep1, length, \"%s_control\" % sample ,subsample=len(lengths))        \n",
    "    cntrl_mat_rep2, cntrl_lengths2, cntrl_labels2 = distill_tipds_flat_centered(rep2, rep2_valid, cntrl_sites_rep2, length, \"%s_control\" % sample,subsample=len(lengths2))    \n",
    "    cntrl_mat_rep3, cntrl_lengths3, cntrl_labels3 = distill_tipds_flat_centered(rep3, rep3_valid, cntrl_sites_rep3, length, \"%s_control\" % sample,subsample=len(lengths3))        \n",
    "    cntrl_mat_rep4, cntrl_lengths4, cntrl_labels4 = distill_tipds_flat_centered(rep4, rep4_valid, cntrl_sites_rep4, length, \"%s_control\" % sample,subsample=len(lengths4))    \n",
    "    mat_msix = np.vstack((mat_rep1, mat_rep2, mat_rep3, mat_rep4))\n",
    "    mat_msix_control = np.vstack((cntrl_mat_rep1, cntrl_mat_rep2, cntrl_mat_rep3, cntrl_mat_rep4))\n",
    "    process = pd.Series(np.nanmean(binarize_sig(mat_msix), axis=0)).rolling(20,center=True,min_periods=1).mean()\n",
    "    process_neg = pd.Series(np.nanmean(binarize_sig(mat_msix_control), axis=0)).rolling(20,center=True,min_periods=1).mean()\n",
    "#     process = np.nanmean(binarize_sig(pd.DataFrame(mat_msix).rolling(10, axis=1, center=True, min_periods=1).mean()), axis=0)\n",
    "#     process_neg = np.nanmean(binarize_sig(pd.DataFrame(mat_msix_control).rolling(10, axis=1, center=True, min_periods=1).mean()), axis=0)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    subplot(121)\n",
    "    plt.plot(range(len(process)), process)\n",
    "    subplot(122)\n",
    "    plt.plot(range(len(process_neg)), process_neg)\n",
    "    plt.show()\n",
    "    fho=open('%s_m6A.txt' % tf,'w')\n",
    "    for i in range(len(process)):\n",
    "        print(\"%s\\t%s\\t%s\\tsite\" % (i, process[i], tf),file=fho)\n",
    "        print(\"%s\\t%s\\t%s\\tcontrol\" % (i, process_neg[i], tf),file=fho)\n",
    "    fho.close()\n",
    "    \n",
    "    \n",
    "\n",
    "tfs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctcf_m6da = np.nanmean(process1, axis=0)\n",
    "myc_m6da = np.nanmean(process2, axis=0)\n",
    "fho=open('ctcf_m6a.txt','w')\n",
    "for i in range(len(ctcf_m6da)):\n",
    "    print(\"%s\\t%s\\tCTCF\" % (i, ctcf_m6da[i]),file=fho)\n",
    "fho.close()\n",
    "\n",
    "fho=open('myc_m6a.txt','w')\n",
    "for i in range(len(ctcf_m6da)):\n",
    "    print(\"%s\\t%s\\tc-MYC\" % (i, ctcf_m6da[i]),file=fho)\n",
    "fho.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on a version of distill_flat that centers the molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_tipds_flat_centered(tipds, valid_zmws, sitelist, length, label):\n",
    "    sites = open(sitelist,'r')\n",
    "    hole_nos = {}\n",
    "    for i in sites:\n",
    "        split = i.split()\n",
    "        if int(split[0]) not in valid_zmws: continue\n",
    "        #check to make sure the feature is in the middle of the read \n",
    "        #and enough on both sites\n",
    "        if int(split[2]) <= int(split[4]) <= int(split[3]):\n",
    "            index1 = int(split[4]) - int(split[2])\n",
    "            index2 = int(split[3]) - int(split[4])\n",
    "            strand = split[-1]\n",
    "            if index1 > (length / 2) and index2 > (length / 2):\n",
    "                hole_nos[split[0]] = (index1, strand)\n",
    "    sites.close()\n",
    "    new_mat = np.zeros(length)\n",
    "    lengths = []\n",
    "    labs = []\n",
    "    for hole in hole_nos:\n",
    "        read = tipds[int(hole)]\n",
    "        index,strand = hole_nos[hole]\n",
    "        if strand == \"+\":\n",
    "            extract = tipds[int(hole)][int(index - (length / 2)): int(index + (length / 2))]\n",
    "            if len(extract) != length: continue\n",
    "            new_mat = np.vstack((new_mat, extract))\n",
    "            labs.append(label)\n",
    "            lengths.append(len(tipds[int(hole)]))\n",
    "        else:\n",
    "            extract = tipds[int(hole)][::-1][int(index - (length / 2)):int(index + (length / 2))]\n",
    "            if len(extract) != length: continue\n",
    "            new_mat = np.vstack((new_mat, extract))\n",
    "            labs.append(label)\n",
    "            lengths.append(len(tipds[int(hole)]))\n",
    "    return (new_mat[1:], np.array(lengths), np.array(labs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing out the new centered function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 500\n",
    "#BINARIZED\n",
    "k562_rep1 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/binarized/pbrun4_purple_k562_postlyso_chromatin_bingmm'\n",
    "k562_rep2 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/binarized/pbrun4_purple_k562_postlyso_chromatin_rep2_bingmm'\n",
    "\n",
    "k562_rep3 = '/avicenna/vramani/analyses/pacbio/pbrun6/processed/binarized/pbrun6_k562_postlyso_chromatin_bingmm'\n",
    "k562_rep4 = '/avicenna/vramani/analyses/pacbio/pbrun6/processed/binarized/pbrun6_k562_postlyso_chromatin_rep2_bingmm'\n",
    "\n",
    "# k562_neg_rep1 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/binarized/pbrun4_purple_k562_postlyso_neg_bingmm'\n",
    "# k562_neg_rep2 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/binarized/pbrun4_purple_k562_postlyso_neg_rep2_bingmm'\n",
    "\n",
    "# k562_neg_rep3 = '/avicenna/vramani/analyses/pacbio/pbrun6/processed/binarized/pbrun6_k562_postlyso_neg_bingmm'\n",
    "# k562_neg_rep4 = '/avicenna/vramani/analyses/pacbio/pbrun6/processed/binarized/pbrun6_k562_postlyso_neg_rep2_bingmm'\n",
    "\n",
    "rep1, rep1_valid = eat_pickle_binary(\"%s.pickle\" % k562_rep1)\n",
    "rep2, rep2_valid = eat_pickle_binary(\"%s.pickle\" % k562_rep2)\n",
    "rep3, rep3_valid = eat_pickle_binary(\"%s.pickle\" % k562_rep3)\n",
    "rep4, rep4_valid = eat_pickle_binary(\"%s.pickle\" % k562_rep4)\n",
    "\n",
    "\n",
    "\n",
    "#NONBINARIZED\n",
    "# k562_rep1 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/onlyT/pbrun4_purple_k562_postlyso_chromatin_onlyT'\n",
    "# k562_rep2 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/onlyT/pbrun4_purple_k562_postlyso_chromatin_rep2_onlyT'\n",
    "\n",
    "# k562_rep3 = '/avicenna/vramani/analyses/pacbio/pbrun6/processed/onlyT/pbrun6_k562_postlyso_chromatin_onlyT'\n",
    "# k562_rep4 = '/avicenna/vramani/analyses/pacbio/pbrun6/processed/onlyT/pbrun6_k562_postlyso_chromatin_rep2_onlyT'\n",
    "\n",
    "# # k562_neg_rep1 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/onlyT/pbrun4_purple_k562_postlyso_neg_onlyT'\n",
    "# # k562_neg_rep2 = '/avicenna/vramani/analyses/pacbio/pbrun4_purple/processed/onlyT/pbrun4_purple_k562_postlyso_neg_rep2_onlyT'\n",
    "\n",
    "# # k562_neg_rep3 = '/avicenna/vramani/analyses/pacbio/pbrun6/processed/onlyT/pbrun6_k562_postlyso_neg_onlyT'\n",
    "# # k562_neg_rep4 = '/avicenna/vramani/analyses/pacbio/pbrun6/processed/onlyT/pbrun6_k562_postlyso_neg_rep2_onlyT'\n",
    "\n",
    "# rep1, rep1_valid = eat_pickle(\"%s.pickle\" % k562_rep1, \"%s_zmwinfo.pickle\" % k562_rep1)\n",
    "# rep2, rep2_valid = eat_pickle(\"%s.pickle\" % k562_rep2, \"%s_zmwinfo.pickle\" % k562_rep2)\n",
    "# rep3, rep3_valid = eat_pickle(\"%s.pickle\" % k562_rep3, \"%s_zmwinfo.pickle\" % k562_rep3)\n",
    "# rep4, rep4_valid = eat_pickle(\"%s.pickle\" % k562_rep4, \"%s_zmwinfo.pickle\" % k562_rep4)\n",
    "\n",
    "\n",
    "sample1 = 'TSS_human_exprs.flat.unexpressed_filtered_flat'\n",
    "sites_rep1 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % sample1\n",
    "sites_rep2 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample1\n",
    "sites_rep3 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % sample1\n",
    "sites_rep4 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample1\n",
    "\n",
    "# sites_neg_rep1 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_neg--k562_postlyso_neg.ccs.aligned.sorted.bam.%s_zmws' % sample1\n",
    "# sites_neg_rep2 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_neg_rep2--k562_postlyso_neg_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample1\n",
    "\n",
    "sample2 = 'TSS_human_exprs.flat.0.8_filtered_flat' \n",
    "cntrl_sites_rep1 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % sample2\n",
    "cntrl_sites_rep2 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample2\n",
    "cntrl_sites_rep3 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % sample2\n",
    "cntrl_sites_rep4 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample2\n",
    "\n",
    "# cntrl_sites_neg_rep1 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_neg--k562_postlyso_neg.ccs.aligned.sorted.bam.%s_zmws' % sample2\n",
    "# cntrl_sites_neg_rep2 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_neg_rep2--k562_postlyso_neg_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample2\n",
    "\n",
    "\n",
    "\n",
    "# neg_rep1, neg_rep1_valid = eat_pickle(\"%s.pickle\" % k562_neg_rep1, \"%s_zmwinfo.pickle\" % k562_neg_rep1)\n",
    "# neg_rep2, neg_rep2_valid = eat_pickle(\"%s.pickle\" % k562_neg_rep2, \"%s_zmwinfo.pickle\" % k562_neg_rep2)\n",
    "\n",
    "mat_rep1, lengths, labels = distill_tipds_flat_centered(rep1, rep1_valid, sites_rep1, length, sample1)\n",
    "mat_rep2, lengths2, labels2 = distill_tipds_flat_centered(rep2, rep2_valid, sites_rep2, length, sample1)\n",
    "mat_rep3, lengths3, labels3 = distill_tipds_flat_centered(rep3, rep3_valid, sites_rep3, length, sample1)\n",
    "mat_rep4, lengths4, labels4 = distill_tipds_flat_centered(rep4, rep4_valid, sites_rep4, length, sample1)\n",
    "\n",
    "mat_rep5, lengths5, labels5 = distill_tipds_flat_centered(rep1, rep1_valid, cntrl_sites_rep1, length, sample2)\n",
    "mat_rep6, lengths6, labels6 = distill_tipds_flat_centered(rep2, rep2_valid, cntrl_sites_rep2, length, sample2)\n",
    "mat_rep7, lengths7, labels7 = distill_tipds_flat_centered(rep3, rep3_valid, cntrl_sites_rep3, length, sample2)\n",
    "mat_rep8, lengths8, labels8 = distill_tipds_flat_centered(rep4, rep4_valid, cntrl_sites_rep4, length, sample2)\n",
    "\n",
    "\n",
    "# neg_mat_rep1, l1, labels5 = distill_tipds_flat(neg_rep1, neg_rep1_valid, sites_neg_rep1, length, \"%s_neg\" % sample1)\n",
    "# neg_mat_rep2, l1, labels6 = distill_tipds_flat(neg_rep2, neg_rep2_valid, sites_neg_rep2, length, \"%s_neg\" % sample1)\n",
    "# neg_mat_rep3, l1, labels7 = distill_tipds_flat(neg_rep1, neg_rep1_valid, cntrl_sites_neg_rep1, length, \"%s_neg\" % sample2)\n",
    "# neg_mat_rep4, l1, labels8 = distill_tipds_flat(neg_rep2, neg_rep2_valid, cntrl_sites_neg_rep2, length, \"%s_neg\" % sample2)\n",
    "\n",
    "\n",
    "#mat_total = np.vstack((mat_rep1,mat_rep2,mat_rep3,mat_rep4, neg_mat_rep1, neg_mat_rep2, neg_mat_rep3, neg_mat_rep4))\n",
    "mat_total = np.vstack((mat_rep1,mat_rep2,mat_rep3,mat_rep4,mat_rep5,mat_rep6,mat_rep7,mat_rep8))\n",
    "mat_samp1 = np.vstack((mat_rep1,mat_rep2, mat_rep3,mat_rep4))\n",
    "mat_samp2 = np.vstack((mat_rep5,mat_rep6, mat_rep7,mat_rep8))\n",
    "\n",
    "\n",
    "# mat_samp1 = np.vstack((mat_rep1,mat_rep2,mat_rep3,mat_rep4))\n",
    "# mat_samp2 = np.vstack((mat_rep5,mat_rep6,mat_rep7,mat_rep8))                      \n",
    "mat_rep1,mat_rep2,mat_rep3,mat_rep4,mat_rep5,mat_rep6,mat_rep7,mat_rep8 = [0,0,0,0,0,0,0,0]\n",
    "\n",
    "lengths = np.concatenate((lengths, lengths2,lengths3,lengths4,lengths5,lengths6,lengths7,lengths8))\n",
    "#labels = np.concatenate((labels, labels2,labels5,labels6))\n",
    "labels = np.concatenate((labels, labels2,labels3,labels4,labels5,labels6,labels7,labels8))\n",
    "#lengths2, labels2, lengths3, labels3, lengths4, labels4 = (0,0,0,0,0,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process1 = pd.DataFrame(mat_samp1).rolling(33,axis=1, center=True, min_periods=1).mean()\n",
    "process2 = pd.DataFrame(mat_samp2).rolling(33,axis=1, center=True, min_periods=1).mean()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "subplot(121)\n",
    "plt.plot(range(len(mat_samp1[0])), np.nanmean(process1, axis=0))\n",
    "subplot(122)\n",
    "plt.plot(range(len(mat_samp2[0])), np.nanmean(process2, axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "auto_cor = process_xcors(mat_total, 500)\n",
    "clusters = cluster_mats(mat_total)\n",
    "plotter(mat_total, auto_cor, clusters)\n",
    "\n",
    "labs = np.array(labels)\n",
    "print(\"Cluster\\t%s\\t%s\\tOR\" % (sample2, sample1))\n",
    "for i in np.unique(clusters):\n",
    "    sub = labs[clusters == i]\n",
    "    num_control = len(sub[sub == sample2]) / len(labs[labs == sample2])\n",
    "    tot1 += num_control\n",
    "    num_ctcf = len(sub[sub == sample1]) / len(labs[labs==sample1])\n",
    "    tot2 += num_ctcf\n",
    "    #num_neg = len(sub[sub == '%s_neg' % sample1]) / len(labs[labs=='%s_neg' % sample1])\n",
    "    lo = np.log(num_ctcf / num_control)\n",
    "    #num_neg2 = len(sub[sub == '%s_neg' % sample2]) / len(labs[labs=='%s_neg' % sample2])\n",
    "    #print(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\" % (i, num_control, num_ctcf, num_neg, num_neg2, lo))\n",
    "    print(\"%s\\t%s\\t%s\\t%s\" % (i, num_control, num_ctcf,lo)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'd like to sample molecules so that we have ~1000 per sequencing run per replicate per domain\n",
    "length = 500\n",
    "\n",
    "tfs = open('./pbrun4_purple/biology_analyses/all_bed_files.txt')\n",
    "lengths_total = np.array([0])\n",
    "labels_total = np.array([0])\n",
    "mat_total = np.zeros(length)\n",
    "for line in tfs:\n",
    "    tf = line.split()[0]\n",
    "    sample = '%s' % tf\n",
    "    sites_rep1 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "    sites_rep2 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "    sites_rep3 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "    sites_rep4 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "    mat_rep1, lengths, labels = distill_tipds_bed(rep1, rep1_valid, sites_rep1, length, sample, subsample=1000)\n",
    "    mat_rep2, lengths2, labels2 = distill_tipds_bed(rep2, rep2_valid, sites_rep2, length, sample,subsample=1000)\n",
    "    mat_rep3, lengths3, labels3 = distill_tipds_bed(rep3, rep3_valid, sites_rep3, length, sample,subsample=1000)\n",
    "    mat_rep4, lengths4, labels4 = distill_tipds_bed(rep4, rep4_valid, sites_rep4, length, sample,subsample=1000)\n",
    "    mat_total = np.vstack((mat_total, mat_rep1,mat_rep2,mat_rep3,mat_rep4))\n",
    "    lengths_total = np.concatenate((lengths_total, lengths, lengths2,lengths3,lengths4))\n",
    "    labels_total = np.concatenate((labels_total, labels, labels2,labels3,labels4))\n",
    "\n",
    "mat_total = mat_total[1:]\n",
    "lengths_total = lengths_total[1:]\n",
    "labels_total = labels_total[1:]\n",
    "\n",
    "tfs.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting TF code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_tipds_flat_centered_test(tipds, valid_zmws, sitelist, length, label,subsample=0):\n",
    "    sites = open(sitelist,'r')\n",
    "    hole_nos = {}\n",
    "    for i in sites:\n",
    "        split = i.split()\n",
    "        if int(split[0]) not in valid_zmws: continue\n",
    "        #check to make sure the feature is in the middle of the read \n",
    "        #and enough on both sites\n",
    "        if int(split[2]) <= int(split[4]) <= int(split[3]):\n",
    "            index1 = int(split[4]) - int(split[2])\n",
    "            index2 = int(split[3]) - int(split[4])\n",
    "            strand = split[-1]\n",
    "            if index1 > (length / 2) and index2 > (length / 2):\n",
    "                hole_nos[split[0]] = (index1, strand, int(split[2]), int(split[3]))\n",
    "    sites.close()\n",
    "    lengths = []\n",
    "    labs = []\n",
    "    reads = []\n",
    "    mno = 0\n",
    "    for hole in hole_nos:\n",
    "        read = tipds[int(hole)]\n",
    "        index,strand, start, end = hole_nos[hole]\n",
    "        print(\"%s\\t%s\" % (len(read), (end-start)))\n",
    "        if strand == \"+\":\n",
    "            extract = tipds[int(hole)][int(index - (length / 2)): int(index + (length / 2))]\n",
    "            if len(extract) != length: continue\n",
    "            reads.append(extract)\n",
    "            labs.append(label)\n",
    "            lengths.append(len(tipds[int(hole)]))\n",
    "        else:\n",
    "            extract = tipds[int(hole)][::-1][int(index - (length / 2)):int(index + (length / 2))]\n",
    "            if len(extract) != length: continue\n",
    "            reads.append(extract)\n",
    "            labs.append(label)\n",
    "            lengths.append(len(tipds[int(hole)]))\n",
    "        if subsample != 0:\n",
    "            mno += 1\n",
    "            if mno == subsample: break\n",
    "    new_mat = np.vstack(reads)\n",
    "    return (new_mat, np.array(lengths), np.array(labs))\n",
    "\n",
    "length = 500\n",
    "\n",
    "#tfs = open('./pbrun4_purple/biology_analyses/tfs.txt')\n",
    "tfs = ['REST\\n']\n",
    "lengths_total = np.array([0])\n",
    "labels_total = np.array([0])\n",
    "mat_total = []\n",
    "\n",
    "for line in tfs:\n",
    "    tf = line.split()[0]\n",
    "    sample = '%s' % tf\n",
    "    sites_rep1 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_flat_zmws' % sample\n",
    "    sites_rep2 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_flat_zmws' % sample\n",
    "    sites_rep3 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_flat_zmws' % sample\n",
    "    sites_rep4 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_flat_zmws' % sample\n",
    "    mat_rep1, lengths, labels = distill_tipds_flat_centered_test(rep1, rep1_valid, sites_rep1, length, sample)    \n",
    "    mat_rep2, lengths2, labels2 = distill_tipds_flat_centered_test(rep2, rep2_valid, sites_rep2, length, sample)\n",
    "    mat_rep3, lengths3, labels3 = distill_tipds_flat_centered_test(rep3, rep3_valid, sites_rep3, length, sample)\n",
    "    mat_rep4, lengths4, labels4 = distill_tipds_flat_centered_test (rep4, rep4_valid, sites_rep4, length, sample)\n",
    "    mat_total.extend((mat_rep1,mat_rep2,mat_rep3,mat_rep4,cntrl_mat_rep1,cntrl_mat_rep2,cntrl_mat_rep3,cntrl_mat_rep4))\n",
    "#    mat_total.extend((mat_rep1,mat_rep2,mat_rep3,mat_rep4))\n",
    "\n",
    "    print(\"%s\\t%s\" % (sample, (len(mat_rep1)+len(mat_rep2)+len(mat_rep3)+len(mat_rep4))))\n",
    "    print(\"%s_control\\t%s\" % (sample, (len(cntrl_mat_rep1)+len(cntrl_mat_rep2)+len(cntrl_mat_rep3)+len(cntrl_mat_rep4))))\n",
    "    lengths_total = np.concatenate((lengths_total, lengths, lengths2,lengths3,lengths4, cntrl_lengths, cntrl_lengths2,cntrl_lengths3,cntrl_lengths4))\n",
    "    labels_total = np.concatenate((labels_total, labels, labels2,labels3,labels4,cntrl_labels, cntrl_labels2, cntrl_labels3, cntrl_labels4))\n",
    "#     lengths_total = np.concatenate((lengths_total, lengths, lengths2,lengths3,lengths4))\n",
    "#     labels_total = np.concatenate((labels_total, labels, labels2,labels3,labels4))\n",
    "    \n",
    "mat_total = np.vstack(mat_total)\n",
    "lengths_total = lengths_total[1:]\n",
    "labels_total = labels_total[1:]\n",
    "\n",
    "tfs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Stergachis et al data for concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def eat_pickle_filt(pick, samp_label,length, valid_zmws, min_len = 500, samp= 0):\n",
    "    valid = {}\n",
    "    zmws = open(valid_zmws)\n",
    "    for line in zmws:\n",
    "        split = line.split()\n",
    "        valid[split[0]] = split[-1]\n",
    "    #print(valid)\n",
    "    with open(pick, 'rb') as fout:\n",
    "        tipds = pickle.load(fout, encoding=\"latin1\")    \n",
    "    nmol = 0\n",
    "    lengths = []\n",
    "    reads_f = []\n",
    "    labels = []\n",
    "    holes = []\n",
    "    mean_probs = []\n",
    "    index = 0\n",
    "    print(valid)\n",
    "    for hole in tipds:\n",
    "        if str(hole) not in valid: continue\n",
    "        print(hole)\n",
    "        read = tipds[int(hole)]\n",
    "        if len(read) < min_len: continue\n",
    "        if len(tipds[int(hole)]) >= length:\n",
    "            reads_f.append(read[:length])\n",
    "            lengths.append(len(tipds[int(hole)]))\n",
    "        elif len(tipds[int(hole)]) < length:\n",
    "            continue\n",
    "        labels.append(valid[str(hole)])\n",
    "        nmol += 1\n",
    "        index += 1\n",
    "        if samp != 0:\n",
    "            if nmol == samp: break\n",
    "    new_mat = np.vstack(reads_f)\n",
    "    tipds = 0\n",
    "    return (new_mat, lengths, labels)\n",
    "\n",
    "rep1=0\n",
    "length = 1000\n",
    "k562_rep1_stam = '/avicenna/vramani/analyses/pacbio/pbrun_STAM/processed/binarized/pbrun_STAM_k562_chromatin_500U_Hia5_run1_bingmm'\n",
    "k562_rep1_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun_STAM/processed/onlyT/pbrun_STAM_k562_chromatin_500U_Hia5_run1_onlyT_zmwinfo'\n",
    "valid_zmws = '/avicenna/vramani/analyses/pacbio/pbrun_STAM/ccs/total_zmws.txt'\n",
    "rep1, l1, lbl = eat_pickle_filt(\"%s.pickle\" % k562_rep1_stam, 'K562_Rep1', length, valid_zmws, samp = 250000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(rep1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "imshow(np.nan_to_num(rep1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xcors(new_mat, max_len=1000, lengths=False):\n",
    "    process = np.nan_to_num(pd.DataFrame(new_mat).rolling(33,axis=1, center=True, min_periods=1).mean())\n",
    "    if isinstance(lengths, np.ndarray):\n",
    "        auto_cor = xcor(process, process, maxlen=max_len, lengths=lengths)\n",
    "    else:\n",
    "        auto_cor = xcor(process, process, maxlen=max_len)\n",
    "    process = None\n",
    "    return auto_cor\n",
    "\n",
    "def plotter(new_mat, auto_cor, clusters,smooth):\n",
    "    fho = open('plot_stergachis_clusters.txt','w')\n",
    "    for i in np.unique(clusters):\n",
    "        print(i)\n",
    "        mat_new = auto_cor[clusters == i]\n",
    "        autocor_mean = np.nanmean(mat_new,axis=0)\n",
    "        for j in range(len(autocor_mean)):\n",
    "            print(\"%s\\t%s\\t%s\" % (j, autocor_mean[j],i),file=fho)\n",
    "        print(len(mat_new))\n",
    "        mat_new2 = new_mat[clusters == i]\n",
    "        mat_new2 = pd.DataFrame(mat_new2).rolling(smooth, axis=1, center=True, min_periods=1).mean()\n",
    "        plt.figure(figsize=(30,10))\n",
    "        subplot(121)\n",
    "        plt.plot(range(len(mat_new[0])), np.nanmean(mat_new, axis=0))\n",
    "        peak, data = signal.find_peaks(np.nanmean(mat_new, axis=0), height = 0.10, width = 25)\n",
    "        if len(peak) == 0:\n",
    "            print(\"undef\")\n",
    "        else:\n",
    "            print(peak[0])\n",
    "        xlabel('Offset (bp)')\n",
    "        ylabel('Pearson\\'s r')\n",
    "        subplot(122)\n",
    "        plt.plot(range(len(np.nanmean(mat_new2, axis=0))), np.nanmean(mat_new2, axis=0))\n",
    "        xlabel('Distance to 5\\'-MNase cut')\n",
    "        ylabel('Average smoothed IPD (frames)')\n",
    "        plt.show()\n",
    "    fho.close()\n",
    "\n",
    "# auto_cor = np.nan_to_num(process_xcors(rep1,max_len=5000))\n",
    "# plt.figure(figsize=(20,20))\n",
    "# imshow(np.nan_to_num(auto_cor))\n",
    "# plt.show()\n",
    "# mat = scanpy.AnnData(X=auto_cor)\n",
    "# scanpy.tl.pca(mat)\n",
    "# scanpy.pp.neighbors(mat,metric='correlation',n_neighbors=10)\n",
    "# scanpy.tl.leiden(mat,resolution=0.4)\n",
    "# clusters = np.array(mat.obs['leiden'])\n",
    "plotter(np.array(rep1[:,:500]), auto_cor[:,:500], clusters, 33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data frame of labels\n",
    "total_dataframe = pd.DataFrame({'cluster_id': clusters, 'chromatin_type': lbl \\\n",
    "    })\n",
    "total_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "# test = total_dataframe[(total_dataframe['chromatin_type'] == 'H3K9me3') | (total_dataframe['chromatin_type'] == 'H3K9me1')]['total_index']\n",
    "# print(len(test))\n",
    "# print(len(np.unique(test)))\n",
    "\n",
    "fho = open('fishers_tests_fig_sterg.txt','w')\n",
    "for i in np.unique(total_dataframe['cluster_id']):\n",
    "    if i == '7': continue\n",
    "    for j in np.unique(total_dataframe['chromatin_type']):\n",
    "        num_clust_lab = len(total_dataframe[(total_dataframe['cluster_id'] == i) & (total_dataframe['chromatin_type'] == j)])\n",
    "        num_clust_notlab = len(total_dataframe[(total_dataframe['cluster_id'] == i) & (total_dataframe['chromatin_type'] != j)])\n",
    "        num_notclust_lab = len(total_dataframe[(total_dataframe['cluster_id'] != i) & (total_dataframe['chromatin_type'] == j)])\n",
    "        num_notclust_notlab = len(total_dataframe[(total_dataframe['cluster_id'] != i) & (total_dataframe['chromatin_type'] != j)])\n",
    "        odds_r, pval = sp.stats.fisher_exact([[num_clust_lab, num_notclust_lab], \\\n",
    "            [num_clust_notlab, num_notclust_notlab]])\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\" % (i, j, num_clust_lab, odds_r, pval), file=fho)\n",
    "fho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicate analyses for the reviews (Replicate 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def find_indices(sites, zmw_dict, samp_label, chrom_label):\n",
    "    lines = open(sites)\n",
    "    indices = []\n",
    "    for entry in lines:\n",
    "        split = entry.split()\n",
    "        hole = \"%s_%s\" % (samp_label, split[0])\n",
    "        if hole in zmw_dict:\n",
    "            indices.append(zmw_dict[hole])\n",
    "    return indices\n",
    "        \n",
    "#print(list(zmw2index.keys())[:100])\n",
    "cluster_file = open('final_cluster_labels_total.txt')\n",
    "lengths_total = np.load('length_filtered_lengths.npy')\n",
    "clusters = []\n",
    "for line in cluster_file:\n",
    "    split = line.split()\n",
    "    clusters.append(split[1])\n",
    "clusters = np.array(clusters)\n",
    "\n",
    "#total_dataframe = pd.DataFrame({'cluster_id': [], 'chromatin_type': [], 'total_index': [], 'frag_length' : []})\n",
    "cluster_ids = []\n",
    "chromatin_types = []\n",
    "total_index = []\n",
    "frag_length = []\n",
    "\n",
    "tfs = open('./pbrun4_purple/biology_analyses/new_beds.txt')\n",
    "#tfs = ['coopfoots\\n']\n",
    "\n",
    "for line in tfs:\n",
    "    tf = line.split()[0]\n",
    "    sample = '%s' % tf\n",
    "    print(sample)\n",
    "    sites_rep1 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "#     sites_rep2 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "    sites_rep3 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "#     sites_rep4 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "    rep1_indices = find_indices(sites_rep1, zmw2index, 'K562_Rep1', tf)\n",
    "#     rep2_indices = find_indices(sites_rep2, zmw2index, 'K562_Rep2', tf)\n",
    "    rep3_indices = find_indices(sites_rep3, zmw2index, 'K562_Rep3', tf)\n",
    "#     rep4_indices = find_indices(sites_rep4, zmw2index, 'K562_Rep4', tf)\n",
    "#    tot_indices = np.concatenate((rep1_indices,rep2_indices,rep3_indices,rep4_indices))    \n",
    "    tot_indices = np.concatenate((rep1_indices,rep3_indices))    \n",
    "    cluster_indices = clusters[tot_indices]\n",
    "    lengths_indices = lengths_total[tot_indices]\n",
    "    tot_labels = np.repeat(tf, len(tot_indices))\n",
    "    cluster_ids.append(cluster_indices)\n",
    "    chromatin_types.append(tot_labels)\n",
    "    frag_length.append(lengths_indices)\n",
    "    total_index.append(tot_indices)\n",
    "\n",
    "print(cluster_ids)    \n",
    "\n",
    "#Also include Siva Satellite ZMWs\n",
    "sites_rep1 = './pbrun4_purple/biology_analyses/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.blast.satellite.txt.zmws'\n",
    "#sites_rep2 = './pbrun4_purple/biology_analyses/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.blast.satellite.txt.zmws'\n",
    "sites_rep3 = './pbrun4_purple/biology_analyses/pbrun6.split.k562_postlyso_chromatin.ccs.blast.satellite.txt.zmws'\n",
    "#sites_rep4 = './pbrun4_purple/biology_analyses/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.blast.satellite.txt.zmws'\n",
    "tf='Satellite'\n",
    "rep1_indices = find_indices(sites_rep1, zmw2index, 'K562_Rep1', tf)\n",
    "#rep2_indices = find_indices(sites_rep2, zmw2index, 'K562_Rep2', tf)\n",
    "rep3_indices = find_indices(sites_rep3, zmw2index, 'K562_Rep3', tf)\n",
    "#rep4_indices = find_indices(sites_rep4, zmw2index, 'K562_Rep4', tf)\n",
    "#tot_indices = np.concatenate((rep1_indices,rep2_indices,rep3_indices,rep4_indices))    \n",
    "tot_indices = np.concatenate((rep1_indices,rep3_indices))\n",
    "cluster_indices = clusters[tot_indices]\n",
    "lengths_indices = lengths_total[tot_indices]\n",
    "tot_labels = np.repeat(tf, len(tot_indices))\n",
    "cluster_ids.append(cluster_indices)\n",
    "chromatin_types.append(tot_labels)\n",
    "frag_length.append(lengths_indices)\n",
    "total_index.append(tot_indices)\n",
    "\n",
    "total_dataframe = pd.DataFrame({'cluster_id': np.concatenate(cluster_ids), 'chromatin_type': np.concatenate(chromatin_types) \\\n",
    "    , 'total_index': np.concatenate(total_index), 'frag_length' : np.concatenate(frag_length)})\n",
    "total_dataframe\n",
    "\n",
    "fho = open('fishers_tests_rep1.txt','w')\n",
    "for i in np.unique(total_dataframe['cluster_id']):\n",
    "    if i == '7': continue\n",
    "    for j in np.unique(total_dataframe['chromatin_type']):\n",
    "        num_clust_lab = len(total_dataframe[(total_dataframe['cluster_id'] == i) & (total_dataframe['chromatin_type'] == j)])\n",
    "        num_clust_notlab = len(total_dataframe[(total_dataframe['cluster_id'] == i) & (total_dataframe['chromatin_type'] != j)])\n",
    "        num_notclust_lab = len(total_dataframe[(total_dataframe['cluster_id'] != i) & (total_dataframe['chromatin_type'] == j)])\n",
    "        num_notclust_notlab = len(total_dataframe[(total_dataframe['cluster_id'] != i) & (total_dataframe['chromatin_type'] != j)])\n",
    "        odds_r, pval = sp.stats.fisher_exact([[num_clust_lab, num_notclust_lab], \\\n",
    "            [num_clust_notlab, num_notclust_notlab]])\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\" % (i, j, num_clust_lab, odds_r, pval), file=fho)\n",
    "fho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicate 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def find_indices(sites, zmw_dict, samp_label, chrom_label):\n",
    "    lines = open(sites)\n",
    "    indices = []\n",
    "    for entry in lines:\n",
    "        split = entry.split()\n",
    "        hole = \"%s_%s\" % (samp_label, split[0])\n",
    "        if hole in zmw_dict:\n",
    "            indices.append(zmw_dict[hole])\n",
    "    return indices\n",
    "        \n",
    "#print(list(zmw2index.keys())[:100])\n",
    "cluster_file = open('final_cluster_labels_total.txt')\n",
    "lengths_total = np.load('length_filtered_lengths.npy')\n",
    "clusters = []\n",
    "for line in cluster_file:\n",
    "    split = line.split()\n",
    "    clusters.append(split[1])\n",
    "clusters = np.array(clusters)\n",
    "\n",
    "#total_dataframe = pd.DataFrame({'cluster_id': [], 'chromatin_type': [], 'total_index': [], 'frag_length' : []})\n",
    "cluster_ids = []\n",
    "chromatin_types = []\n",
    "total_index = []\n",
    "frag_length = []\n",
    "\n",
    "tfs = open('./pbrun4_purple/biology_analyses/new_beds.txt')\n",
    "#tfs = ['coopfoots\\n']\n",
    "\n",
    "for line in tfs:\n",
    "    tf = line.split()[0]\n",
    "    sample = '%s' % tf\n",
    "    print(sample)\n",
    "#    sites_rep1 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "    sites_rep2 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "#   sites_rep3 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "    sites_rep4 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_zmws' % sample\n",
    "#    rep1_indices = find_indices(sites_rep1, zmw2index, 'K562_Rep1', tf)\n",
    "    rep2_indices = find_indices(sites_rep2, zmw2index, 'K562_Rep2', tf)\n",
    "#    rep3_indices = find_indices(sites_rep3, zmw2index, 'K562_Rep3', tf)\n",
    "    rep4_indices = find_indices(sites_rep4, zmw2index, 'K562_Rep4', tf)\n",
    "#    tot_indices = np.concatenate((rep1_indices,rep2_indices,rep3_indices,rep4_indices))    \n",
    "    tot_indices = np.concatenate((rep2_indices,rep4_indices))    \n",
    "    cluster_indices = clusters[tot_indices]\n",
    "    lengths_indices = lengths_total[tot_indices]\n",
    "    tot_labels = np.repeat(tf, len(tot_indices))\n",
    "    cluster_ids.append(cluster_indices)\n",
    "    chromatin_types.append(tot_labels)\n",
    "    frag_length.append(lengths_indices)\n",
    "    total_index.append(tot_indices)\n",
    "\n",
    "print(cluster_ids)    \n",
    "\n",
    "#Also include Siva Satellite ZMWs\n",
    "#sites_rep1 = './pbrun4_purple/biology_analyses/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.blast.satellite.txt.zmws'\n",
    "sites_rep2 = './pbrun4_purple/biology_analyses/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.blast.satellite.txt.zmws'\n",
    "#sites_rep3 = './pbrun4_purple/biology_analyses/pbrun6.split.k562_postlyso_chromatin.ccs.blast.satellite.txt.zmws'\n",
    "sites_rep4 = './pbrun4_purple/biology_analyses/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.blast.satellite.txt.zmws'\n",
    "tf='Satellite'\n",
    "#rep1_indices = find_indices(sites_rep1, zmw2index, 'K562_Rep1', tf)\n",
    "rep2_indices = find_indices(sites_rep2, zmw2index, 'K562_Rep2', tf)\n",
    "#rep3_indices = find_indices(sites_rep3, zmw2index, 'K562_Rep3', tf)\n",
    "rep4_indices = find_indices(sites_rep4, zmw2index, 'K562_Rep4', tf)\n",
    "#tot_indices = np.concatenate((rep1_indices,rep2_indices,rep3_indices,rep4_indices))    \n",
    "tot_indices = np.concatenate((rep2_indices,rep4_indices))\n",
    "cluster_indices = clusters[tot_indices]\n",
    "lengths_indices = lengths_total[tot_indices]\n",
    "tot_labels = np.repeat(tf, len(tot_indices))\n",
    "cluster_ids.append(cluster_indices)\n",
    "chromatin_types.append(tot_labels)\n",
    "frag_length.append(lengths_indices)\n",
    "total_index.append(tot_indices)\n",
    "\n",
    "total_dataframe = pd.DataFrame({'cluster_id': np.concatenate(cluster_ids), 'chromatin_type': np.concatenate(chromatin_types) \\\n",
    "    , 'total_index': np.concatenate(total_index), 'frag_length' : np.concatenate(frag_length)})\n",
    "total_dataframe\n",
    "\n",
    "fho = open('fishers_tests_rep2.txt','w')\n",
    "for i in np.unique(total_dataframe['cluster_id']):\n",
    "    if i == '7': continue\n",
    "    for j in np.unique(total_dataframe['chromatin_type']):\n",
    "        num_clust_lab = len(total_dataframe[(total_dataframe['cluster_id'] == i) & (total_dataframe['chromatin_type'] == j)])\n",
    "        num_clust_notlab = len(total_dataframe[(total_dataframe['cluster_id'] == i) & (total_dataframe['chromatin_type'] != j)])\n",
    "        num_notclust_lab = len(total_dataframe[(total_dataframe['cluster_id'] != i) & (total_dataframe['chromatin_type'] == j)])\n",
    "        num_notclust_notlab = len(total_dataframe[(total_dataframe['cluster_id'] != i) & (total_dataframe['chromatin_type'] != j)])\n",
    "        odds_r, pval = sp.stats.fisher_exact([[num_clust_lab, num_notclust_lab], \\\n",
    "            [num_clust_notlab, num_notclust_notlab]])\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\" % (i, j, num_clust_lab, odds_r, pval), file=fho)\n",
    "fho.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 500\n",
    "\n",
    "tfs = open('./pbrun4_purple/biology_analyses/tfs.txt')\n",
    "#tfs = ['GATA1\\n']\n",
    "lengths_total = np.array([0])\n",
    "labels_total = np.array([0])\n",
    "mat_total = []\n",
    "replicate_tracker = []\n",
    "for line in tfs:\n",
    "    tf = line.split()[0]\n",
    "    sample = '%s' % tf\n",
    "    sites_rep1 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_flat_zmws' % sample\n",
    "    sites_rep2 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_flat_zmws' % sample\n",
    "    sites_rep3 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_flat_zmws' % sample\n",
    "    sites_rep4 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_flat_zmws' % sample\n",
    "    cntrl_sites_rep1 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_random_flat_zmws' % sample\n",
    "    cntrl_sites_rep2 = './pbrun4_purple/ccs/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_random_flat_zmws' % sample\n",
    "    cntrl_sites_rep3 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin.ccs.aligned.sorted.bam.%s_random_flat_zmws' % sample\n",
    "    cntrl_sites_rep4 = './pbrun6/ccs/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.aligned.sorted.bam.%s_random_flat_zmws' % sample\n",
    "    mat_rep1, lengths, labels = distill_tipds_flat_centered(rep1, sites_rep1, length, sample)\n",
    "    \n",
    "    cntrl_mat_rep1, cntrl_lengths, cntrl_labels = distill_tipds_flat_centered(rep1, cntrl_sites_rep1, length, \"%s_control\" % sample ,subsample=len(lengths))    \n",
    "    \n",
    "    mat_rep2, lengths2, labels2 = distill_tipds_flat_centered(rep2,  sites_rep2, length, sample)\n",
    "    \n",
    "    cntrl_mat_rep2, cntrl_lengths2, cntrl_labels2 = distill_tipds_flat_centered(rep2,  cntrl_sites_rep2, length, \"%s_control\" % sample,subsample=len(lengths2))    \n",
    "    \n",
    "    mat_rep3, lengths3, labels3 = distill_tipds_flat_centered(rep3,  sites_rep3, length, sample)\n",
    "    \n",
    "    cntrl_mat_rep3, cntrl_lengths3, cntrl_labels3 = distill_tipds_flat_centered(rep3,cntrl_sites_rep3, length, \"%s_control\" % sample,subsample=len(lengths3))    \n",
    "    \n",
    "    mat_rep4, lengths4, labels4 = distill_tipds_flat_centered(rep4, sites_rep4, length, sample)\n",
    "    \n",
    "    cntrl_mat_rep4, cntrl_lengths4, cntrl_labels4 = distill_tipds_flat_centered(rep4, cntrl_sites_rep4, length, \"%s_control\" % sample,subsample=len(lengths4))    \n",
    "    \n",
    "#    mat_total.extend((mat_rep1,mat_rep2,mat_rep3,mat_rep4,cntrl_mat_rep1,cntrl_mat_rep2,cntrl_mat_rep3,cntrl_mat_rep4))\n",
    "#    mat_total.extend((mat_rep1,mat_rep2,mat_rep3,mat_rep4))\n",
    "#    mat_msix = np.vstack((mat_rep1, mat_rep2, mat_rep3, mat_rep4))\n",
    "#    mat_msix_control = np.vstack((cntrl_mat_rep1, cntrl_mat_rep2, cntrl_mat_rep3, cntrl_mat_rep4))\n",
    "    for i in range(len(mat_rep1)):\n",
    "        replicate_tracker.append('Replicate 1')\n",
    "    for i in range(len(mat_rep2)):\n",
    "        replicate_tracker.append('Replicate 2')\n",
    "    for i in range(len(mat_rep3)):\n",
    "        replicate_tracker.append('Replicate 1')\n",
    "    for i in range(len(mat_rep4)):\n",
    "        replicate_tracker.append('Replicate 2')\n",
    "    for i in range(len(cntrl_mat_rep1)):\n",
    "        replicate_tracker.append('Replicate 1')\n",
    "    for i in range(len(cntrl_mat_rep2)):\n",
    "        replicate_tracker.append('Replicate 2')\n",
    "    for i in range(len(cntrl_mat_rep3)):\n",
    "        replicate_tracker.append('Replicate 1')\n",
    "    for i in range(len(cntrl_mat_rep4)):\n",
    "        replicate_tracker.append('Replicate 2')\n",
    "    print(\"%s\\t%s\" % (sample, (len(mat_rep1)+len(mat_rep2)+len(mat_rep3)+len(mat_rep4))))\n",
    "    print(\"%s_control\\t%s\" % (sample, (len(cntrl_mat_rep1)+len(cntrl_mat_rep2)+len(cntrl_mat_rep3)+len(cntrl_mat_rep4))))\n",
    "    lengths_total = np.concatenate((lengths_total, lengths, lengths2,lengths3,lengths4, cntrl_lengths, cntrl_lengths2,cntrl_lengths3,cntrl_lengths4))\n",
    "    labels_total = np.concatenate((labels_total, labels, labels2,labels3,labels4,cntrl_labels, cntrl_labels2, cntrl_labels3, cntrl_labels4))\n",
    "#     lengths_total = np.concatenate((lengths_total, lengths, lengths2,lengths3,lengths4))\n",
    "#     labels_total = np.concatenate((labels_total, labels, labels2,labels3,labels4))\n",
    "#     process = pd.Series(np.nanmean(mat_msix, axis=0)).rolling(33,center=True,min_periods=1).mean()\n",
    "#     process_neg = pd.Series(np.nanmean(mat_msix_control, axis=0)).rolling(33,center=True,min_periods=1).mean()\n",
    "#     process = np.nanmean(binarize_sig(pd.DataFrame(mat_msix).rolling(10, axis=1, center=True, min_periods=1).mean()), axis=0)\n",
    "#     process_neg = np.nanmean(binarize_sig(pd.DataFrame(mat_msix_control).rolling(10, axis=1, center=True, min_periods=1).mean()), axis=0)\n",
    "#     plt.figure(figsize=(10,10))\n",
    "#     subplot(121)\n",
    "#     plt.plot(range(len(process)), process)\n",
    "#     subplot(122)\n",
    "#     plt.plot(range(len(process_neg)), process_neg)\n",
    "#     plt.show()\n",
    "#     fho=open('%s_m6A.txt' % tf,'w')\n",
    "#     for i in range(len(process)):\n",
    "#         print(\"%s\\t%s\\t%s\\tsite\" % (i, process[i], tf),file=fho)\n",
    "#         print(\"%s\\t%s\\t%s\\tcontrol\" % (i, process_neg[i], tf),file=fho)\n",
    "#     fho.close()\n",
    "\n",
    "mat_total = np.vstack(mat_total)\n",
    "lengths_total = lengths_total[1:]\n",
    "labels_total = labels_total[1:]\n",
    "\n",
    "tfs.close()\n",
    "\n",
    "#np.save('length_filtered_molecules_tfs',mat_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_file = open('final_cluster_labels_tfs.txt')\n",
    "clusters = []\n",
    "for line in cluster_file:\n",
    "    split = line.split()\n",
    "    clusters.append(split[0])\n",
    "clusters = np.array(clusters)\n",
    "cluster_file.close()\n",
    "print(len(labels_total))\n",
    "print(len(replicate_tracker))\n",
    "print(len(clusters))\n",
    "fho = open('repro_tfs_rep1.txt','w')\n",
    "#Replicate 1\n",
    "cluster_df = {'clusters':clusters, 'labels':labels_total[1:], 'replicates':replicate_tracker}\n",
    "cluster_df = pd.DataFrame(cluster_df)\n",
    "cluster_df = cluster_df[cluster_df['replicates'] == 'Replicate 1']\n",
    "for i in np.unique(clusters):\n",
    "    for j in np.unique(labels_total[1:]):\n",
    "        num_clust_lab = len(cluster_df[(cluster_df['clusters'] == i) & (cluster_df['labels'] == j)])\n",
    "        num_clust_notlab = len(cluster_df[(cluster_df['clusters'] == i) & (cluster_df['labels'] != j)])\n",
    "        num_notclust_lab = len(cluster_df[(cluster_df['clusters'] != i) & (cluster_df['labels'] == j)])\n",
    "        num_notclust_notlab = len(cluster_df[(cluster_df['clusters'] != i) & (cluster_df['labels'] != j)])\n",
    "        odds_r, pval = sp.stats.fisher_exact([[num_clust_lab, num_notclust_lab], \\\n",
    "            [num_clust_notlab, num_notclust_notlab]])\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\" % (i, j, num_clust_lab, odds_r, pval), file=fho)\n",
    "fho.close()\n",
    "#labs = np.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate out alpha, beta, and gamma satellite sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def find_indices(sites, zmw_dict, samp_label, chrom_label):\n",
    "    lines = open(sites)\n",
    "    indices = []\n",
    "    labels = []\n",
    "    for entry in lines:\n",
    "        split = entry.split()\n",
    "        hole = \"%s_%s\" % (samp_label, split[0])\n",
    "        if hole in zmw_dict:\n",
    "            indices.append(zmw_dict[hole])\n",
    "            if split[4][0] == 'A':\n",
    "                labels.append('alpha')\n",
    "            elif split[4][0] =='B':\n",
    "                labels.append('beta')\n",
    "            elif split[4][0] == 'G':\n",
    "                labels.append('gamma')\n",
    "    return (indices, labels)\n",
    "        \n",
    "#print(list(zmw2index.keys())[:100])\n",
    "cluster_file = open('final_cluster_labels_total.txt')\n",
    "lengths_total = np.load('length_filtered_lengths.npy')\n",
    "clusters = []\n",
    "for line in cluster_file:\n",
    "    split = line.split()\n",
    "    clusters.append(split[1])\n",
    "clusters = np.array(clusters)\n",
    "\n",
    "#total_dataframe = pd.DataFrame({'cluster_id': [], 'chromatin_type': [], 'total_index': [], 'frag_length' : []})\n",
    "cluster_ids = []\n",
    "chromatin_types = []\n",
    "total_index = []\n",
    "frag_length = []\n",
    "\n",
    " \n",
    "\n",
    "#Also include Siva Satellite ZMWs\n",
    "sites_rep1 = './pbrun4_purple/biology_analyses/Purple_PBRun.k562_postlyso_chromatin--k562_postlyso_chromatin.ccs.blast.satellite.txt.zmws'\n",
    "sites_rep2 = './pbrun4_purple/biology_analyses/Purple_PBRun.k562_postlyso_chromatin_rep2--k562_postlyso_chromatin_rep2.ccs.blast.satellite.txt.zmws'\n",
    "sites_rep3 = './pbrun4_purple/biology_analyses/pbrun6.split.k562_postlyso_chromatin.ccs.blast.satellite.txt.zmws'\n",
    "sites_rep4 = './pbrun4_purple/biology_analyses/pbrun6.split.k562_postlyso_chromatin_rep2.ccs.blast.satellite.txt.zmws'\n",
    "tfs = []\n",
    "\n",
    "rep1_indices,labels1 = find_indices(sites_rep1, zmw2index, 'K562_Rep1', tf)\n",
    "rep2_indices,labels2 = find_indices(sites_rep2, zmw2index, 'K562_Rep2', tf)\n",
    "rep3_indices,labels3 = find_indices(sites_rep3, zmw2index, 'K562_Rep3', tf)\n",
    "rep4_indices,labels4 = find_indices(sites_rep4, zmw2index, 'K562_Rep4', tf)\n",
    "tot_indices = np.concatenate((rep1_indices,rep2_indices,rep3_indices,rep4_indices))    \n",
    "cluster_indices = clusters[tot_indices]\n",
    "lengths_indices = lengths_total[tot_indices]\n",
    "tot_labels = np.concatenate((labels1,labels2,labels3,labels4))\n",
    "cluster_ids.append(cluster_indices)\n",
    "chromatin_types.append(tot_labels)\n",
    "frag_length.append(lengths_indices)\n",
    "total_index.append(tot_indices)\n",
    "\n",
    "total_dataframe = pd.DataFrame({'cluster_id': np.concatenate(cluster_ids), 'chromatin_type': np.concatenate(chromatin_types) \\\n",
    "    , 'total_index': np.concatenate(total_index), 'frag_length' : np.concatenate(frag_length)})\n",
    "total_dataframe\n",
    "\n",
    "fho = open('fishers_tests_satellite.txt','w')\n",
    "for i in np.unique(total_dataframe['cluster_id']):\n",
    "    if i == '7': continue\n",
    "    for j in np.unique(total_dataframe['chromatin_type']):\n",
    "        num_clust_lab = len(total_dataframe[(total_dataframe['cluster_id'] == i) & (total_dataframe['chromatin_type'] == j)])\n",
    "        num_clust_notlab = len(total_dataframe[(total_dataframe['cluster_id'] == i) & (total_dataframe['chromatin_type'] != j)])\n",
    "        num_notclust_lab = len(total_dataframe[(total_dataframe['cluster_id'] != i) & (total_dataframe['chromatin_type'] == j)])\n",
    "        num_notclust_notlab = len(total_dataframe[(total_dataframe['cluster_id'] != i) & (total_dataframe['chromatin_type'] != j)])\n",
    "        odds_r, pval = sp.stats.fisher_exact([[num_clust_lab, num_notclust_lab], \\\n",
    "            [num_clust_notlab, num_notclust_notlab]])\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\" % (i, j, num_clust_lab, odds_r, pval), file=fho)\n",
    "fho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New data analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 1000\n",
    "\n",
    "fho = open('for_supplement_sig.txt','w')\n",
    "fho2 = open('for_supplement_lens.txt','w')\n",
    "\n",
    "##BINARIZED\n",
    "k562_rep3_long1 = '/avicenna/vramani/analyses/pacbio/pbrun8_berkeley/processed/binarized/pbrun8_berkeley_k562_4C_10m_plusM_rep1_bingmm'\n",
    "k562_rep4_long1 = '/avicenna/vramani/analyses/pacbio/pbrun8_berkeley/processed/binarized/pbrun8_berkeley_k562_4C_10m_plusM_rep2_bingmm'\n",
    "k562_rep3_long1_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun8_berkeley/processed/onlyT/pbrun8_berkeley_k562_4C_10m_plusM_rep1_onlyT'\n",
    "k562_rep4_long1_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun8_berkeley/processed/onlyT/pbrun8_berkeley_k562_4C_10m_plusM_rep2_onlyT'\n",
    "\n",
    "k562_rep3_long2 = '/avicenna/vramani/analyses/pacbio/pbrun8_cell2/processed/binarized/pbrun8_cell2_k562_4C_10m_plusM_rep1_bingmm'\n",
    "k562_rep4_long2 = '/avicenna/vramani/analyses/pacbio/pbrun8_cell2/processed/binarized/pbrun8_cell2_k562_4C_10m_plusM_rep2_bingmm'\n",
    "k562_rep3_long2_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun8_cell2/processed/onlyT/pbrun8_cell2_k562_4C_10m_plusM_rep1_onlyT'\n",
    "k562_rep4_long2_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun8_cell2/processed/onlyT/pbrun8_cell2_k562_4C_10m_plusM_rep2_onlyT'\n",
    "\n",
    "rep1, l1, lb1, h1 = eat_pickle_b2m(\"%s.pickle\" % k562_rep3_long1, 'K562_Rep1', length)\n",
    "rep2, l2, lb2, h2 = eat_pickle_b2m(\"%s.pickle\" % k562_rep4_long1, 'K562_Rep2', length)\n",
    "rep3, l3, lb3, h3 = eat_pickle_b2m(\"%s.pickle\" % k562_rep3_long2, 'K562_Rep3', length)\n",
    "rep4, l4, lb4, h4 = eat_pickle_b2m(\"%s.pickle\" % k562_rep4_long2, 'K562_Rep4', length)\n",
    "mat_total=np.vstack((rep1,rep2,rep3,rep4))\n",
    "lengths_total = np.array(np.concatenate((l1,l2,l3,l4), axis=None))\n",
    "# #lb_total = np.array(np.concatenate((lb1,lb2,lb3,lb4), axis=None))\n",
    "# h_total = np.array(np.concatenate((h1,h2,h3,h4), axis=None))\n",
    "# rep1, rep2, rep3, rep4 = [None, None, None, None]\n",
    "plotme_sig = np.nanmean(mat_total,axis=0)\n",
    "\n",
    "for i in range(len(plotme_sig)):\n",
    "    print(\"%s\\t%s\\t4deg_10min\" % (i, plotme_sig[i]),file=fho)\n",
    "\n",
    "for i in lengths_total:\n",
    "    print(\"%s\\t4deg_10min\" % i, file=fho2)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(len(mat_total[0])), np.nanmean(mat_total,axis=0))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 1000\n",
    "\n",
    "\n",
    "# #BINARIZED\n",
    "k562_rep3_long1 = '/avicenna/vramani/analyses/pacbio/pbrun8_berkeley/processed/binarized/pbrun8_berkeley_k562_37C_1m_plusM_rep1_bingmm'\n",
    "#k562_rep4_long1 = '/avicenna/vramani/analyses/pacbio/pbrun8_berkeley/processed/binarized/pbrun8_berkeley_k562_37C_1m_plusM_rep2_bingmm'\n",
    "k562_rep3_long1_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun8_berkeley/processed/onlyT/pbrun8_berkeley_k562_37C_1m_plusM_rep1_onlyT'\n",
    "#k562_rep4_long1_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun8_berkeley/processed/onlyT/pbrun8_berkeley_k562_37C_1m_plusM_rep2_onlyT'\n",
    "\n",
    "\n",
    "k562_rep3_long2 = '/avicenna/vramani/analyses/pacbio/pbrun8_cell2/processed/binarized/pbrun8_cell2_k562_37C_1m_plusM_rep1_bingmm'\n",
    "#k562_rep4_long2 = '/avicenna/vramani/analyses/pacbio/pbrun8_cell2/processed/binarized/pbrun8_cell2_k562_37C_1m_plusM_rep2_bingmm'\n",
    "k562_rep3_long2_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun8_cell2/processed/onlyT/pbrun8_cell2_k562_37C_1m_plusM_rep1_onlyT'\n",
    "#k562_rep4_long2_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun8_cell2/processed/onlyT/pbrun8_cell2_k562_37C_1m_plusM_rep2_onlyT'\n",
    "\n",
    "rep1, l1, lb1, h1 = eat_pickle_b2m(\"%s.pickle\" % k562_rep3_long1, 'K562_Rep1', length)\n",
    "#rep2, l2, lb2, h2 = eat_pickle_b2m(\"%s.pickle\" % k562_rep4_long1, 'K562_Rep2', length)\n",
    "rep3, l3, lb3, h3 = eat_pickle_b2m(\"%s.pickle\" % k562_rep3_long2, 'K562_Rep3', length)\n",
    "#rep4, l4, lb4, h4 = eat_pickle_b2m(\"%s.pickle\" % k562_rep4_long2, 'K562_Rep4', length)\n",
    "mat_total=np.vstack((rep1,rep3))\n",
    "lengths_total = np.array(np.concatenate((l1,l3), axis=None))\n",
    "# #lb_total = np.array(np.concatenate((lb1,lb2,lb3,lb4), axis=None))\n",
    "# h_total = np.array(np.concatenate((h1,h2,h3,h4), axis=None))\n",
    "# rep1, rep2, rep3, rep4 = [None, None, None, None]\n",
    "print(mat_total)\n",
    "plotme_sig = np.nanmean(mat_total,axis=0)\n",
    "\n",
    "for i in range(len(plotme_sig)):\n",
    "    print(\"%s\\t%s\\t37deg_1min\" % (i, plotme_sig[i]),file=fho)\n",
    "\n",
    "for i in lengths_total:\n",
    "    print(\"%s\\t37deg_1min\" % i, file=fho2)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(len(mat_total[0])), np.nanmean(mat_total,axis=0))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.distplot(lengths_total)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 1000\n",
    "\n",
    "\n",
    "# #BINARIZED\n",
    "k562_rep3_long1 = '/avicenna/vramani/analyses/pacbio/pbrun8_berkeley/processed/binarized/pbrun8_berkeley_k562_4C_60m_plusM_rep1_bingmm'\n",
    "#k562_rep4_long1 = '/avicenna/vramani/analyses/pacbio/pbrun8_berkeley/processed/binarized/pbrun8_berkeley_k562_37C_1m_plusM_rep2_bingmm'\n",
    "k562_rep3_long1_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun8_berkeley/processed/onlyT/pbrun8_berkeley_k562_4C_60m_plusM_rep1_onlyT'\n",
    "#k562_rep4_long1_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun8_berkeley/processed/onlyT/pbrun8_berkeley_k562_37C_1m_plusM_rep2_onlyT'\n",
    "\n",
    "\n",
    "k562_rep3_long2 = '/avicenna/vramani/analyses/pacbio/pbrun8_cell2/processed/binarized/pbrun8_cell2_k562_4C_60m_plusM_rep1_bingmm'\n",
    "k562_rep4_long2 = '/avicenna/vramani/analyses/pacbio/pbrun8_cell2/processed/binarized/pbrun8_cell2_k562_4C_60m_plusM_rep2_bingmm'\n",
    "k562_rep3_long2_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun8_cell2/processed/onlyT/pbrun8_cell2_k562_4C_60m_plusM_rep1_onlyT'\n",
    "k562_rep4_long2_zmwinfo = '/avicenna/vramani/analyses/pacbio/pbrun8_cell2/processed/onlyT/pbrun8_cell2_k562_4C_60m_plusM_rep2_onlyT'\n",
    "\n",
    "rep1, l1, lb1, h1 = eat_pickle_b2m(\"%s.pickle\" % k562_rep3_long1, 'K562_Rep1', length)\n",
    "#rep2, l2, lb2, h2 = eat_pickle_b2m(\"%s.pickle\" % k562_rep4_long1, 'K562_Rep2', length)\n",
    "rep3, l3, lb3, h3 = eat_pickle_b2m(\"%s.pickle\" % k562_rep3_long2, 'K562_Rep3', length)\n",
    "#rep4, l4, lb4, h4 = eat_pickle_b2m(\"%s.pickle\" % k562_rep4_long2, 'K562_Rep4', length)\n",
    "mat_total=np.vstack((rep1,rep3,rep4))\n",
    "lengths_total = np.array(np.concatenate((l1,l3,l4), axis=None))\n",
    "# #lb_total = np.array(np.concatenate((lb1,lb2,lb3,lb4), axis=None))\n",
    "# h_total = np.array(np.concatenate((h1,h2,h3,h4), axis=None))\n",
    "# rep1, rep2, rep3, rep4 = [None, None, None, None]\n",
    "print(mat_total)\n",
    "plotme_sig = np.nanmean(mat_total,axis=0)\n",
    "\n",
    "for i in range(len(plotme_sig)):\n",
    "    print(\"%s\\t%s\\t4deg_60min\" % (i, plotme_sig[i]),file=fho)\n",
    "\n",
    "for i in lengths_total:\n",
    "    print(\"%s\\t4deg_60min\" % i, file=fho2)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(len(mat_total[0])), np.nanmean(mat_total,axis=0))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.distplot(lengths_total)\n",
    "plt.show()\n",
    "\n",
    "fho.close()\n",
    "fho2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(len(mat_total[0])), np.nanmean(mat_total,axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fho = open('4deg_10min_lengths.txt','w')\n",
    "for i in range(len(lengths_total)):\n",
    "    print(lengths_total[i], file=fho)\n",
    "fho.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
